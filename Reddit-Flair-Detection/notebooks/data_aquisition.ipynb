{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Aquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reference the publically available [Reddit dump](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/). The dataset is publically available on Google BigQuery and is divided across months from December 2015 - October 2018. BigQuery allows us to perform low latency queries on massive datasets. One example is [this](https://bigquery.cloud.google.com/table/fh-bigquery:reddit_posts.2018_08). Unfortunately the posts have not been tagged with their comments. To extract this information, in addition to BigQuery, we will use [PRAW](https://praw.readthedocs.io/en/latest/) for this task. \n",
    "\n",
    "The idea is to randomly query a subset of posts from December 2015 - October 2018. Then for each of the post, use praw to get comments for each one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are considering 11 flairs:\n",
    "```\n",
    "1. AskIndia\n",
    "2. Politics\n",
    "3. Sports\n",
    "4. Food\n",
    "5. [R]eddiquette\n",
    "6. Non-Political\n",
    "7. Scheduled\n",
    "8. Business/Finance\n",
    "9. Science/Technology\n",
    "10. Photography\n",
    "11. Policy/Economy \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: This notebook requires a GCP Acount, a Reddit account,  CLOUD SDK installed.\n",
    "Go to [cloud sdk for more info](https://cloud.google.com/sdk/)\n",
    "\n",
    "Follow the following steps before running this notebook:\n",
    "```\n",
    "1. Intall big query locally using- pip install --upgrade google-cloud-bigquery\n",
    "2. In the GCP Console, go to the Create service account key page.\n",
    "3. From the Service account drop-down list, select New service account.\n",
    "4. In the Service account name field, enter a name .\n",
    "5. From the Role drop-down list, select Project > Owner.\n",
    "6. Click Create. A JSON file that contains your key downloads to your computer.\n",
    "7. In a new session, execute the following command- \n",
    "    export GOOGLE_APPLICATION_CREDENTIALS=\"/home/user/Downloads/[FILE_NAME].json\"\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To setup reddit credentials\n",
    "```\n",
    "1. Go to https://www.reddit.com/prefs/apps\n",
    "2. Click create app at the bottom\n",
    "3. Enter an app name, choose 'script' and enter http://localhost:8080 in redirect uri\n",
    "4. Save the client id, client secret\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run mongo client in a different session\n",
    "```\n",
    "1. In a seperate session(terminal), run ./mongod before proceeding.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing all libraries\n",
    "\n",
    "Here we will be using \n",
    "1. PyMongo - a python wrapper for MongoDB to build our train and test datasets\n",
    "2. PRAW - a python wrapper for reddit API\n",
    "3. Numpy\n",
    "4. Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import praw\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initializing the praw.reddit(), biguery.Client() and MongoClient() objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enter your credentials here\n",
    "client_id = ''\n",
    "client_secret = ''\n",
    "user_agent = ''\n",
    "username = ''\n",
    "password = ''\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id, \\\n",
    "                     client_secret=client_secret, \\\n",
    "                     user_agent=user_agent, \\\n",
    "                     username=username, \\\n",
    "                     password=password)\n",
    "\n",
    "client = bigquery.Client()\n",
    "mongo_client = MongoClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are querying the dataset from 2015-2018 limiting results to 100000 records and save to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "QUERY_POSTS = (\n",
    "'SELECT * except (domain, subreddit, author_flair_css_class, link_flair_css_class, author_flair_text,'\n",
    "                 'from_kind, saved, hide_score, archived, from_id, name, quarantine, distinguished, stickied,'\n",
    "                 'thumbnail, is_self, retrieved_on, gilded, subreddit_id) '\n",
    "'FROM `fh-bigquery.reddit_posts.201*`'\n",
    "'WHERE subreddit = \"india\" and link_flair_text in (\"Sports\", \"Politics\", \"AskIndia\", \"Business/Finance\", \"Food\",' \n",
    "    '\"Science/Technology\", \"Non-Political\", \"Photography\", \"Policy/Economy\", \"Scheduled\", \"[R]eddiquette\") ' \n",
    "'LIMIT 100000'\n",
    ")\n",
    "\n",
    "query_job = client.query(QUERY_POSTS)\n",
    "query = query_job.result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buidling our train and test sets\n",
    "\n",
    "To build a balanced dataset, we will limit the number of samples for each flair at 2000 and randomly sample from the extracted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Policy/Economy', '[R]eddiquette', 'Sports', 'Business/Finance', 'Photography', 'Politics', 'Scheduled', 'Food', 'Science/Technology', 'AskIndia', 'Non-Political']\n",
      "20608\n"
     ]
    }
   ],
   "source": [
    "keep = []\n",
    "data = query\n",
    "flairs = [flair for flair in flairs if not str(flair) == 'nan']\n",
    "for flair in flairs:\n",
    "    l = len(data[data['link_flair_text'] == flair])\n",
    "    if l > 2000:\n",
    "        l = 2000\n",
    "    idx = list(data[data['link_flair_text'] == flair]['id'])\n",
    "    c = np.random.choice(idx, l, replace=False)\n",
    "    for i in c:\n",
    "        keep.append(i)\n",
    "\n",
    "print (len(keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep only these samples and discard others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['81cfa2', '61zg8u', '7md8i9', '701ysh', '7o9rjd', '6zt3kv', '6ko0s4', '8s6mm2', '98sra2', '68sy9z']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>id</th>\n",
       "      <th>over_18</th>\n",
       "      <th>permalink</th>\n",
       "      <th>link_flair_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1524926782</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/8fkbkh...</td>\n",
       "      <td>29</td>\n",
       "      <td>50</td>\n",
       "      <td>[R] Losing the will to live every single day</td>\n",
       "      <td>So, I didn't do well in JEE Mains, even after ...</td>\n",
       "      <td>8fkbkh</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/india/comments/8fkbkh/r_losing_the_will_to_...</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc     author                                                url  \\\n",
       "7  1524926782  [deleted]  https://www.reddit.com/r/india/comments/8fkbkh...   \n",
       "\n",
       "  num_comments score                                         title  \\\n",
       "7           29    50  [R] Losing the will to live every single day   \n",
       "\n",
       "                                            selftext      id over_18  \\\n",
       "7  So, I didn't do well in JEE Mains, even after ...  8fkbkh   False   \n",
       "\n",
       "                                           permalink link_flair_text  \n",
       "7  /r/india/comments/8fkbkh/r_losing_the_will_to_...   [R]eddiquette  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['id'].isin(keep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dataset to a mongoDB collection\n",
    "\n",
    "Here we define a mongodb database - \"dataset\" and dump the dataframe to collection \"reddit_data\". Before doing this, we use praw to get comments for each dataset and add this feature as comments to our dataset. For each post, we limit to top 10 comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mongo_client = MongoClient('mongodb://localhost:27017/')\n",
    "db = mongo_client.dataset\n",
    "collection = db['reddit_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:30: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153.4097396651904\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "np.random.seed(42)\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    comments = []\n",
    "    num_comm = 10\n",
    "    \n",
    "    submission = reddit.submission(id=row['id'])\n",
    "    l = len(submission.comments)\n",
    "    \n",
    "    if l > 0:\n",
    "        if l < 10:\n",
    "            num_comm = l\n",
    "        r = np.random.choice(l, num_comm, replace=False) \n",
    "        for i in r:\n",
    "            comments.append(submission.comments[i].body)\n",
    "    \n",
    "    t = {'created_utc': row['created_utc'],\n",
    "        'title': row['title'],\n",
    "        'selftext': row['selftext'],\n",
    "        'author': row['author'],\n",
    "        'num_comments': row['num_comments'],\n",
    "        'id': row['id'],\n",
    "        'link_flair_text': row['link_flair_text'],\n",
    "        'comments': comments,\n",
    "        'url': row['url'],\n",
    "        'score': row['score'],\n",
    "        'over_18': row['over_18']}\n",
    "    collection.insert(t)\n",
    "\n",
    "print ((time.time()-start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now export this dataset as json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-05T17:55:48.216+0530\tconnected to: localhost\r\n",
      "2019-02-05T17:55:48.216+0530\texported 0 records\r\n"
     ]
    }
   ],
   "source": [
    "!mongoexport --db dataset -c reddit_dataset --out ./reddit_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Note: We manually split the dataset from this collection to training and test sets by a 80/20 split into train.json and test.json available in the github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
