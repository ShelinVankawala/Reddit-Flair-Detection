{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "x4ldbnG24qQZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reddit Flair Classification\n",
        "\n",
        "**Important** \n",
        "1. Install pytorch-pretrained-bert\n",
        "2. Download the pretrained models in current working directory\n",
        "3. Download the patient dataset in current working directory\n",
        "4. Converting TF Checkpoints to Pytorch model files\n",
        "5. Creating output directories for saving model files\n",
        "\n",
        "NOTE- Most of the code in this notebook is referenced from the [huggingface bert repo](https://github.com/huggingface/pytorch-pretrained-BERT)"
      ]
    },
    {
      "metadata": {
        "id": "l9RC1qv167nR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Downloading the train and test sets from S3 buckets**"
      ]
    },
    {
      "metadata": {
        "id": "0oeSeGok67xy",
        "colab_type": "code",
        "outputId": "1474dc5e-d74a-4c49-c857-eda833aa666e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate --no-proxy \"https://s3.amazonaws.com/redditdata2/train.json\"\n",
        "!wget --no-check-certificate --no-proxy \"https://s3.amazonaws.com/redditdata2/test.json\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-09 07:45:51--  https://s3.amazonaws.com/redditdata2/train.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.177.173\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.177.173|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24001937 (23M) [application/json]\n",
            "Saving to: ‘train.json’\n",
            "\n",
            "train.json          100%[===================>]  22.89M  25.2MB/s    in 0.9s    \n",
            "\n",
            "2019-02-09 07:45:52 (25.2 MB/s) - ‘train.json’ saved [24001937/24001937]\n",
            "\n",
            "--2019-02-09 07:45:53--  https://s3.amazonaws.com/redditdata2/test.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.177.173\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.177.173|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7902209 (7.5M) [application/json]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>]   7.54M  13.7MB/s    in 0.5s    \n",
            "\n",
            "2019-02-09 07:45:54 (13.7 MB/s) - ‘test.json’ saved [7902209/7902209]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m92o6tHTSXqU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Install pytorch-pretrained-bert, downloading the pretrained model and converting the tensorflow checkpoints to a pytorch state**"
      ]
    },
    {
      "metadata": {
        "id": "S3EZACu_56qi",
        "colab_type": "code",
        "outputId": "66f974c0-0846-4af3-a8cb-0ef33849feaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7976
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip\n",
        "\n",
        "!pytorch_pretrained_bert convert_tf_checkpoint_to_pytorch \\\n",
        "  uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
        "  uncased_L-12_H-768_A-12/bert_config.json \\\n",
        "  uncased_L-12_H-768_A-12/pytorch_model.bin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/68/84de54aea460eb5b2e90bf47a429aacc1ce97ff052ec40874ea38ae2331d/pytorch_pretrained_bert-0.4.0-py3-none-any.whl (45kB)\n",
            "\r\u001b[K    22% |███████▎                        | 10kB 10.5MB/s eta 0:00:01\r\u001b[K    45% |██████████████▌                 | 20kB 3.6MB/s eta 0:00:01\r\u001b[K    68% |█████████████████████▊          | 30kB 5.0MB/s eta 0:00:01\r\u001b[K    90% |█████████████████████████████   | 40kB 3.1MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 51kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.1.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.3)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.88 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.88)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.88->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.88->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.88->boto3->pytorch-pretrained-bert) (1.11.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.4.0\n",
            "--2019-02-08 05:51:12--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 2607:f8b0:400e:c09::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   120MB/s    in 3.2s    \n",
            "\n",
            "2019-02-08 05:51:16 (120 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
            "Converting TensorFlow checkpoint from /content/uncased_L-12_H-768_A-12/bert_model.ckpt with config at /content/uncased_L-12_H-768_A-12/bert_config.json\n",
            "Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "Loading TF weight bert/embeddings/word_embeddings with shape [30522, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "Loading TF weight cls/predictions/output_bias with shape [30522]\n",
            "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\n",
            "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight cls/predictions/transform/dense/bias with shape [768]\n",
            "Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\n",
            "Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
            "Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n",
            "Building PyTorch model from configuration: {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
            "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
            "Save PyTorch model to uncased_L-12_H-768_A-12/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SAHPlBpESmtO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Importing various libraries"
      ]
    },
    {
      "metadata": {
        "id": "kUh1B9Xg5PkK",
        "colab_type": "code",
        "outputId": "899290a3-b8a3-48db-c8c3-93a566cc2e4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import logging\n",
        "import argparse\n",
        "import six\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S89VFiR_5nT7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Basic Exploration"
      ]
    },
    {
      "metadata": {
        "id": "8elwbBmm0cjZ",
        "colab_type": "code",
        "outputId": "72badc95-edd7-4551-d015-e5bff926955d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "train_json = []\n",
        "\n",
        "for line in (open('train.json', 'r')):\n",
        "    train_json.append(json.loads(line))\n",
        "\n",
        "train = pd.DataFrame.from_records(train_json)\n",
        "\n",
        "flairs = list(set(train['link_flair_text']))\n",
        "for flair in flairs:\n",
        "    print (flair,': ' , len(train[train['link_flair_text'] == flair]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Non-Political :  1586\n",
            "Politics :  1587\n",
            "Food :  1656\n",
            "[R]eddiquette :  1604\n",
            "Science/Technology :  1626\n",
            "Photography :  554\n",
            "Scheduled :  1596\n",
            "AskIndia :  1523\n",
            "Sports :  1719\n",
            "Business/Finance :  1604\n",
            "Policy/Economy :  1431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KaFnaa7FqSWM",
        "colab_type": "code",
        "outputId": "9cc86473-5c62-49e4-c948-f6d213b0a00b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "test_json = []\n",
        "\n",
        "for line in (open('test.json', 'r')):\n",
        "    test_json.append(json.loads(line))\n",
        "\n",
        "test = pd.DataFrame.from_records(test_json)\n",
        "\n",
        "flairs = list(set(test['link_flair_text']))\n",
        "for flair in flairs:\n",
        "    print (flair,': ' , len(test[test['link_flair_text'] == flair]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Non-Political :  414\n",
            "Sports :  281\n",
            "Food :  344\n",
            "[R]eddiquette :  396\n",
            "Politics :  413\n",
            "Science/Technology :  374\n",
            "Photography :  86\n",
            "Scheduled :  372\n",
            "AskIndia :  477\n",
            "Business/Finance :  396\n",
            "Policy/Economy :  569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1N9r5lY6wTSB",
        "colab_type": "code",
        "outputId": "ed2f3570-35eb-4a11-f493-a1bb9be25b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print ('Original Size Train: ', (train.shape))\n",
        "print ('Test: ', (test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Size Train:  (16486, 12)\n",
            "Test:  (4122, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oWn6j8l2aLlk",
        "colab_type": "code",
        "outputId": "bc58d7fe-06c1-409e-ed09-3e9883fcbd99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "label_to_id = {'AskIndia': 0,\n",
        " 'Business/Finance': 1,\n",
        " 'Food': 2,\n",
        " 'Non-Political': 3,\n",
        " 'Photography': 4,\n",
        " 'Policy/Economy': 5,\n",
        " 'Politics': 6,\n",
        " 'Scheduled': 7,\n",
        " 'Science/Technology': 8,\n",
        " 'Sports': 9,\n",
        " '[R]eddiquette': 10}\n",
        "\n",
        "id_to_label = {v: k for k, v in label_to_id.items()}\n",
        "id_to_label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'AskIndia',\n",
              " 1: 'Business/Finance',\n",
              " 2: 'Food',\n",
              " 3: 'Non-Political',\n",
              " 4: 'Photography',\n",
              " 5: 'Policy/Economy',\n",
              " 6: 'Politics',\n",
              " 7: 'Scheduled',\n",
              " 8: 'Science/Technology',\n",
              " 9: 'Sports',\n",
              " 10: '[R]eddiquette'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "Hc0juubnXLKS",
        "colab_type": "code",
        "outputId": "a182c86e-69e9-480a-8086-9b592d34b52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "train.head(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>author</th>\n",
              "      <th>comments</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>id</th>\n",
              "      <th>link_flair_text</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>over_18</th>\n",
              "      <th>score</th>\n",
              "      <th>selftext</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'$oid': '5c5959797cf03b047caaa61a'}</td>\n",
              "      <td>[deleted]</td>\n",
              "      <td>[Listen to me. \\n\\n5 years down the line, this...</td>\n",
              "      <td>1524926782</td>\n",
              "      <td>8fkbkh</td>\n",
              "      <td>[R]eddiquette</td>\n",
              "      <td>29</td>\n",
              "      <td>False</td>\n",
              "      <td>50</td>\n",
              "      <td>So, I didn't do well in JEE Mains, even after ...</td>\n",
              "      <td>[R] Losing the will to live every single day</td>\n",
              "      <td>https://www.reddit.com/r/india/comments/8fkbkh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    _id     author  \\\n",
              "0  {'$oid': '5c5959797cf03b047caaa61a'}  [deleted]   \n",
              "\n",
              "                                            comments created_utc      id  \\\n",
              "0  [Listen to me. \\n\\n5 years down the line, this...  1524926782  8fkbkh   \n",
              "\n",
              "  link_flair_text num_comments  over_18 score  \\\n",
              "0   [R]eddiquette           29    False    50   \n",
              "\n",
              "                                            selftext  \\\n",
              "0  So, I didn't do well in JEE Mains, even after ...   \n",
              "\n",
              "                                          title  \\\n",
              "0  [R] Losing the will to live every single day   \n",
              "\n",
              "                                                 url  \n",
              "0  https://www.reddit.com/r/india/comments/8fkbkh...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "ysN2n1m8c7d_",
        "colab_type": "code",
        "outputId": "2d342115-25d8-4469-fa2d-38b3a1f6bd75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "train['selftext'] = train['selftext'].astype(str)\n",
        "test['selftext'] = test['selftext'].astype(str)\n",
        "\n",
        "empties = ['nan', '[deleted]', '[removed]']\n",
        "for empty in empties:\n",
        "  print(empty, len(train[train['selftext'] == empty]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nan 10262\n",
            "[deleted] 2428\n",
            "[removed] 926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "srR758hrc-12",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train['selftext'] = train['selftext'].apply(lambda x: '' if x in empties else x)\n",
        "test['selftext'] = test['selftext'].apply(lambda x: '' if x in empties else x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nvbrIkU6c_t3",
        "colab_type": "code",
        "outputId": "ffb32231-67f6-440d-cc64-8b0801386148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "empties = ['nan', '[deleted]', '[removed]']\n",
        "for empty in empties:\n",
        "  print(empty, len(train[train['selftext'] == empty]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nan 0\n",
            "[deleted] 0\n",
            "[removed] 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jek9cSqpc_xm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train['title'] = train['title'] + ' ' + train['selftext']\n",
        "test['title'] = test['title'] + ' ' + test['selftext']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2pl923w3S_w8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we will storing the titles and labels in lists. We will then use these to build tsv files required by the DataProcessor."
      ]
    },
    {
      "metadata": {
        "id": "co-F7UF1wZC1",
        "colab_type": "code",
        "outputId": "55520564-2a4d-4dbf-8d1f-e98101a85086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "train_texts = list(train['title'])\n",
        "test_texts = list(test['title'])\n",
        "\n",
        "print (len(train_texts))\n",
        "print (len(test_texts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16486\n",
            "4122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DRqwyjh4a1RW",
        "colab_type": "code",
        "outputId": "a04fe932-52c2-44f1-a3ab-3dcd560ceafb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "train_labels = list()\n",
        "test_labels = list()\n",
        "\n",
        "lables=[]\n",
        "\n",
        "labels = list(train['link_flair_text'])\n",
        "for label in labels:\n",
        "  train_labels.append((label_to_id[label]))\n",
        "  \n",
        "labels =[]\n",
        "labels = list(test['link_flair_text'])\n",
        "for label in labels:\n",
        "  test_labels.append((label_to_id[label]))\n",
        "  \n",
        "print (len(train_labels))\n",
        "print (len(test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16486\n",
            "4122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8oTKtohXTNNu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Normalizing and preprocessing the titles**"
      ]
    },
    {
      "metadata": {
        "id": "5jKhDDVxdUuJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "  \n",
        "train_texts = [normalizeString(i) for i in train_texts]\n",
        "test_texts = [normalizeString(i) for i in test_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n-BcmrHyTSNT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Removing 0 length titles since they carry no information**"
      ]
    },
    {
      "metadata": {
        "id": "RnFkmKJDdfnW",
        "colab_type": "code",
        "outputId": "92161f02-833d-4e48-b114-588f5ca4dbb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "text_lens = Counter([len(x) for x in train_texts])\n",
        "print(\"Zero-length reviews: {}\".format(text_lens[1]))\n",
        "print(\"Maximum review length: {}\".format(max(text_lens)))\n",
        "\n",
        "print('Number of reviews before removing outliers: ', len(train_texts))\n",
        "\n",
        "non_zero_idx = [ii for ii, text in enumerate(train_texts) if len(text) != 1]\n",
        "\n",
        "train_texts = [train_texts[ii] for ii in non_zero_idx]\n",
        "train_labels = np.array([train_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(train_texts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 17\n",
            "Maximum review length: 21558\n",
            "Number of reviews before removing outliers:  16486\n",
            "Number of reviews after removing outliers:  16469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "5829c78d-9b9a-4cf2-9f88-c1cbed3fcd6b",
        "id": "g2pvj0zfSPae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "text_lens = Counter([len(x) for x in test_texts])\n",
        "print(\"Zero-length reviews: {}\".format(text_lens[1]))\n",
        "print(\"Maximum review length: {}\".format(max(text_lens)))\n",
        "print('Number of reviews before removing outliers: ', len(test_texts))\n",
        "\n",
        "non_zero_idx = [ii for ii, text in enumerate(test_texts) if len(text) != 1]\n",
        "\n",
        "test_texts = [test_texts[ii] for ii in non_zero_idx]\n",
        "test_labels = np.array([test_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(test_texts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 0\n",
            "Maximum review length: 23973\n",
            "Number of reviews before removing outliers:  4122\n",
            "Number of reviews after removing outliers:  4122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9VkF4HWmTg7D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Building tsv files used by the DataPreprocessor**"
      ]
    },
    {
      "metadata": {
        "id": "Eoy3AQb7LPEC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf dataset\n",
        "!mkdir dataset/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o07wgyBO2Mb-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"dataset/train.tsv\", \"w\") as f:\n",
        "    f.write(\"sentence    label\\n\")\n",
        "    for text, label in zip(train_texts, train_labels):\n",
        "        f.write(str(text) + \"\\t\" + str(label)+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrLTDWOPf9-5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"dataset/dev.tsv\", \"w\") as f:\n",
        "    f.write(\"sentence    label\\n\")\n",
        "    for text, label in zip(test_texts, test_labels):\n",
        "        f.write(str(text) + \"\\t\" + str(label)+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A6dh8Kgj7Xbd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fine tuning the BERT model for out classification task\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4jjNFFzN8SB1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Implementing Some Helper Functions**"
      ]
    },
    {
      "metadata": {
        "id": "RN-BANcg7Vov",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_to_unicode(text):\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    elif isinstance(text, unicode):\n",
        "      return text\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_WfCSSBK7Vre",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "                \n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        label_id = label_map[example.label]\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_id=label_id))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g4qbWAA17Vwj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m9qDS5Hf7VvD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(out, labels):\n",
        "    outputs = np.argmax(out, axis=1)\n",
        "    return np.sum(outputs == labels)\n",
        "\n",
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0 - x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FpzFuy9371Du",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InputExample(object):\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tsha1Fpp8IOO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataProcessor(object):\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        with open(input_file, \"r\", encoding='utf-8') as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JmHwUXzX71G9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RedditDataProcessor(DataProcessor):\n",
        "\n",
        "  def get_train_examples(self, data_dir):\n",
        "    return self._create_examples(\n",
        "        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "  def get_dev_examples(self, data_dir):\n",
        "    return self._create_examples(\n",
        "        self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
        "\n",
        "  def get_labels(self):\n",
        "    return [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]\n",
        "\n",
        "  def _create_examples(self, lines, set_type):\n",
        "    examples = []\n",
        "    for (i, line) in enumerate(lines):\n",
        "      if i == 0:\n",
        "        continue\n",
        "      guid = \"%s-%s\" % (set_type, i)\n",
        "      if set_type == \"test\":\n",
        "        text_a = convert_to_unicode(line[1])\n",
        "        label = \"0\"\n",
        "      else:\n",
        "        text_a = convert_to_unicode(line[0])\n",
        "        label = convert_to_unicode(line[1])\n",
        "      examples.append(\n",
        "          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RzylppC_AMNL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setting up hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "0zxIMcXN9Fod",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf output/\n",
        "!mkdir output\n",
        "output_dir = 'output'\n",
        "bert_model = 'uncased_L-12_H-768_A-12'\n",
        "data_dir = 'dataset'\n",
        "\n",
        "seed = 42\n",
        "train_batch_size = 16\n",
        "gradient_accumulation_steps = 1\n",
        "max_seq_length = 256\n",
        "learning_rate = 2e-5\n",
        "num_train_epochs = 3.0\n",
        "local_rank = -1\n",
        "warmup_proportion = 0.1\n",
        "eval_batch_size = train_batch_size\n",
        "test_batch_size = train_batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hzxeAIZw8W5l",
        "colab_type": "code",
        "outputId": "5c96d204-d61c-4ff3-962e-83752d89468d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "train_batch_size = int(train_batch_size / gradient_accumulation_steps)\n",
        "\n",
        "logger.info(\"device: {} n_gpu: {}\".format(device, n_gpu))\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/08/2019 05:59:23 - INFO - __main__ -   device: cuda n_gpu: 1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "wcWd8Y8V8W3X",
        "colab_type": "code",
        "outputId": "1ccd6af3-900e-4e64-f401-6eb32ecae8a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "processor = RedditDataProcessor()\n",
        "num_labels = 11 #['0', '1']\n",
        "label_list = processor.get_labels()\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/08/2019 05:59:24 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file uncased_L-12_H-768_A-12/vocab.txt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "8wUn5Ze38W0v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_examples = None\n",
        "num_train_steps = None\n",
        "train_examples = processor.get_train_examples(data_dir)\n",
        "num_train_steps = int(len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h7DmdQ_0_5IG",
        "colab_type": "code",
        "outputId": "9eab1afc-6a54-4e51-b9e1-8038b62f9c30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "# Prepare model\n",
        "model = BertForSequenceClassification.from_pretrained(bert_model,\n",
        "          cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(local_rank),\n",
        "          num_labels = num_labels)\n",
        "\n",
        "model.to(device)\n",
        "model = torch.nn.DataParallel(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/08/2019 05:59:45 - INFO - pytorch_pretrained_bert.modeling -   loading archive file uncased_L-12_H-768_A-12\n",
            "02/08/2019 05:59:45 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "02/08/2019 05:59:51 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "02/08/2019 05:59:51 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "e1IBKpSP_5M-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Prepare optimizer\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "t_total = num_train_steps\n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=learning_rate,\n",
        "                     warmup=warmup_proportion,\n",
        "                     t_total=t_total)\n",
        "\n",
        "global_step = 0\n",
        "nb_tr_steps = 0\n",
        "tr_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWTon9rSBI0J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Start Training Loop"
      ]
    },
    {
      "metadata": {
        "id": "VXFfbxcxBCQV",
        "colab_type": "code",
        "outputId": "0789a10a-666c-449b-d83f-5aaccddf1abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54369
        }
      },
      "cell_type": "code",
      "source": [
        "train_features = convert_examples_to_features(train_examples, label_list, max_seq_length, tokenizer)\n",
        "\n",
        "logger.info(\"***** Running training *****\")\n",
        "logger.info(\"  Num examples = %d\", len(train_examples))\n",
        "logger.info(\"  Batch size = %d\", train_batch_size)\n",
        "logger.info(\"  Num steps = %d\", num_train_steps)\n",
        "\n",
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
        "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, segment_ids, label_ids = batch\n",
        "        loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "        loss.backward()\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        if (nb_tr_steps % 100 == 0):\n",
        "          print ('\\nCurrent Loss', loss.item(), ' Mean Loss: ', tr_loss/ nb_tr_steps)\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            # modify learning rate with special warm up BERT uses\n",
        "            lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_this_step\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/08/2019 06:00:33 - INFO - __main__ -   *** Example ***\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   guid: train-1\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   tokens: [CLS] r losing the will to live every single day so i didn t do well in je ##e main ##s even after taking an year off . it s not even average . i attempted around questions and i think that only rd of them will be correct . i couldn t assemble the courage to even check the answer key . the reason ? i have let my parents down . every parent deserves and hopes to see their child excel . but me ? i ve let them down at every damn moment . the last years they ll and i recall it as a disappointment . it hurts a lot when i cannot fulfill the hopes that they had in my dreams . it hurts a lot when i cannot be that kid from the block who s in the best college of the region . it hurts . i take complete responsibility for where i am right now and what i am . no other person is responsible for my failed and worthless existence i did what i could do but it was far from enough . obviously i failed . but failing twice is a big let down . with each day i get that notion of just running away or just ending everyday . call me coward or es ##cap ##ist but i cannot imagine a victory in this situation . i cry every night and i ap ##olo ##gis ##e to my parents but only when they [SEP]\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   input_ids: 101 1054 3974 1996 2097 2000 2444 2296 2309 2154 2061 1045 2134 1056 2079 2092 1999 15333 2063 2364 2015 2130 2044 2635 2019 2095 2125 1012 2009 1055 2025 2130 2779 1012 1045 4692 2105 3980 1998 1045 2228 2008 2069 16428 1997 2068 2097 2022 6149 1012 1045 2481 1056 21365 1996 8424 2000 2130 4638 1996 3437 3145 1012 1996 3114 1029 1045 2031 2292 2026 3008 2091 1012 2296 6687 17210 1998 8069 2000 2156 2037 2775 24970 1012 2021 2033 1029 1045 2310 2292 2068 2091 2012 2296 4365 2617 1012 1996 2197 2086 2027 2222 1998 1045 9131 2009 2004 1037 10520 1012 2009 13403 1037 2843 2043 1045 3685 13883 1996 8069 2008 2027 2018 1999 2026 5544 1012 2009 13403 1037 2843 2043 1045 3685 2022 2008 4845 2013 1996 3796 2040 1055 1999 1996 2190 2267 1997 1996 2555 1012 2009 13403 1012 1045 2202 3143 5368 2005 2073 1045 2572 2157 2085 1998 2054 1045 2572 1012 2053 2060 2711 2003 3625 2005 2026 3478 1998 22692 4598 1045 2106 2054 1045 2071 2079 2021 2009 2001 2521 2013 2438 1012 5525 1045 3478 1012 2021 7989 3807 2003 1037 2502 2292 2091 1012 2007 2169 2154 1045 2131 2008 9366 1997 2074 2770 2185 2030 2074 4566 10126 1012 2655 2033 16592 2030 9686 17695 2923 2021 1045 3685 5674 1037 3377 1999 2023 3663 1012 1045 5390 2296 2305 1998 1045 9706 12898 17701 2063 2000 2026 3008 2021 2069 2043 2027 102\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   label: 10 (id = 10)\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   *** Example ***\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   guid: train-2\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   tokens: [CLS] random daily discussion thread for late night owls and early morning worship ##ers come on rand ##ia . let us bring back the rd ##d ! also let me use this thread to encourage everyone to emi ##grate . sorry [SEP]\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   input_ids: 101 6721 3679 6594 11689 2005 2397 2305 22388 1998 2220 2851 7425 2545 2272 2006 14566 2401 1012 2292 2149 3288 2067 1996 16428 2094 999 2036 2292 2033 2224 2023 11689 2000 8627 3071 2000 12495 22780 1012 3374 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   label: 7 (id = 7)\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   *** Example ***\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   guid: train-3\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   tokens: [CLS] i was doing some research and came across this data about indian s internet interests w ##tf ! ! [SEP]\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   input_ids: 101 1045 2001 2725 2070 2470 1998 2234 2408 2023 2951 2055 2796 1055 4274 5426 1059 24475 999 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   label: 8 (id = 8)\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   *** Example ***\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   guid: train-4\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   tokens: [CLS] need some advice pay ##tm and domino s taking me for a ride . so i had a failed order on the domino s website weeks ago and i paid through the pay ##tm wallet . the amount was de ##ducted the the order never went through and so i got an automatic mail from domino ##s regarding the ref ##und of the amount . some days after the incident i checked if i had received the ref ##und and i hadn ##t so i called up the customer care at domino ##s and they said that they have ref ##und ##ed my money already . so i checked up with pay ##tm and they say they haven ##t received any ref ##und on that order . a lot of phone ##cal ##ls and emails later i m still stuck in between two companies who are saying the opposite things and there doesn ##t seem to be a resolution in sight . i have rec ##ie ##ved screens ##hot ##s from domino ##s showing the ref ##und processed from their end but pay ##tm is still maintaining they haven ##t received any amount . they have no customer care representative to talk to and the only way is through the dumb tickets on their website which seem to get handled by a new person every ##time . any suggestions on how i should proceed ? [SEP]\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   input_ids: 101 2342 2070 6040 3477 21246 1998 23968 1055 2635 2033 2005 1037 4536 1012 2061 1045 2018 1037 3478 2344 2006 1996 23968 1055 4037 3134 3283 1998 1045 3825 2083 1996 3477 21246 15882 1012 1996 3815 2001 2139 29510 1996 1996 2344 2196 2253 2083 1998 2061 1045 2288 2019 6882 5653 2013 23968 2015 4953 1996 25416 8630 1997 1996 3815 1012 2070 2420 2044 1996 5043 1045 7039 2065 1045 2018 2363 1996 25416 8630 1998 1045 2910 2102 2061 1045 2170 2039 1996 8013 2729 2012 23968 2015 1998 2027 2056 2008 2027 2031 25416 8630 2098 2026 2769 2525 1012 2061 1045 7039 2039 2007 3477 21246 1998 2027 2360 2027 4033 2102 2363 2151 25416 8630 2006 2008 2344 1012 1037 2843 1997 3042 9289 4877 1998 22028 2101 1045 1049 2145 5881 1999 2090 2048 3316 2040 2024 3038 1996 4500 2477 1998 2045 2987 2102 4025 2000 2022 1037 5813 1999 4356 1012 1045 2031 28667 2666 7178 12117 12326 2015 2013 23968 2015 4760 1996 25416 8630 13995 2013 2037 2203 2021 3477 21246 2003 2145 8498 2027 4033 2102 2363 2151 3815 1012 2027 2031 2053 8013 2729 4387 2000 2831 2000 1998 1996 2069 2126 2003 2083 1996 12873 9735 2006 2037 4037 2029 4025 2000 2131 8971 2011 1037 2047 2711 2296 7292 1012 2151 15690 2006 2129 1045 2323 10838 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   label: 10 (id = 10)\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   *** Example ***\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   guid: train-5\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   tokens: [CLS] weekly code ##rs hacker ##s amp all tech related thread last week s issue https np . red ##dit . com r india comments ck ##q b all threads https www . red ##dit . com r india search ? q weekly code ##rs c hacker ##s all tech related thread author aa ##vina ##ss ##h amp amp restrict sr on amp amp sort new amp amp t all every week on saturday i will post this thread . feel free to discuss anything related to hacking coding startup ##s etc . share your gi ##th ##ub project show off your di ##y project etc . so post anything that interests to hacker ##s and tin ##ker ##ers . let me know if you have some suggestions or anything you want to add to op . the thread will be posted on every saturday . pm . we now have a slack channel . join now ! http slack . dev ##up . in . [SEP]\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   input_ids: 101 4882 3642 2869 23307 2015 23713 2035 6627 3141 11689 2197 2733 1055 3277 16770 27937 1012 2417 23194 1012 4012 1054 2634 7928 23616 4160 1038 2035 16457 16770 7479 1012 2417 23194 1012 4012 1054 2634 3945 1029 1053 4882 3642 2869 1039 23307 2015 2035 6627 3141 11689 3166 9779 17948 4757 2232 23713 23713 21573 5034 2006 23713 23713 4066 2047 23713 23713 1056 2035 2296 2733 2006 5095 1045 2097 2695 2023 11689 1012 2514 2489 2000 6848 2505 3141 2000 23707 16861 22752 2015 4385 1012 3745 2115 21025 2705 12083 2622 2265 2125 2115 4487 2100 2622 4385 1012 2061 2695 2505 2008 5426 2000 23307 2015 1998 9543 5484 2545 1012 2292 2033 2113 2065 2017 2031 2070 15690 2030 2505 2017 2215 2000 5587 2000 6728 1012 1996 11689 2097 2022 6866 2006 2296 5095 1012 7610 1012 2057 2085 2031 1037 19840 3149 1012 3693 2085 999 8299 19840 1012 16475 6279 1012 1999 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 06:00:33 - INFO - __main__ -   label: 7 (id = 7)\n",
            "02/08/2019 06:00:49 - INFO - __main__ -   ***** Running training *****\n",
            "02/08/2019 06:00:49 - INFO - __main__ -     Num examples = 16469\n",
            "02/08/2019 06:00:49 - INFO - __main__ -     Batch size = 16\n",
            "02/08/2019 06:00:49 - INFO - __main__ -     Num steps = 3087\n",
            "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/1030 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 1/1030 [00:01<29:52,  1.74s/it]\u001b[A\n",
            "Iteration:   0%|          | 2/1030 [00:03<28:59,  1.69s/it]\u001b[A\n",
            "Iteration:   0%|          | 3/1030 [00:04<28:11,  1.65s/it]\u001b[A\n",
            "Iteration:   0%|          | 4/1030 [00:06<27:41,  1.62s/it]\u001b[A\n",
            "Iteration:   0%|          | 5/1030 [00:07<27:17,  1.60s/it]\u001b[A\n",
            "Iteration:   1%|          | 6/1030 [00:09<27:26,  1.61s/it]\u001b[A\n",
            "Iteration:   1%|          | 7/1030 [00:11<27:09,  1.59s/it]\u001b[A\n",
            "Iteration:   1%|          | 8/1030 [00:12<26:52,  1.58s/it]\u001b[A\n",
            "Iteration:   1%|          | 9/1030 [00:14<26:42,  1.57s/it]\u001b[A\n",
            "Iteration:   1%|          | 10/1030 [00:15<26:37,  1.57s/it]\u001b[A\n",
            "Iteration:   1%|          | 11/1030 [00:17<26:32,  1.56s/it]\u001b[A\n",
            "Iteration:   1%|          | 12/1030 [00:18<26:24,  1.56s/it]\u001b[A\n",
            "Iteration:   1%|▏         | 13/1030 [00:20<26:18,  1.55s/it]\u001b[A\n",
            "Iteration:   1%|▏         | 14/1030 [00:22<26:20,  1.56s/it]\u001b[A\n",
            "Iteration:   1%|▏         | 15/1030 [00:23<26:16,  1.55s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 16/1030 [00:25<26:13,  1.55s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 17/1030 [00:26<26:11,  1.55s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 18/1030 [00:28<26:11,  1.55s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 19/1030 [00:29<26:09,  1.55s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 20/1030 [00:31<26:13,  1.56s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 21/1030 [00:32<26:13,  1.56s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 22/1030 [00:34<26:13,  1.56s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 23/1030 [00:36<26:11,  1.56s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 24/1030 [00:37<26:11,  1.56s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 25/1030 [00:39<26:13,  1.57s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 26/1030 [00:40<26:17,  1.57s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 27/1030 [00:42<26:08,  1.56s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 28/1030 [00:43<26:06,  1.56s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 29/1030 [00:45<26:04,  1.56s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 30/1030 [00:46<26:06,  1.57s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 31/1030 [00:48<26:01,  1.56s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 32/1030 [00:50<25:59,  1.56s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 33/1030 [00:51<25:56,  1.56s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 34/1030 [00:53<26:19,  1.59s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 35/1030 [00:54<26:14,  1.58s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 36/1030 [00:56<26:29,  1.60s/it]\u001b[A\n",
            "Iteration:   4%|▎         | 37/1030 [00:58<26:16,  1.59s/it]\u001b[A\n",
            "Iteration:   4%|▎         | 38/1030 [00:59<26:09,  1.58s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 39/1030 [01:01<26:00,  1.57s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 40/1030 [01:02<25:55,  1.57s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 41/1030 [01:04<25:53,  1.57s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 42/1030 [01:05<25:48,  1.57s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 43/1030 [01:07<25:46,  1.57s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 44/1030 [01:09<25:43,  1.57s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 45/1030 [01:10<25:42,  1.57s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 46/1030 [01:12<25:38,  1.56s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 47/1030 [01:13<25:40,  1.57s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 48/1030 [01:15<25:40,  1.57s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 49/1030 [01:16<25:39,  1.57s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 50/1030 [01:18<25:38,  1.57s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 51/1030 [01:20<25:36,  1.57s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 52/1030 [01:21<25:34,  1.57s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 53/1030 [01:23<25:33,  1.57s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 54/1030 [01:24<25:29,  1.57s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 55/1030 [01:26<25:30,  1.57s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 56/1030 [01:27<25:29,  1.57s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 57/1030 [01:29<25:29,  1.57s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 58/1030 [01:31<25:27,  1.57s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 59/1030 [01:32<25:21,  1.57s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 60/1030 [01:34<25:22,  1.57s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 61/1030 [01:35<25:18,  1.57s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 62/1030 [01:37<25:18,  1.57s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 63/1030 [01:38<25:17,  1.57s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 64/1030 [01:40<25:20,  1.57s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 65/1030 [01:41<25:15,  1.57s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 66/1030 [01:43<25:42,  1.60s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 67/1030 [01:45<25:33,  1.59s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 68/1030 [01:46<25:26,  1.59s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 69/1030 [01:48<25:21,  1.58s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 70/1030 [01:49<25:20,  1.58s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 71/1030 [01:51<25:12,  1.58s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 72/1030 [01:53<25:12,  1.58s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 73/1030 [01:54<25:07,  1.58s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 74/1030 [01:56<25:18,  1.59s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 75/1030 [01:57<25:19,  1.59s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 76/1030 [01:59<25:11,  1.58s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 77/1030 [02:01<25:08,  1.58s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 78/1030 [02:02<25:08,  1.58s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 79/1030 [02:04<25:24,  1.60s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 80/1030 [02:05<25:15,  1.59s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 81/1030 [02:07<25:04,  1.59s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 82/1030 [02:09<25:11,  1.59s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 83/1030 [02:10<25:03,  1.59s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 84/1030 [02:12<24:56,  1.58s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 85/1030 [02:13<24:54,  1.58s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 86/1030 [02:15<24:48,  1.58s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 87/1030 [02:16<24:42,  1.57s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 88/1030 [02:18<24:43,  1.57s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 89/1030 [02:20<24:40,  1.57s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 90/1030 [02:21<24:43,  1.58s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 91/1030 [02:23<24:43,  1.58s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 92/1030 [02:24<24:42,  1.58s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 93/1030 [02:26<24:36,  1.58s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 94/1030 [02:27<24:31,  1.57s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 95/1030 [02:29<24:30,  1.57s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 96/1030 [02:31<24:49,  1.59s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 97/1030 [02:32<24:38,  1.59s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 98/1030 [02:34<24:31,  1.58s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 99/1030 [02:35<24:25,  1.57s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 100/1030 [02:37<24:25,  1.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 2.329841136932373  Mean Loss:  2.4236915349960326\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  10%|▉         | 101/1030 [02:38<24:26,  1.58s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 102/1030 [02:40<24:24,  1.58s/it]\u001b[A\n",
            "Iteration:  10%|█         | 103/1030 [02:42<24:20,  1.58s/it]\u001b[A\n",
            "Iteration:  10%|█         | 104/1030 [02:43<24:13,  1.57s/it]\u001b[A\n",
            "Iteration:  10%|█         | 105/1030 [02:45<24:13,  1.57s/it]\u001b[A\n",
            "Iteration:  10%|█         | 106/1030 [02:46<24:13,  1.57s/it]\u001b[A\n",
            "Iteration:  10%|█         | 107/1030 [02:48<24:12,  1.57s/it]\u001b[A\n",
            "Iteration:  10%|█         | 108/1030 [02:49<24:08,  1.57s/it]\u001b[A\n",
            "Iteration:  11%|█         | 109/1030 [02:51<24:12,  1.58s/it]\u001b[A\n",
            "Iteration:  11%|█         | 110/1030 [02:53<24:09,  1.58s/it]\u001b[A\n",
            "Iteration:  11%|█         | 111/1030 [02:54<24:08,  1.58s/it]\u001b[A\n",
            "Iteration:  11%|█         | 112/1030 [02:56<24:02,  1.57s/it]\u001b[A\n",
            "Iteration:  11%|█         | 113/1030 [02:57<24:04,  1.58s/it]\u001b[A\n",
            "Iteration:  11%|█         | 114/1030 [02:59<24:01,  1.57s/it]\u001b[A\n",
            "Iteration:  11%|█         | 115/1030 [03:01<24:23,  1.60s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 116/1030 [03:02<24:14,  1.59s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 117/1030 [03:04<24:42,  1.62s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 118/1030 [03:06<24:54,  1.64s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 119/1030 [03:07<24:34,  1.62s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 120/1030 [03:09<24:17,  1.60s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 121/1030 [03:10<24:07,  1.59s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 122/1030 [03:12<23:59,  1.59s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 123/1030 [03:13<24:21,  1.61s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 124/1030 [03:15<24:13,  1.60s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 125/1030 [03:17<24:43,  1.64s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 126/1030 [03:18<24:24,  1.62s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 127/1030 [03:20<24:09,  1.61s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 128/1030 [03:22<24:05,  1.60s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 129/1030 [03:23<24:04,  1.60s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 130/1030 [03:25<23:53,  1.59s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 131/1030 [03:26<23:44,  1.58s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 132/1030 [03:28<23:41,  1.58s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 133/1030 [03:30<24:06,  1.61s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 134/1030 [03:31<23:57,  1.60s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 135/1030 [03:33<23:47,  1.59s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 136/1030 [03:34<24:06,  1.62s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 137/1030 [03:36<23:52,  1.60s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 138/1030 [03:38<23:43,  1.60s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 139/1030 [03:39<23:34,  1.59s/it]\u001b[A\n",
            "Iteration:  14%|█▎        | 140/1030 [03:41<23:27,  1.58s/it]\u001b[A\n",
            "Iteration:  14%|█▎        | 141/1030 [03:42<23:22,  1.58s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 142/1030 [03:44<23:22,  1.58s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 143/1030 [03:45<23:19,  1.58s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 144/1030 [03:47<23:17,  1.58s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 145/1030 [03:49<23:11,  1.57s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 146/1030 [03:50<23:10,  1.57s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 147/1030 [03:52<23:06,  1.57s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 148/1030 [03:53<23:28,  1.60s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 149/1030 [03:55<23:18,  1.59s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 150/1030 [03:57<23:39,  1.61s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 151/1030 [03:58<23:29,  1.60s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 152/1030 [04:00<23:19,  1.59s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 153/1030 [04:01<23:11,  1.59s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 154/1030 [04:03<23:05,  1.58s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 155/1030 [04:04<23:03,  1.58s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 156/1030 [04:06<23:00,  1.58s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 157/1030 [04:08<22:59,  1.58s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 158/1030 [04:09<22:55,  1.58s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 159/1030 [04:11<22:51,  1.57s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 160/1030 [04:12<22:46,  1.57s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 161/1030 [04:14<22:44,  1.57s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 162/1030 [04:15<22:43,  1.57s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 163/1030 [04:17<22:43,  1.57s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 164/1030 [04:19<22:41,  1.57s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 165/1030 [04:20<22:39,  1.57s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 166/1030 [04:22<22:36,  1.57s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 167/1030 [04:23<22:55,  1.59s/it]\u001b[A\n",
            "Iteration:  16%|█▋        | 168/1030 [04:25<22:53,  1.59s/it]\u001b[A\n",
            "Iteration:  16%|█▋        | 169/1030 [04:27<23:13,  1.62s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 170/1030 [04:28<23:02,  1.61s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 171/1030 [04:30<22:57,  1.60s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 172/1030 [04:31<22:51,  1.60s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 173/1030 [04:33<22:41,  1.59s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 174/1030 [04:35<22:38,  1.59s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 175/1030 [04:36<22:32,  1.58s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 176/1030 [04:38<22:32,  1.58s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 177/1030 [04:39<22:27,  1.58s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 178/1030 [04:41<22:29,  1.58s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 179/1030 [04:42<22:25,  1.58s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 180/1030 [04:44<22:22,  1.58s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 181/1030 [04:46<22:19,  1.58s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 182/1030 [04:47<22:15,  1.58s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 183/1030 [04:49<22:15,  1.58s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 184/1030 [04:50<22:32,  1.60s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 185/1030 [04:52<22:24,  1.59s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 186/1030 [04:54<22:14,  1.58s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 187/1030 [04:55<22:08,  1.58s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 188/1030 [04:57<22:07,  1.58s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 189/1030 [04:58<22:05,  1.58s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 190/1030 [05:00<22:03,  1.58s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 191/1030 [05:01<22:00,  1.57s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 192/1030 [05:03<22:00,  1.58s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 193/1030 [05:05<22:00,  1.58s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 194/1030 [05:06<22:16,  1.60s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 195/1030 [05:08<22:07,  1.59s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 196/1030 [05:09<22:05,  1.59s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 197/1030 [05:11<21:56,  1.58s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 198/1030 [05:12<21:51,  1.58s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 199/1030 [05:14<22:11,  1.60s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 200/1030 [05:16<22:02,  1.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.9508038759231567  Mean Loss:  2.2773089945316314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  20%|█▉        | 201/1030 [05:17<22:20,  1.62s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 202/1030 [05:19<22:09,  1.61s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 203/1030 [05:21<22:03,  1.60s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 204/1030 [05:22<21:57,  1.60s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 205/1030 [05:24<21:54,  1.59s/it]\u001b[A\n",
            "Iteration:  20%|██        | 206/1030 [05:25<21:46,  1.59s/it]\u001b[A\n",
            "Iteration:  20%|██        | 207/1030 [05:27<21:45,  1.59s/it]\u001b[A\n",
            "Iteration:  20%|██        | 208/1030 [05:28<21:38,  1.58s/it]\u001b[A\n",
            "Iteration:  20%|██        | 209/1030 [05:30<21:36,  1.58s/it]\u001b[A\n",
            "Iteration:  20%|██        | 210/1030 [05:32<21:31,  1.57s/it]\u001b[A\n",
            "Iteration:  20%|██        | 211/1030 [05:33<21:48,  1.60s/it]\u001b[A\n",
            "Iteration:  21%|██        | 212/1030 [05:35<21:40,  1.59s/it]\u001b[A\n",
            "Iteration:  21%|██        | 213/1030 [05:36<22:04,  1.62s/it]\u001b[A\n",
            "Iteration:  21%|██        | 214/1030 [05:38<21:49,  1.61s/it]\u001b[A\n",
            "Iteration:  21%|██        | 215/1030 [05:40<21:40,  1.60s/it]\u001b[A\n",
            "Iteration:  21%|██        | 216/1030 [05:41<21:32,  1.59s/it]\u001b[A\n",
            "Iteration:  21%|██        | 217/1030 [05:43<21:27,  1.58s/it]\u001b[A\n",
            "Iteration:  21%|██        | 218/1030 [05:44<21:23,  1.58s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 219/1030 [05:46<21:19,  1.58s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 220/1030 [05:47<21:13,  1.57s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 221/1030 [05:49<21:10,  1.57s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 222/1030 [05:51<21:11,  1.57s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 223/1030 [05:52<21:11,  1.58s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 224/1030 [05:54<21:12,  1.58s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 225/1030 [05:55<21:08,  1.58s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 226/1030 [05:57<21:08,  1.58s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 227/1030 [05:59<21:05,  1.58s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 228/1030 [06:00<21:23,  1.60s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 229/1030 [06:02<21:10,  1.59s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 230/1030 [06:03<21:28,  1.61s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 231/1030 [06:05<21:20,  1.60s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 232/1030 [06:07<21:12,  1.59s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 233/1030 [06:08<21:05,  1.59s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 234/1030 [06:10<20:59,  1.58s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 235/1030 [06:11<20:55,  1.58s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 236/1030 [06:13<20:51,  1.58s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 237/1030 [06:15<21:13,  1.61s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 238/1030 [06:16<21:04,  1.60s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 239/1030 [06:18<20:57,  1.59s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 240/1030 [06:19<20:51,  1.58s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 241/1030 [06:21<20:50,  1.59s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 242/1030 [06:22<20:45,  1.58s/it]\u001b[A\n",
            "Iteration:  24%|██▎       | 243/1030 [06:24<20:41,  1.58s/it]\u001b[A\n",
            "Iteration:  24%|██▎       | 244/1030 [06:26<20:36,  1.57s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 245/1030 [06:27<20:34,  1.57s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 246/1030 [06:29<20:31,  1.57s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 247/1030 [06:30<20:33,  1.58s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 248/1030 [06:32<20:29,  1.57s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 249/1030 [06:33<20:48,  1.60s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 250/1030 [06:35<20:39,  1.59s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 251/1030 [06:37<20:34,  1.58s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 252/1030 [06:38<20:31,  1.58s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 253/1030 [06:40<20:25,  1.58s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 254/1030 [06:41<20:47,  1.61s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 255/1030 [06:43<20:36,  1.60s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 256/1030 [06:45<20:33,  1.59s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 257/1030 [06:46<20:28,  1.59s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 258/1030 [06:48<20:28,  1.59s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 259/1030 [06:49<20:23,  1.59s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 260/1030 [06:51<20:20,  1.58s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 261/1030 [06:52<20:15,  1.58s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 262/1030 [06:54<20:31,  1.60s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 263/1030 [06:56<20:23,  1.59s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 264/1030 [06:57<20:38,  1.62s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 265/1030 [06:59<20:23,  1.60s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 266/1030 [07:01<20:16,  1.59s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 267/1030 [07:02<20:27,  1.61s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 268/1030 [07:04<20:20,  1.60s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 269/1030 [07:05<20:12,  1.59s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 270/1030 [07:07<20:06,  1.59s/it]\u001b[A\n",
            "Iteration:  26%|██▋       | 271/1030 [07:08<19:59,  1.58s/it]\u001b[A\n",
            "Iteration:  26%|██▋       | 272/1030 [07:10<19:55,  1.58s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 273/1030 [07:12<19:52,  1.57s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 274/1030 [07:13<19:51,  1.58s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 275/1030 [07:15<19:50,  1.58s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 276/1030 [07:16<19:46,  1.57s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 277/1030 [07:18<19:44,  1.57s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 278/1030 [07:19<19:47,  1.58s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 279/1030 [07:21<19:46,  1.58s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 280/1030 [07:23<19:43,  1.58s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 281/1030 [07:24<20:04,  1.61s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 282/1030 [07:26<19:54,  1.60s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 283/1030 [07:27<19:48,  1.59s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 284/1030 [07:29<19:57,  1.61s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 285/1030 [07:31<19:50,  1.60s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 286/1030 [07:32<20:06,  1.62s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 287/1030 [07:34<19:55,  1.61s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 288/1030 [07:36<19:46,  1.60s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 289/1030 [07:37<19:38,  1.59s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 290/1030 [07:39<19:38,  1.59s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 291/1030 [07:40<19:33,  1.59s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 292/1030 [07:42<19:29,  1.58s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 293/1030 [07:43<19:24,  1.58s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 294/1030 [07:45<19:23,  1.58s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 295/1030 [07:47<19:19,  1.58s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 296/1030 [07:48<19:33,  1.60s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 297/1030 [07:50<19:25,  1.59s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 298/1030 [07:51<19:42,  1.62s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 299/1030 [07:53<19:30,  1.60s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 300/1030 [07:55<19:23,  1.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.4177407026290894  Mean Loss:  2.0517717170715333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  29%|██▉       | 301/1030 [07:56<19:17,  1.59s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 302/1030 [07:58<19:11,  1.58s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 303/1030 [07:59<19:06,  1.58s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 304/1030 [08:01<19:04,  1.58s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 305/1030 [08:02<19:01,  1.57s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 306/1030 [08:04<18:58,  1.57s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 307/1030 [08:06<18:57,  1.57s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 308/1030 [08:07<18:58,  1.58s/it]\u001b[A\n",
            "Iteration:  30%|███       | 309/1030 [08:09<18:58,  1.58s/it]\u001b[A\n",
            "Iteration:  30%|███       | 310/1030 [08:10<18:54,  1.58s/it]\u001b[A\n",
            "Iteration:  30%|███       | 311/1030 [08:12<18:53,  1.58s/it]\u001b[A\n",
            "Iteration:  30%|███       | 312/1030 [08:13<18:51,  1.58s/it]\u001b[A\n",
            "Iteration:  30%|███       | 313/1030 [08:15<18:49,  1.58s/it]\u001b[A\n",
            "Iteration:  30%|███       | 314/1030 [08:17<18:49,  1.58s/it]\u001b[A\n",
            "Iteration:  31%|███       | 315/1030 [08:18<19:06,  1.60s/it]\u001b[A\n",
            "Iteration:  31%|███       | 316/1030 [08:20<18:59,  1.60s/it]\u001b[A\n",
            "Iteration:  31%|███       | 317/1030 [08:21<18:53,  1.59s/it]\u001b[A\n",
            "Iteration:  31%|███       | 318/1030 [08:23<18:48,  1.59s/it]\u001b[A\n",
            "Iteration:  31%|███       | 319/1030 [08:25<18:43,  1.58s/it]\u001b[A\n",
            "Iteration:  31%|███       | 320/1030 [08:26<19:01,  1.61s/it]\u001b[A\n",
            "Iteration:  31%|███       | 321/1030 [08:28<18:50,  1.59s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 322/1030 [08:29<18:42,  1.59s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 323/1030 [08:31<18:38,  1.58s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 324/1030 [08:33<18:41,  1.59s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 325/1030 [08:34<18:42,  1.59s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 326/1030 [08:36<19:17,  1.64s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 327/1030 [08:38<19:02,  1.63s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 328/1030 [08:39<18:56,  1.62s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 329/1030 [08:41<18:49,  1.61s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 330/1030 [08:43<19:35,  1.68s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 331/1030 [08:44<20:09,  1.73s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 332/1030 [08:46<19:39,  1.69s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 333/1030 [08:48<20:09,  1.74s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 334/1030 [08:49<19:37,  1.69s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 335/1030 [08:51<19:20,  1.67s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 336/1030 [08:53<19:14,  1.66s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 337/1030 [08:54<19:09,  1.66s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 338/1030 [08:56<18:59,  1.65s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 339/1030 [08:58<18:54,  1.64s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 340/1030 [08:59<18:52,  1.64s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 341/1030 [09:01<18:44,  1.63s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 342/1030 [09:03<18:43,  1.63s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 343/1030 [09:04<18:44,  1.64s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 344/1030 [09:06<18:42,  1.64s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 345/1030 [09:08<18:57,  1.66s/it]\u001b[A\n",
            "Iteration:  34%|███▎      | 346/1030 [09:09<18:53,  1.66s/it]\u001b[A\n",
            "Iteration:  34%|███▎      | 347/1030 [09:11<18:42,  1.64s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 348/1030 [09:13<19:07,  1.68s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 349/1030 [09:14<18:56,  1.67s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 350/1030 [09:16<18:45,  1.65s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 351/1030 [09:18<19:15,  1.70s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 352/1030 [09:19<19:17,  1.71s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 353/1030 [09:21<19:09,  1.70s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 354/1030 [09:23<18:43,  1.66s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 355/1030 [09:24<18:32,  1.65s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 356/1030 [09:26<18:19,  1.63s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 357/1030 [09:27<18:15,  1.63s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 358/1030 [09:29<18:37,  1.66s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 359/1030 [09:31<18:51,  1.69s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 360/1030 [09:33<19:02,  1.71s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 361/1030 [09:34<19:10,  1.72s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 362/1030 [09:36<18:54,  1.70s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 363/1030 [09:38<18:48,  1.69s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 364/1030 [09:39<18:29,  1.67s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 365/1030 [09:41<18:45,  1.69s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 366/1030 [09:43<18:37,  1.68s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 367/1030 [09:44<18:39,  1.69s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 368/1030 [09:46<18:18,  1.66s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 369/1030 [09:48<18:02,  1.64s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 370/1030 [09:49<18:10,  1.65s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 371/1030 [09:51<18:24,  1.68s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 372/1030 [09:53<18:54,  1.72s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 373/1030 [09:54<18:28,  1.69s/it]\u001b[A\n",
            "Iteration:  36%|███▋      | 374/1030 [09:56<18:23,  1.68s/it]\u001b[A\n",
            "Iteration:  36%|███▋      | 375/1030 [09:58<18:05,  1.66s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 376/1030 [09:59<17:53,  1.64s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 377/1030 [10:01<18:31,  1.70s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 378/1030 [10:03<18:56,  1.74s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 379/1030 [10:05<18:27,  1.70s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 380/1030 [10:06<18:06,  1.67s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 381/1030 [10:08<18:21,  1.70s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 382/1030 [10:10<17:59,  1.67s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 383/1030 [10:11<18:01,  1.67s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 384/1030 [10:13<18:50,  1.75s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 385/1030 [10:15<18:21,  1.71s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 386/1030 [10:17<18:26,  1.72s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 387/1030 [10:18<18:02,  1.68s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 388/1030 [10:20<17:45,  1.66s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 389/1030 [10:21<17:31,  1.64s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 390/1030 [10:23<17:16,  1.62s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 391/1030 [10:25<17:14,  1.62s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 392/1030 [10:26<17:05,  1.61s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 393/1030 [10:28<17:16,  1.63s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 394/1030 [10:29<17:12,  1.62s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 395/1030 [10:31<17:22,  1.64s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 396/1030 [10:33<17:24,  1.65s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 397/1030 [10:34<17:25,  1.65s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 398/1030 [10:36<17:27,  1.66s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 399/1030 [10:38<17:29,  1.66s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 400/1030 [10:40<18:03,  1.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.331984281539917  Mean Loss:  1.8880588126182556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  39%|███▉      | 401/1030 [10:41<17:39,  1.68s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 402/1030 [10:43<17:21,  1.66s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 403/1030 [10:44<17:23,  1.66s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 404/1030 [10:46<17:10,  1.65s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 405/1030 [10:48<17:10,  1.65s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 406/1030 [10:49<17:01,  1.64s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 407/1030 [10:51<16:53,  1.63s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 408/1030 [10:53<16:42,  1.61s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 409/1030 [10:54<16:36,  1.61s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 410/1030 [10:56<16:48,  1.63s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 411/1030 [10:58<17:09,  1.66s/it]\u001b[A\n",
            "Iteration:  40%|████      | 412/1030 [10:59<16:58,  1.65s/it]\u001b[A\n",
            "Iteration:  40%|████      | 413/1030 [11:01<17:22,  1.69s/it]\u001b[A\n",
            "Iteration:  40%|████      | 414/1030 [11:03<17:03,  1.66s/it]\u001b[A\n",
            "Iteration:  40%|████      | 415/1030 [11:04<16:50,  1.64s/it]\u001b[A\n",
            "Iteration:  40%|████      | 416/1030 [11:06<16:41,  1.63s/it]\u001b[A\n",
            "Iteration:  40%|████      | 417/1030 [11:07<16:38,  1.63s/it]\u001b[A\n",
            "Iteration:  41%|████      | 418/1030 [11:09<16:29,  1.62s/it]\u001b[A\n",
            "Iteration:  41%|████      | 419/1030 [11:11<16:36,  1.63s/it]\u001b[A\n",
            "Iteration:  41%|████      | 420/1030 [11:12<16:32,  1.63s/it]\u001b[A\n",
            "Iteration:  41%|████      | 421/1030 [11:14<16:22,  1.61s/it]\u001b[A\n",
            "Iteration:  41%|████      | 422/1030 [11:15<16:21,  1.61s/it]\u001b[A\n",
            "Iteration:  41%|████      | 423/1030 [11:17<16:46,  1.66s/it]\u001b[A\n",
            "Iteration:  41%|████      | 424/1030 [11:19<17:01,  1.69s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 425/1030 [11:21<16:45,  1.66s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 426/1030 [11:22<16:32,  1.64s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 427/1030 [11:24<16:23,  1.63s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 428/1030 [11:25<16:15,  1.62s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 429/1030 [11:27<16:23,  1.64s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 430/1030 [11:29<16:39,  1.67s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 431/1030 [11:30<16:26,  1.65s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 432/1030 [11:32<16:31,  1.66s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 433/1030 [11:34<17:02,  1.71s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 434/1030 [11:36<16:39,  1.68s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 435/1030 [11:37<16:39,  1.68s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 436/1030 [11:39<16:22,  1.65s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 437/1030 [11:40<16:13,  1.64s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 438/1030 [11:42<16:06,  1.63s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 439/1030 [11:44<15:59,  1.62s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 440/1030 [11:45<15:51,  1.61s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 441/1030 [11:47<15:58,  1.63s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 442/1030 [11:48<15:52,  1.62s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 443/1030 [11:50<15:59,  1.63s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 444/1030 [11:52<15:47,  1.62s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 445/1030 [11:53<15:47,  1.62s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 446/1030 [11:55<15:42,  1.61s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 447/1030 [11:57<15:56,  1.64s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 448/1030 [11:58<15:59,  1.65s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 449/1030 [12:00<15:53,  1.64s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 450/1030 [12:02<15:43,  1.63s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 451/1030 [12:03<15:43,  1.63s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 452/1030 [12:05<15:42,  1.63s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 453/1030 [12:06<15:41,  1.63s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 454/1030 [12:08<15:36,  1.63s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 455/1030 [12:10<15:30,  1.62s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 456/1030 [12:11<15:47,  1.65s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 457/1030 [12:13<15:45,  1.65s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 458/1030 [12:15<15:43,  1.65s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 459/1030 [12:16<15:56,  1.68s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 460/1030 [12:18<15:48,  1.66s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 461/1030 [12:20<15:57,  1.68s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 462/1030 [12:21<15:49,  1.67s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 463/1030 [12:23<15:39,  1.66s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 464/1030 [12:25<15:33,  1.65s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 465/1030 [12:26<15:47,  1.68s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 466/1030 [12:28<15:40,  1.67s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 467/1030 [12:30<15:36,  1.66s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 468/1030 [12:31<15:45,  1.68s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 469/1030 [12:33<16:10,  1.73s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 470/1030 [12:35<15:52,  1.70s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 471/1030 [12:37<15:52,  1.70s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 472/1030 [12:38<15:37,  1.68s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 473/1030 [12:40<15:24,  1.66s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 474/1030 [12:42<15:21,  1.66s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 475/1030 [12:43<15:13,  1.65s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 476/1030 [12:45<15:13,  1.65s/it]\u001b[A\n",
            "Iteration:  46%|████▋     | 477/1030 [12:46<15:12,  1.65s/it]\u001b[A\n",
            "Iteration:  46%|████▋     | 478/1030 [12:48<15:09,  1.65s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 479/1030 [12:50<15:19,  1.67s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 480/1030 [12:51<15:08,  1.65s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 481/1030 [12:53<15:11,  1.66s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 482/1030 [12:55<15:16,  1.67s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 483/1030 [12:56<15:09,  1.66s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 484/1030 [12:58<14:59,  1.65s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 485/1030 [13:00<15:07,  1.67s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 486/1030 [13:02<15:44,  1.74s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 487/1030 [13:03<15:19,  1.69s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 488/1030 [13:05<15:22,  1.70s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 489/1030 [13:07<15:08,  1.68s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 490/1030 [13:08<14:53,  1.66s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 491/1030 [13:10<14:46,  1.64s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 492/1030 [13:11<14:45,  1.65s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 493/1030 [13:13<14:39,  1.64s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 494/1030 [13:15<14:52,  1.67s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 495/1030 [13:17<14:59,  1.68s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 496/1030 [13:18<15:03,  1.69s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 497/1030 [13:20<14:52,  1.67s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 498/1030 [13:21<14:40,  1.66s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 499/1030 [13:23<14:33,  1.64s/it]\u001b[A\n",
            "Iteration:  49%|████▊     | 500/1030 [13:25<14:30,  1.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.6281300783157349  Mean Loss:  1.7829222809076308\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  49%|████▊     | 501/1030 [13:26<14:24,  1.64s/it]\u001b[A\n",
            "Iteration:  49%|████▊     | 502/1030 [13:28<14:25,  1.64s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 503/1030 [13:30<14:22,  1.64s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 504/1030 [13:31<14:20,  1.64s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 505/1030 [13:33<14:16,  1.63s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 506/1030 [13:35<14:13,  1.63s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 507/1030 [13:36<14:10,  1.63s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 508/1030 [13:38<14:20,  1.65s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 509/1030 [13:39<14:14,  1.64s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 510/1030 [13:41<14:20,  1.66s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 511/1030 [13:43<14:15,  1.65s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 512/1030 [13:44<14:09,  1.64s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 513/1030 [13:46<14:44,  1.71s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 514/1030 [13:48<14:28,  1.68s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 515/1030 [13:50<14:18,  1.67s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 516/1030 [13:51<14:10,  1.65s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 517/1030 [13:53<14:02,  1.64s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 518/1030 [13:55<14:52,  1.74s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 519/1030 [13:56<14:32,  1.71s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 520/1030 [13:58<14:14,  1.67s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 521/1030 [14:00<14:10,  1.67s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 522/1030 [14:01<14:01,  1.66s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 523/1030 [14:03<13:55,  1.65s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 524/1030 [14:04<13:45,  1.63s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 525/1030 [14:06<13:57,  1.66s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 526/1030 [14:08<13:57,  1.66s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 527/1030 [14:10<14:06,  1.68s/it]\u001b[A\n",
            "Iteration:  51%|█████▏    | 528/1030 [14:11<14:12,  1.70s/it]\u001b[A\n",
            "Iteration:  51%|█████▏    | 529/1030 [14:13<14:06,  1.69s/it]\u001b[A\n",
            "Iteration:  51%|█████▏    | 530/1030 [14:15<13:48,  1.66s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 531/1030 [14:16<13:37,  1.64s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 532/1030 [14:18<13:38,  1.64s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 533/1030 [14:19<13:29,  1.63s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 534/1030 [14:21<13:33,  1.64s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 535/1030 [14:23<13:26,  1.63s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 536/1030 [14:25<13:55,  1.69s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 537/1030 [14:26<13:39,  1.66s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 538/1030 [14:28<13:52,  1.69s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 539/1030 [14:29<13:35,  1.66s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 540/1030 [14:31<13:35,  1.66s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 541/1030 [14:33<13:27,  1.65s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 542/1030 [14:34<13:17,  1.63s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 543/1030 [14:36<13:35,  1.67s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 544/1030 [14:38<13:20,  1.65s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 545/1030 [14:40<13:46,  1.70s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 546/1030 [14:41<13:50,  1.72s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 547/1030 [14:43<13:31,  1.68s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 548/1030 [14:45<14:05,  1.75s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 549/1030 [14:46<13:49,  1.73s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 550/1030 [14:48<13:41,  1.71s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 551/1030 [14:50<13:23,  1.68s/it]\u001b[A\n",
            "Iteration:  54%|█████▎    | 552/1030 [14:51<13:33,  1.70s/it]\u001b[A\n",
            "Iteration:  54%|█████▎    | 553/1030 [14:53<13:15,  1.67s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 554/1030 [14:55<13:15,  1.67s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 555/1030 [14:56<13:01,  1.65s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 556/1030 [14:58<12:52,  1.63s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 557/1030 [15:00<12:44,  1.62s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 558/1030 [15:01<12:48,  1.63s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 559/1030 [15:03<12:50,  1.64s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 560/1030 [15:04<12:41,  1.62s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 561/1030 [15:06<12:33,  1.61s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 562/1030 [15:08<13:06,  1.68s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 563/1030 [15:09<12:53,  1.66s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 564/1030 [15:11<12:41,  1.63s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 565/1030 [15:13<12:35,  1.62s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 566/1030 [15:14<12:52,  1.66s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 567/1030 [15:16<13:02,  1.69s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 568/1030 [15:18<12:49,  1.66s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 569/1030 [15:19<12:50,  1.67s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 570/1030 [15:21<12:40,  1.65s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 571/1030 [15:23<12:42,  1.66s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 572/1030 [15:24<12:29,  1.64s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 573/1030 [15:26<13:05,  1.72s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 574/1030 [15:28<12:46,  1.68s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 575/1030 [15:30<12:52,  1.70s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 576/1030 [15:31<12:45,  1.69s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 577/1030 [15:33<12:52,  1.71s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 578/1030 [15:35<12:33,  1.67s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 579/1030 [15:36<12:18,  1.64s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 580/1030 [15:38<12:10,  1.62s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 581/1030 [15:39<12:02,  1.61s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 582/1030 [15:41<12:10,  1.63s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 583/1030 [15:43<12:23,  1.66s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 584/1030 [15:44<12:12,  1.64s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 585/1030 [15:46<12:02,  1.62s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 586/1030 [15:47<11:54,  1.61s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 587/1030 [15:49<12:14,  1.66s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 588/1030 [15:51<12:22,  1.68s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 589/1030 [15:53<12:29,  1.70s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 590/1030 [15:54<12:37,  1.72s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 591/1030 [15:56<12:16,  1.68s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 592/1030 [15:58<12:39,  1.73s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 593/1030 [16:00<12:30,  1.72s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 594/1030 [16:01<12:12,  1.68s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 595/1030 [16:03<11:56,  1.65s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 596/1030 [16:04<12:07,  1.68s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 597/1030 [16:06<12:03,  1.67s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 598/1030 [16:08<11:50,  1.64s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 599/1030 [16:09<12:02,  1.68s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 600/1030 [16:11<11:46,  1.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.3243129253387451  Mean Loss:  1.7038385012745858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  58%|█████▊    | 601/1030 [16:13<11:59,  1.68s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 602/1030 [16:14<11:47,  1.65s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 603/1030 [16:16<11:36,  1.63s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 604/1030 [16:18<12:03,  1.70s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 605/1030 [16:19<11:45,  1.66s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 606/1030 [16:21<12:05,  1.71s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 607/1030 [16:23<12:08,  1.72s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 608/1030 [16:25<11:48,  1.68s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 609/1030 [16:26<11:55,  1.70s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 610/1030 [16:28<12:00,  1.72s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 611/1030 [16:30<12:06,  1.73s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 612/1030 [16:32<11:56,  1.71s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 613/1030 [16:33<11:39,  1.68s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 614/1030 [16:35<11:36,  1.68s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 615/1030 [16:37<11:43,  1.70s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 616/1030 [16:38<11:27,  1.66s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 617/1030 [16:40<11:27,  1.66s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 618/1030 [16:41<11:24,  1.66s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 619/1030 [16:43<11:14,  1.64s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 620/1030 [16:45<11:06,  1.62s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 621/1030 [16:46<10:59,  1.61s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 622/1030 [16:48<11:14,  1.65s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 623/1030 [16:50<11:33,  1.70s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 624/1030 [16:51<11:28,  1.70s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 625/1030 [16:53<11:25,  1.69s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 626/1030 [16:55<11:43,  1.74s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 627/1030 [16:57<11:23,  1.70s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 628/1030 [16:58<11:26,  1.71s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 629/1030 [17:00<11:30,  1.72s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 630/1030 [17:02<11:12,  1.68s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 631/1030 [17:03<11:30,  1.73s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 632/1030 [17:05<11:13,  1.69s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 633/1030 [17:07<11:19,  1.71s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 634/1030 [17:09<11:15,  1.71s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 635/1030 [17:10<11:19,  1.72s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 636/1030 [17:12<11:11,  1.70s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 637/1030 [17:14<11:04,  1.69s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 638/1030 [17:15<10:50,  1.66s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 639/1030 [17:17<10:38,  1.63s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 640/1030 [17:18<10:40,  1.64s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 641/1030 [17:20<10:42,  1.65s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 642/1030 [17:22<10:52,  1.68s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 643/1030 [17:24<11:10,  1.73s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 644/1030 [17:25<10:51,  1.69s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 645/1030 [17:27<10:56,  1.71s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 646/1030 [17:29<10:50,  1.69s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 647/1030 [17:30<10:38,  1.67s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 648/1030 [17:32<10:44,  1.69s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 649/1030 [17:34<10:49,  1.70s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 650/1030 [17:35<10:35,  1.67s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 651/1030 [17:37<10:23,  1.64s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 652/1030 [17:39<10:32,  1.67s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 653/1030 [17:40<10:22,  1.65s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 654/1030 [17:42<10:29,  1.68s/it]\u001b[A\n",
            "Iteration:  64%|██████▎   | 655/1030 [17:44<10:18,  1.65s/it]\u001b[A\n",
            "Iteration:  64%|██████▎   | 656/1030 [17:45<10:12,  1.64s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 657/1030 [17:47<10:03,  1.62s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 658/1030 [17:49<10:17,  1.66s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 659/1030 [17:50<10:08,  1.64s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 660/1030 [17:52<10:00,  1.62s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 661/1030 [17:53<10:03,  1.63s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 662/1030 [17:55<10:33,  1.72s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 663/1030 [17:57<10:33,  1.73s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 664/1030 [17:59<10:23,  1.70s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 665/1030 [18:00<10:18,  1.69s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 666/1030 [18:02<10:06,  1.67s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 667/1030 [18:04<09:56,  1.64s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 668/1030 [18:05<09:49,  1.63s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 669/1030 [18:07<09:42,  1.61s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 670/1030 [18:08<09:48,  1.63s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 671/1030 [18:10<09:52,  1.65s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 672/1030 [18:12<09:54,  1.66s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 673/1030 [18:13<09:45,  1.64s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 674/1030 [18:15<09:38,  1.63s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 675/1030 [18:17<09:32,  1.61s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 676/1030 [18:18<09:38,  1.63s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 677/1030 [18:20<09:31,  1.62s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 678/1030 [18:22<09:34,  1.63s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 679/1030 [18:23<09:28,  1.62s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 680/1030 [18:25<09:40,  1.66s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 681/1030 [18:27<09:49,  1.69s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 682/1030 [18:28<09:44,  1.68s/it]\u001b[A\n",
            "Iteration:  66%|██████▋   | 683/1030 [18:30<09:50,  1.70s/it]\u001b[A\n",
            "Iteration:  66%|██████▋   | 684/1030 [18:32<09:53,  1.72s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 685/1030 [18:33<09:39,  1.68s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 686/1030 [18:35<09:39,  1.69s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 687/1030 [18:37<09:34,  1.68s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 688/1030 [18:38<09:25,  1.65s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 689/1030 [18:40<09:43,  1.71s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 690/1030 [18:42<09:29,  1.67s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 691/1030 [18:43<09:17,  1.65s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 692/1030 [18:45<09:25,  1.67s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 693/1030 [18:47<09:13,  1.64s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 694/1030 [18:49<09:40,  1.73s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 695/1030 [18:50<09:32,  1.71s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 696/1030 [18:52<09:18,  1.67s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 697/1030 [18:53<09:08,  1.65s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 698/1030 [18:55<09:01,  1.63s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 699/1030 [18:57<08:54,  1.62s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 700/1030 [18:58<08:50,  1.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.1511601209640503  Mean Loss:  1.6428326568433216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  68%|██████▊   | 701/1030 [19:00<08:48,  1.61s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 702/1030 [19:02<09:07,  1.67s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 703/1030 [19:03<08:56,  1.64s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 704/1030 [19:05<09:13,  1.70s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 705/1030 [19:07<09:08,  1.69s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 706/1030 [19:08<09:07,  1.69s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 707/1030 [19:10<08:57,  1.66s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 708/1030 [19:12<08:48,  1.64s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 709/1030 [19:13<08:56,  1.67s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 710/1030 [19:15<08:46,  1.64s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 711/1030 [19:17<08:47,  1.65s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 712/1030 [19:18<08:38,  1.63s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 713/1030 [19:20<08:32,  1.62s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 714/1030 [19:21<08:35,  1.63s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 715/1030 [19:23<08:31,  1.62s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 716/1030 [19:25<08:35,  1.64s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 717/1030 [19:26<08:28,  1.63s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 718/1030 [19:28<08:46,  1.69s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 719/1030 [19:30<08:50,  1.71s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 720/1030 [19:32<09:00,  1.74s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 721/1030 [19:33<08:43,  1.69s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 722/1030 [19:35<08:30,  1.66s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 723/1030 [19:36<08:32,  1.67s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 724/1030 [19:38<08:24,  1.65s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 725/1030 [19:40<08:18,  1.63s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 726/1030 [19:41<08:10,  1.61s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 727/1030 [19:43<08:29,  1.68s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 728/1030 [19:45<08:25,  1.67s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 729/1030 [19:46<08:15,  1.65s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 730/1030 [19:48<08:08,  1.63s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 731/1030 [19:50<08:11,  1.65s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 732/1030 [19:51<08:20,  1.68s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 733/1030 [19:53<08:10,  1.65s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 734/1030 [19:55<08:34,  1.74s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 735/1030 [19:56<08:18,  1.69s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 736/1030 [19:58<08:15,  1.69s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 737/1030 [20:00<08:05,  1.66s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 738/1030 [20:01<07:56,  1.63s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 739/1030 [20:03<07:50,  1.62s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 740/1030 [20:05<07:54,  1.64s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 741/1030 [20:06<07:49,  1.62s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 742/1030 [20:08<07:43,  1.61s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 743/1030 [20:09<07:46,  1.63s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 744/1030 [20:11<07:49,  1.64s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 745/1030 [20:13<07:44,  1.63s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 746/1030 [20:14<07:39,  1.62s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 747/1030 [20:16<07:36,  1.61s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 748/1030 [20:17<07:33,  1.61s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 749/1030 [20:19<07:43,  1.65s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 750/1030 [20:21<07:56,  1.70s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 751/1030 [20:23<08:05,  1.74s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 752/1030 [20:24<07:51,  1.70s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 753/1030 [20:26<07:41,  1.67s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 754/1030 [20:28<07:33,  1.64s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 755/1030 [20:29<07:43,  1.69s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 756/1030 [20:31<07:32,  1.65s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 757/1030 [20:33<07:33,  1.66s/it]\u001b[A\n",
            "Iteration:  74%|███████▎  | 758/1030 [20:34<07:39,  1.69s/it]\u001b[A\n",
            "Iteration:  74%|███████▎  | 759/1030 [20:36<07:30,  1.66s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 760/1030 [20:38<07:21,  1.64s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 761/1030 [20:39<07:37,  1.70s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 762/1030 [20:41<07:39,  1.72s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 763/1030 [20:43<07:39,  1.72s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 764/1030 [20:45<07:45,  1.75s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 765/1030 [20:46<07:36,  1.72s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 766/1030 [20:48<07:24,  1.68s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 767/1030 [20:50<07:28,  1.71s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 768/1030 [20:51<07:19,  1.68s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 769/1030 [20:53<07:16,  1.67s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 770/1030 [20:55<07:08,  1.65s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 771/1030 [20:56<07:03,  1.63s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 772/1030 [20:58<06:56,  1.61s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 773/1030 [21:00<06:59,  1.63s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 774/1030 [21:01<06:53,  1.61s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 775/1030 [21:03<06:56,  1.63s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 776/1030 [21:05<07:10,  1.69s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 777/1030 [21:06<07:00,  1.66s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 778/1030 [21:08<06:59,  1.67s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 779/1030 [21:09<06:51,  1.64s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 780/1030 [21:11<06:50,  1.64s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 781/1030 [21:13<06:49,  1.64s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 782/1030 [21:14<06:55,  1.68s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 783/1030 [21:16<06:58,  1.69s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 784/1030 [21:18<06:49,  1.66s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 785/1030 [21:20<06:53,  1.69s/it]\u001b[A\n",
            "Iteration:  76%|███████▋  | 786/1030 [21:21<07:01,  1.73s/it]\u001b[A\n",
            "Iteration:  76%|███████▋  | 787/1030 [21:23<06:55,  1.71s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 788/1030 [21:25<06:50,  1.70s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 789/1030 [21:26<06:41,  1.66s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 790/1030 [21:28<06:51,  1.72s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 791/1030 [21:30<06:58,  1.75s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 792/1030 [21:32<06:43,  1.70s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 793/1030 [21:33<06:32,  1.66s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 794/1030 [21:35<06:31,  1.66s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 795/1030 [21:37<06:35,  1.68s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 796/1030 [21:38<06:26,  1.65s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 797/1030 [21:40<06:18,  1.62s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 798/1030 [21:41<06:18,  1.63s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 799/1030 [21:43<06:19,  1.64s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 800/1030 [21:45<06:13,  1.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.9923834800720215  Mean Loss:  1.589499129280448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  78%|███████▊  | 801/1030 [21:46<06:10,  1.62s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 802/1030 [21:48<06:05,  1.60s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 803/1030 [21:49<06:09,  1.63s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 804/1030 [21:51<06:28,  1.72s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 805/1030 [21:53<06:23,  1.70s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 806/1030 [21:55<06:19,  1.69s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 807/1030 [21:56<06:11,  1.67s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 808/1030 [21:58<06:06,  1.65s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 809/1030 [21:59<06:01,  1.63s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 810/1030 [22:01<06:00,  1.64s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 811/1030 [22:03<06:06,  1.67s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 812/1030 [22:05<06:16,  1.73s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 813/1030 [22:06<06:04,  1.68s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 814/1030 [22:08<05:55,  1.65s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 815/1030 [22:10<05:54,  1.65s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 816/1030 [22:11<05:47,  1.62s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 817/1030 [22:13<05:55,  1.67s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 818/1030 [22:14<05:47,  1.64s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 819/1030 [22:16<05:41,  1.62s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 820/1030 [22:18<05:38,  1.61s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 821/1030 [22:19<05:40,  1.63s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 822/1030 [22:21<05:36,  1.62s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 823/1030 [22:22<05:34,  1.61s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 824/1030 [22:24<05:30,  1.61s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 825/1030 [22:26<05:33,  1.63s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 826/1030 [22:27<05:34,  1.64s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 827/1030 [22:29<05:29,  1.62s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 828/1030 [22:31<05:52,  1.74s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 829/1030 [22:33<05:50,  1.74s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 830/1030 [22:34<05:39,  1.70s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 831/1030 [22:36<05:45,  1.74s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 832/1030 [22:38<05:43,  1.74s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 833/1030 [22:40<05:36,  1.71s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 834/1030 [22:41<05:32,  1.70s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 835/1030 [22:43<05:29,  1.69s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 836/1030 [22:45<05:24,  1.67s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 837/1030 [22:46<05:21,  1.66s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 838/1030 [22:48<05:15,  1.64s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 839/1030 [22:49<05:10,  1.62s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 840/1030 [22:51<05:15,  1.66s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 841/1030 [22:53<05:14,  1.66s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 842/1030 [22:54<05:07,  1.64s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 843/1030 [22:56<05:17,  1.70s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 844/1030 [22:58<05:17,  1.71s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 845/1030 [23:00<05:22,  1.74s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 846/1030 [23:01<05:12,  1.70s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 847/1030 [23:03<05:13,  1.72s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 848/1030 [23:05<05:05,  1.68s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 849/1030 [23:06<05:08,  1.70s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 850/1030 [23:08<05:01,  1.67s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 851/1030 [23:10<04:55,  1.65s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 852/1030 [23:11<04:59,  1.68s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 853/1030 [23:13<04:56,  1.68s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 854/1030 [23:15<04:58,  1.70s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 855/1030 [23:16<04:56,  1.69s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 856/1030 [23:18<04:49,  1.66s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 857/1030 [23:20<04:48,  1.67s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 858/1030 [23:21<04:43,  1.65s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 859/1030 [23:23<04:42,  1.65s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 860/1030 [23:25<04:37,  1.63s/it]\u001b[A\n",
            "Iteration:  84%|████████▎ | 861/1030 [23:26<04:32,  1.61s/it]\u001b[A\n",
            "Iteration:  84%|████████▎ | 862/1030 [23:28<04:30,  1.61s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 863/1030 [23:29<04:31,  1.62s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 864/1030 [23:31<04:39,  1.68s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 865/1030 [23:33<04:32,  1.65s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 866/1030 [23:35<04:35,  1.68s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 867/1030 [23:36<04:40,  1.72s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 868/1030 [23:38<04:31,  1.68s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 869/1030 [23:40<04:37,  1.73s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 870/1030 [23:41<04:29,  1.68s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 871/1030 [23:43<04:35,  1.73s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 872/1030 [23:45<04:26,  1.68s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 873/1030 [23:47<04:27,  1.70s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 874/1030 [23:48<04:20,  1.67s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 875/1030 [23:50<04:18,  1.67s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 876/1030 [23:51<04:12,  1.64s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 877/1030 [23:53<04:07,  1.62s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 878/1030 [23:55<04:04,  1.61s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 879/1030 [23:56<04:01,  1.60s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 880/1030 [23:58<04:02,  1.62s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 881/1030 [23:59<04:00,  1.61s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 882/1030 [24:01<03:57,  1.60s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 883/1030 [24:03<03:55,  1.60s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 884/1030 [24:04<03:53,  1.60s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 885/1030 [24:06<03:51,  1.60s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 886/1030 [24:07<03:49,  1.59s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 887/1030 [24:09<03:54,  1.64s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 888/1030 [24:11<03:53,  1.64s/it]\u001b[A\n",
            "Iteration:  86%|████████▋ | 889/1030 [24:12<03:52,  1.65s/it]\u001b[A\n",
            "Iteration:  86%|████████▋ | 890/1030 [24:14<03:48,  1.63s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 891/1030 [24:16<03:44,  1.62s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 892/1030 [24:17<03:47,  1.65s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 893/1030 [24:19<03:47,  1.66s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 894/1030 [24:21<03:42,  1.64s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 895/1030 [24:22<03:39,  1.62s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 896/1030 [24:24<03:35,  1.61s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 897/1030 [24:25<03:36,  1.63s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 898/1030 [24:27<03:33,  1.61s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 899/1030 [24:29<03:37,  1.66s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 900/1030 [24:30<03:32,  1.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.2218600511550903  Mean Loss:  1.5438764967521033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  87%|████████▋ | 901/1030 [24:32<03:29,  1.63s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 902/1030 [24:34<03:26,  1.62s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 903/1030 [24:35<03:27,  1.63s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 904/1030 [24:37<03:24,  1.62s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 905/1030 [24:39<03:27,  1.66s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 906/1030 [24:40<03:22,  1.63s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 907/1030 [24:42<03:22,  1.65s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 908/1030 [24:44<03:24,  1.67s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 909/1030 [24:45<03:19,  1.65s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 910/1030 [24:47<03:20,  1.67s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 911/1030 [24:48<03:18,  1.67s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 912/1030 [24:50<03:19,  1.69s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 913/1030 [24:52<03:14,  1.66s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 914/1030 [24:53<03:09,  1.64s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 915/1030 [24:55<03:05,  1.62s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 916/1030 [24:57<03:02,  1.60s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 917/1030 [24:58<03:05,  1.64s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 918/1030 [25:00<03:01,  1.62s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 919/1030 [25:02<03:01,  1.64s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 920/1030 [25:03<03:03,  1.67s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 921/1030 [25:05<03:04,  1.70s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 922/1030 [25:07<02:59,  1.67s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 923/1030 [25:08<02:55,  1.64s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 924/1030 [25:10<02:53,  1.63s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 925/1030 [25:11<02:50,  1.63s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 926/1030 [25:13<02:50,  1.64s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 927/1030 [25:15<02:49,  1.65s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 928/1030 [25:17<02:50,  1.68s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 929/1030 [25:18<02:46,  1.65s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 930/1030 [25:20<02:44,  1.65s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 931/1030 [25:21<02:40,  1.63s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 932/1030 [25:23<02:45,  1.69s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 933/1030 [25:25<02:45,  1.70s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 934/1030 [25:27<02:44,  1.72s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 935/1030 [25:28<02:40,  1.69s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 936/1030 [25:30<02:35,  1.65s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 937/1030 [25:32<02:38,  1.71s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 938/1030 [25:33<02:40,  1.75s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 939/1030 [25:35<02:34,  1.70s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 940/1030 [25:37<02:29,  1.66s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 941/1030 [25:38<02:25,  1.64s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 942/1030 [25:40<02:26,  1.66s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 943/1030 [25:42<02:22,  1.64s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 944/1030 [25:43<02:23,  1.67s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 945/1030 [25:45<02:26,  1.72s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 946/1030 [25:47<02:20,  1.68s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 947/1030 [25:48<02:17,  1.65s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 948/1030 [25:50<02:13,  1.63s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 949/1030 [25:52<02:16,  1.69s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 950/1030 [25:53<02:16,  1.70s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 951/1030 [25:55<02:15,  1.71s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 952/1030 [25:57<02:14,  1.73s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 953/1030 [25:59<02:09,  1.68s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 954/1030 [26:00<02:05,  1.66s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 955/1030 [26:02<02:02,  1.63s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 956/1030 [26:03<01:59,  1.62s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 957/1030 [26:05<01:57,  1.61s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 958/1030 [26:06<01:55,  1.60s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 959/1030 [26:08<01:53,  1.59s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 960/1030 [26:10<01:51,  1.59s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 961/1030 [26:11<01:51,  1.62s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 962/1030 [26:13<01:52,  1.66s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 963/1030 [26:15<01:49,  1.63s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 964/1030 [26:16<01:46,  1.62s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 965/1030 [26:18<01:44,  1.60s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 966/1030 [26:20<01:45,  1.65s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 967/1030 [26:21<01:42,  1.63s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 968/1030 [26:23<01:40,  1.61s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 969/1030 [26:24<01:38,  1.61s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 970/1030 [26:26<01:35,  1.60s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 971/1030 [26:28<01:35,  1.62s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 972/1030 [26:29<01:34,  1.64s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 973/1030 [26:31<01:32,  1.62s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 974/1030 [26:32<01:30,  1.62s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 975/1030 [26:34<01:28,  1.61s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 976/1030 [26:36<01:26,  1.60s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 977/1030 [26:37<01:26,  1.62s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 978/1030 [26:39<01:26,  1.66s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 979/1030 [26:41<01:25,  1.68s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 980/1030 [26:42<01:25,  1.71s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 981/1030 [26:44<01:21,  1.67s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 982/1030 [26:46<01:19,  1.67s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 983/1030 [26:47<01:18,  1.67s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 984/1030 [26:49<01:17,  1.69s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 985/1030 [26:51<01:17,  1.73s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 986/1030 [26:53<01:14,  1.69s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 987/1030 [26:54<01:11,  1.66s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 988/1030 [26:56<01:10,  1.68s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 989/1030 [26:57<01:07,  1.66s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 990/1030 [26:59<01:05,  1.63s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 991/1030 [27:01<01:04,  1.66s/it]\u001b[A\n",
            "Iteration:  96%|█████████▋| 992/1030 [27:03<01:04,  1.69s/it]\u001b[A\n",
            "Iteration:  96%|█████████▋| 993/1030 [27:04<01:01,  1.66s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 994/1030 [27:06<00:59,  1.64s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 995/1030 [27:07<00:57,  1.65s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 996/1030 [27:09<00:56,  1.67s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 997/1030 [27:11<00:54,  1.65s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 998/1030 [27:12<00:52,  1.64s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 999/1030 [27:14<00:51,  1.65s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 1000/1030 [27:16<00:49,  1.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.1026487350463867  Mean Loss:  1.51063305413723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  97%|█████████▋| 1001/1030 [27:17<00:47,  1.63s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 1002/1030 [27:19<00:45,  1.62s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 1003/1030 [27:20<00:43,  1.61s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 1004/1030 [27:22<00:41,  1.60s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1005/1030 [27:24<00:39,  1.59s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1006/1030 [27:25<00:38,  1.61s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1007/1030 [27:27<00:36,  1.60s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1008/1030 [27:28<00:35,  1.60s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1009/1030 [27:30<00:33,  1.59s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1010/1030 [27:32<00:32,  1.62s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1011/1030 [27:33<00:31,  1.63s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1012/1030 [27:35<00:29,  1.62s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1013/1030 [27:36<00:27,  1.61s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1014/1030 [27:38<00:26,  1.66s/it]\u001b[A\n",
            "Iteration:  99%|█████████▊| 1015/1030 [27:40<00:24,  1.67s/it]\u001b[A\n",
            "Iteration:  99%|█████████▊| 1016/1030 [27:42<00:23,  1.64s/it]\u001b[A\n",
            "Iteration:  99%|█████████▊| 1017/1030 [27:43<00:21,  1.68s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1018/1030 [27:45<00:19,  1.65s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1019/1030 [27:47<00:18,  1.71s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1020/1030 [27:48<00:16,  1.70s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1021/1030 [27:50<00:15,  1.71s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1022/1030 [27:52<00:13,  1.72s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1023/1030 [27:54<00:12,  1.73s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1024/1030 [27:55<00:10,  1.69s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1025/1030 [27:57<00:08,  1.74s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1026/1030 [27:59<00:06,  1.74s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1027/1030 [28:01<00:05,  1.74s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1028/1030 [28:02<00:03,  1.70s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1029/1030 [28:04<00:01,  1.70s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 1030/1030 [28:05<00:00,  1.42s/it]\u001b[A\n",
            "Epoch:  33%|███▎      | 1/3 [28:05<56:10, 1685.14s/it]\n",
            "Iteration:   0%|          | 0/1030 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 1/1030 [00:01<27:29,  1.60s/it]\u001b[A\n",
            "Iteration:   0%|          | 2/1030 [00:03<27:15,  1.59s/it]\u001b[A\n",
            "Iteration:   0%|          | 3/1030 [00:04<27:17,  1.59s/it]\u001b[A\n",
            "Iteration:   0%|          | 4/1030 [00:06<27:10,  1.59s/it]\u001b[A\n",
            "Iteration:   0%|          | 5/1030 [00:07<27:08,  1.59s/it]\u001b[A\n",
            "Iteration:   1%|          | 6/1030 [00:09<27:39,  1.62s/it]\u001b[A\n",
            "Iteration:   1%|          | 7/1030 [00:11<27:30,  1.61s/it]\u001b[A\n",
            "Iteration:   1%|          | 8/1030 [00:13<28:38,  1.68s/it]\u001b[A\n",
            "Iteration:   1%|          | 9/1030 [00:14<28:31,  1.68s/it]\u001b[A\n",
            "Iteration:   1%|          | 10/1030 [00:16<28:03,  1.65s/it]\u001b[A\n",
            "Iteration:   1%|          | 11/1030 [00:17<27:45,  1.63s/it]\u001b[A\n",
            "Iteration:   1%|          | 12/1030 [00:19<27:24,  1.62s/it]\u001b[A\n",
            "Iteration:   1%|▏         | 13/1030 [00:21<27:15,  1.61s/it]\u001b[A\n",
            "Iteration:   1%|▏         | 14/1030 [00:22<27:10,  1.60s/it]\u001b[A\n",
            "Iteration:   1%|▏         | 15/1030 [00:24<26:58,  1.59s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 16/1030 [00:25<27:45,  1.64s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 17/1030 [00:27<27:26,  1.62s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 18/1030 [00:29<28:36,  1.70s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 19/1030 [00:31<30:10,  1.79s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 20/1030 [00:33<29:08,  1.73s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 21/1030 [00:34<28:21,  1.69s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 22/1030 [00:36<27:43,  1.65s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 23/1030 [00:37<27:22,  1.63s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 24/1030 [00:39<27:36,  1.65s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 25/1030 [00:41<27:59,  1.67s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 26/1030 [00:42<27:53,  1.67s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 27/1030 [00:44<27:52,  1.67s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 28/1030 [00:46<28:38,  1.72s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 29/1030 [00:48<28:20,  1.70s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 30/1030 [00:49<28:06,  1.69s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 31/1030 [00:51<27:30,  1.65s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 32/1030 [00:52<27:10,  1.63s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 33/1030 [00:54<26:55,  1.62s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 34/1030 [00:56<27:05,  1.63s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 35/1030 [00:57<26:45,  1.61s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 36/1030 [00:59<26:34,  1.60s/it]\u001b[A\n",
            "Iteration:   4%|▎         | 37/1030 [01:01<27:41,  1.67s/it]\u001b[A\n",
            "Iteration:   4%|▎         | 38/1030 [01:02<27:58,  1.69s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 39/1030 [01:04<28:13,  1.71s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 40/1030 [01:06<28:47,  1.75s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 41/1030 [01:08<29:10,  1.77s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 42/1030 [01:09<28:38,  1.74s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 43/1030 [01:11<29:05,  1.77s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 44/1030 [01:13<28:35,  1.74s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 45/1030 [01:15<28:34,  1.74s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 46/1030 [01:16<28:16,  1.72s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 47/1030 [01:18<28:22,  1.73s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 48/1030 [01:20<29:17,  1.79s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 49/1030 [01:22<28:18,  1.73s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 50/1030 [01:23<27:31,  1.68s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 51/1030 [01:25<27:01,  1.66s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 52/1030 [01:26<27:26,  1.68s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 53/1030 [01:28<27:37,  1.70s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 54/1030 [01:30<27:47,  1.71s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 55/1030 [01:32<27:12,  1.67s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 56/1030 [01:33<26:43,  1.65s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 57/1030 [01:35<27:38,  1.70s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 58/1030 [01:37<27:46,  1.71s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 59/1030 [01:38<27:36,  1.71s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 60/1030 [01:40<26:56,  1.67s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 61/1030 [01:42<26:37,  1.65s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 62/1030 [01:43<26:20,  1.63s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 63/1030 [01:45<26:03,  1.62s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 64/1030 [01:46<25:50,  1.61s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 65/1030 [01:48<25:40,  1.60s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 66/1030 [01:50<26:03,  1.62s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 67/1030 [01:51<26:37,  1.66s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 68/1030 [01:53<27:25,  1.71s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 69/1030 [01:55<27:09,  1.70s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 70/1030 [01:56<26:59,  1.69s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 71/1030 [01:58<26:30,  1.66s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 72/1030 [02:00<27:19,  1.71s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 73/1030 [02:02<26:45,  1.68s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 74/1030 [02:03<26:38,  1.67s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 75/1030 [02:05<26:11,  1.65s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 76/1030 [02:06<25:53,  1.63s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 77/1030 [02:08<26:01,  1.64s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 78/1030 [02:10<26:53,  1.70s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 79/1030 [02:11<26:21,  1.66s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 80/1030 [02:13<25:52,  1.63s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 81/1030 [02:15<25:36,  1.62s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 82/1030 [02:16<25:24,  1.61s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 83/1030 [02:18<25:40,  1.63s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 84/1030 [02:19<25:23,  1.61s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 85/1030 [02:21<25:12,  1.60s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 86/1030 [02:23<25:06,  1.60s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 87/1030 [02:24<25:46,  1.64s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 88/1030 [02:26<25:29,  1.62s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 89/1030 [02:27<25:14,  1.61s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 90/1030 [02:29<25:07,  1.60s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 91/1030 [02:31<26:13,  1.68s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 92/1030 [02:32<25:44,  1.65s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 93/1030 [02:34<25:41,  1.65s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 94/1030 [02:36<25:42,  1.65s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 95/1030 [02:37<25:43,  1.65s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 96/1030 [02:39<25:41,  1.65s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 97/1030 [02:41<26:08,  1.68s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 98/1030 [02:43<26:25,  1.70s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 99/1030 [02:44<25:46,  1.66s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 100/1030 [02:46<25:21,  1.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.5773861408233643  Mean Loss:  0.9392018255591392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  10%|▉         | 101/1030 [02:47<25:05,  1.62s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 102/1030 [02:49<24:54,  1.61s/it]\u001b[A\n",
            "Iteration:  10%|█         | 103/1030 [02:50<24:45,  1.60s/it]\u001b[A\n",
            "Iteration:  10%|█         | 104/1030 [02:52<24:59,  1.62s/it]\u001b[A\n",
            "Iteration:  10%|█         | 105/1030 [02:54<24:49,  1.61s/it]\u001b[A\n",
            "Iteration:  10%|█         | 106/1030 [02:55<24:38,  1.60s/it]\u001b[A\n",
            "Iteration:  10%|█         | 107/1030 [02:57<25:39,  1.67s/it]\u001b[A\n",
            "Iteration:  10%|█         | 108/1030 [02:59<25:19,  1.65s/it]\u001b[A\n",
            "Iteration:  11%|█         | 109/1030 [03:00<24:56,  1.62s/it]\u001b[A\n",
            "Iteration:  11%|█         | 110/1030 [03:02<24:49,  1.62s/it]\u001b[A\n",
            "Iteration:  11%|█         | 111/1030 [03:03<24:34,  1.60s/it]\u001b[A\n",
            "Iteration:  11%|█         | 112/1030 [03:05<24:47,  1.62s/it]\u001b[A\n",
            "Iteration:  11%|█         | 113/1030 [03:07<25:21,  1.66s/it]\u001b[A\n",
            "Iteration:  11%|█         | 114/1030 [03:09<25:41,  1.68s/it]\u001b[A\n",
            "Iteration:  11%|█         | 115/1030 [03:10<26:22,  1.73s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 116/1030 [03:12<26:02,  1.71s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 117/1030 [03:14<25:28,  1.67s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 118/1030 [03:15<25:01,  1.65s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 119/1030 [03:17<25:08,  1.66s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 120/1030 [03:19<24:46,  1.63s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 121/1030 [03:20<24:31,  1.62s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 122/1030 [03:22<24:16,  1.60s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 123/1030 [03:23<24:33,  1.63s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 124/1030 [03:25<24:24,  1.62s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 125/1030 [03:27<24:15,  1.61s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 126/1030 [03:28<24:06,  1.60s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 127/1030 [03:30<24:01,  1.60s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 128/1030 [03:32<25:03,  1.67s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 129/1030 [03:33<25:01,  1.67s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 130/1030 [03:35<25:44,  1.72s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 131/1030 [03:37<26:20,  1.76s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 132/1030 [03:39<25:34,  1.71s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 133/1030 [03:40<25:41,  1.72s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 134/1030 [03:42<26:13,  1.76s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 135/1030 [03:44<25:26,  1.71s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 136/1030 [03:45<25:16,  1.70s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 137/1030 [03:47<24:45,  1.66s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 138/1030 [03:49<24:23,  1.64s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 139/1030 [03:50<24:24,  1.64s/it]\u001b[A\n",
            "Iteration:  14%|█▎        | 140/1030 [03:52<24:11,  1.63s/it]\u001b[A\n",
            "Iteration:  14%|█▎        | 141/1030 [03:54<24:27,  1.65s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 142/1030 [03:55<24:49,  1.68s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 143/1030 [03:57<25:01,  1.69s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 144/1030 [03:59<24:31,  1.66s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 145/1030 [04:00<24:26,  1.66s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 146/1030 [04:02<24:06,  1.64s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 147/1030 [04:03<23:53,  1.62s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 148/1030 [04:05<24:26,  1.66s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 149/1030 [04:07<24:07,  1.64s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 150/1030 [04:09<24:58,  1.70s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 151/1030 [04:10<25:05,  1.71s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 152/1030 [04:12<25:12,  1.72s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 153/1030 [04:14<24:39,  1.69s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 154/1030 [04:15<24:13,  1.66s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 155/1030 [04:17<24:01,  1.65s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 156/1030 [04:19<24:07,  1.66s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 157/1030 [04:20<23:51,  1.64s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 158/1030 [04:22<23:35,  1.62s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 159/1030 [04:23<23:18,  1.61s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 160/1030 [04:25<23:11,  1.60s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 161/1030 [04:27<23:25,  1.62s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 162/1030 [04:28<23:17,  1.61s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 163/1030 [04:30<23:14,  1.61s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 164/1030 [04:32<23:51,  1.65s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 165/1030 [04:33<23:48,  1.65s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 166/1030 [04:35<23:52,  1.66s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 167/1030 [04:36<23:53,  1.66s/it]\u001b[A\n",
            "Iteration:  16%|█▋        | 168/1030 [04:38<23:33,  1.64s/it]\u001b[A\n",
            "Iteration:  16%|█▋        | 169/1030 [04:40<23:46,  1.66s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 170/1030 [04:41<23:23,  1.63s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 171/1030 [04:43<24:15,  1.69s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 172/1030 [04:45<23:48,  1.66s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 173/1030 [04:47<24:07,  1.69s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 174/1030 [04:48<23:37,  1.66s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 175/1030 [04:50<23:35,  1.66s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 176/1030 [04:51<23:35,  1.66s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 177/1030 [04:53<23:15,  1.64s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 178/1030 [04:55<23:00,  1.62s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 179/1030 [04:56<22:47,  1.61s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 180/1030 [04:58<22:43,  1.60s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 181/1030 [04:59<23:02,  1.63s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 182/1030 [05:01<22:57,  1.62s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 183/1030 [05:03<23:09,  1.64s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 184/1030 [05:04<23:17,  1.65s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 185/1030 [05:06<22:59,  1.63s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 186/1030 [05:08<22:44,  1.62s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 187/1030 [05:09<23:24,  1.67s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 188/1030 [05:11<23:03,  1.64s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 189/1030 [05:13<22:52,  1.63s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 190/1030 [05:14<23:00,  1.64s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 191/1030 [05:16<22:45,  1.63s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 192/1030 [05:17<22:31,  1.61s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 193/1030 [05:19<22:22,  1.60s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 194/1030 [05:21<22:12,  1.59s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 195/1030 [05:22<22:46,  1.64s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 196/1030 [05:24<23:11,  1.67s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 197/1030 [05:26<23:28,  1.69s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 198/1030 [05:28<24:04,  1.74s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 199/1030 [05:29<23:26,  1.69s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 200/1030 [05:31<23:01,  1.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.7641462087631226  Mean Loss:  0.9612508642673493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  20%|█▉        | 201/1030 [05:32<22:36,  1.64s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 202/1030 [05:34<23:02,  1.67s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 203/1030 [05:36<22:56,  1.67s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 204/1030 [05:37<22:37,  1.64s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 205/1030 [05:39<22:23,  1.63s/it]\u001b[A\n",
            "Iteration:  20%|██        | 206/1030 [05:41<22:29,  1.64s/it]\u001b[A\n",
            "Iteration:  20%|██        | 207/1030 [05:42<22:17,  1.63s/it]\u001b[A\n",
            "Iteration:  20%|██        | 208/1030 [05:44<22:04,  1.61s/it]\u001b[A\n",
            "Iteration:  20%|██        | 209/1030 [05:45<22:04,  1.61s/it]\u001b[A\n",
            "Iteration:  20%|██        | 210/1030 [05:47<21:56,  1.61s/it]\u001b[A\n",
            "Iteration:  20%|██        | 211/1030 [05:49<22:08,  1.62s/it]\u001b[A\n",
            "Iteration:  21%|██        | 212/1030 [05:50<21:55,  1.61s/it]\u001b[A\n",
            "Iteration:  21%|██        | 213/1030 [05:52<21:45,  1.60s/it]\u001b[A\n",
            "Iteration:  21%|██        | 214/1030 [05:53<21:55,  1.61s/it]\u001b[A\n",
            "Iteration:  21%|██        | 215/1030 [05:55<22:27,  1.65s/it]\u001b[A\n",
            "Iteration:  21%|██        | 216/1030 [05:57<22:48,  1.68s/it]\u001b[A\n",
            "Iteration:  21%|██        | 217/1030 [05:59<22:21,  1.65s/it]\u001b[A\n",
            "Iteration:  21%|██        | 218/1030 [06:00<22:24,  1.66s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 219/1030 [06:02<22:43,  1.68s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 220/1030 [06:04<23:02,  1.71s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 221/1030 [06:05<22:27,  1.67s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 222/1030 [06:07<22:03,  1.64s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 223/1030 [06:09<22:09,  1.65s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 224/1030 [06:10<21:54,  1.63s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 225/1030 [06:12<21:58,  1.64s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 226/1030 [06:13<21:47,  1.63s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 227/1030 [06:15<21:50,  1.63s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 228/1030 [06:17<21:33,  1.61s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 229/1030 [06:18<21:43,  1.63s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 230/1030 [06:20<21:29,  1.61s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 231/1030 [06:22<21:44,  1.63s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 232/1030 [06:23<21:49,  1.64s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 233/1030 [06:25<22:11,  1.67s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 234/1030 [06:27<22:07,  1.67s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 235/1030 [06:28<22:09,  1.67s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 236/1030 [06:30<22:06,  1.67s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 237/1030 [06:32<22:20,  1.69s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 238/1030 [06:34<22:54,  1.74s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 239/1030 [06:35<23:02,  1.75s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 240/1030 [06:37<22:58,  1.75s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 241/1030 [06:39<22:42,  1.73s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 242/1030 [06:40<22:05,  1.68s/it]\u001b[A\n",
            "Iteration:  24%|██▎       | 243/1030 [06:42<21:39,  1.65s/it]\u001b[A\n",
            "Iteration:  24%|██▎       | 244/1030 [06:44<21:37,  1.65s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 245/1030 [06:45<21:23,  1.64s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 246/1030 [06:47<21:51,  1.67s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 247/1030 [06:48<21:34,  1.65s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 248/1030 [06:50<21:15,  1.63s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 249/1030 [06:52<21:04,  1.62s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 250/1030 [06:53<20:55,  1.61s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 251/1030 [06:55<20:48,  1.60s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 252/1030 [06:56<20:39,  1.59s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 253/1030 [06:58<21:00,  1.62s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 254/1030 [07:00<21:13,  1.64s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 255/1030 [07:01<20:52,  1.62s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 256/1030 [07:03<21:08,  1.64s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 257/1030 [07:05<20:53,  1.62s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 258/1030 [07:06<20:45,  1.61s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 259/1030 [07:08<20:52,  1.62s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 260/1030 [07:10<21:59,  1.71s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 261/1030 [07:11<21:30,  1.68s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 262/1030 [07:13<22:06,  1.73s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 263/1030 [07:15<21:31,  1.68s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 264/1030 [07:16<21:30,  1.68s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 265/1030 [07:18<21:03,  1.65s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 266/1030 [07:20<21:43,  1.71s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 267/1030 [07:22<21:30,  1.69s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 268/1030 [07:23<21:05,  1.66s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 269/1030 [07:25<20:46,  1.64s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 270/1030 [07:26<20:34,  1.62s/it]\u001b[A\n",
            "Iteration:  26%|██▋       | 271/1030 [07:28<20:23,  1.61s/it]\u001b[A\n",
            "Iteration:  26%|██▋       | 272/1030 [07:30<20:32,  1.63s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 273/1030 [07:31<20:24,  1.62s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 274/1030 [07:33<21:15,  1.69s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 275/1030 [07:35<20:45,  1.65s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 276/1030 [07:36<21:04,  1.68s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 277/1030 [07:38<20:43,  1.65s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 278/1030 [07:40<20:45,  1.66s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 279/1030 [07:41<20:24,  1.63s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 280/1030 [07:43<20:14,  1.62s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 281/1030 [07:44<20:05,  1.61s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 282/1030 [07:46<19:57,  1.60s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 283/1030 [07:48<20:09,  1.62s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 284/1030 [07:49<20:01,  1.61s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 285/1030 [07:51<20:27,  1.65s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 286/1030 [07:53<20:31,  1.65s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 287/1030 [07:54<20:19,  1.64s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 288/1030 [07:56<20:07,  1.63s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 289/1030 [07:57<19:52,  1.61s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 290/1030 [07:59<20:06,  1.63s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 291/1030 [08:01<20:15,  1.64s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 292/1030 [08:02<20:36,  1.67s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 293/1030 [08:04<21:30,  1.75s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 294/1030 [08:06<21:23,  1.74s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 295/1030 [08:08<21:17,  1.74s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 296/1030 [08:09<20:53,  1.71s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 297/1030 [08:11<21:02,  1.72s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 298/1030 [08:13<20:30,  1.68s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 299/1030 [08:14<20:28,  1.68s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 300/1030 [08:16<20:06,  1.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.6208821535110474  Mean Loss:  0.9795845821499825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  29%|██▉       | 301/1030 [08:18<20:10,  1.66s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 302/1030 [08:19<20:26,  1.68s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 303/1030 [08:21<20:58,  1.73s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 304/1030 [08:23<20:24,  1.69s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 305/1030 [08:25<20:35,  1.70s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 306/1030 [08:26<20:13,  1.68s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 307/1030 [08:28<19:55,  1.65s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 308/1030 [08:29<19:45,  1.64s/it]\u001b[A\n",
            "Iteration:  30%|███       | 309/1030 [08:31<19:32,  1.63s/it]\u001b[A\n",
            "Iteration:  30%|███       | 310/1030 [08:33<19:22,  1.62s/it]\u001b[A\n",
            "Iteration:  30%|███       | 311/1030 [08:34<19:19,  1.61s/it]\u001b[A\n",
            "Iteration:  30%|███       | 312/1030 [08:36<19:10,  1.60s/it]\u001b[A\n",
            "Iteration:  30%|███       | 313/1030 [08:37<19:22,  1.62s/it]\u001b[A\n",
            "Iteration:  30%|███       | 314/1030 [08:39<19:13,  1.61s/it]\u001b[A\n",
            "Iteration:  31%|███       | 315/1030 [08:41<20:02,  1.68s/it]\u001b[A\n",
            "Iteration:  31%|███       | 316/1030 [08:43<20:00,  1.68s/it]\u001b[A\n",
            "Iteration:  31%|███       | 317/1030 [08:44<20:10,  1.70s/it]\u001b[A\n",
            "Iteration:  31%|███       | 318/1030 [08:46<20:38,  1.74s/it]\u001b[A\n",
            "Iteration:  31%|███       | 319/1030 [08:48<20:20,  1.72s/it]\u001b[A\n",
            "Iteration:  31%|███       | 320/1030 [08:50<20:08,  1.70s/it]\u001b[A\n",
            "Iteration:  31%|███       | 321/1030 [08:51<19:39,  1.66s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 322/1030 [08:53<20:15,  1.72s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 323/1030 [08:55<19:49,  1.68s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 324/1030 [08:56<19:27,  1.65s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 325/1030 [08:58<19:10,  1.63s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 326/1030 [09:00<19:54,  1.70s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 327/1030 [09:01<19:32,  1.67s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 328/1030 [09:03<19:12,  1.64s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 329/1030 [09:04<19:32,  1.67s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 330/1030 [09:06<19:28,  1.67s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 331/1030 [09:08<19:10,  1.65s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 332/1030 [09:09<18:57,  1.63s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 333/1030 [09:11<18:47,  1.62s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 334/1030 [09:13<18:57,  1.63s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 335/1030 [09:14<19:17,  1.67s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 336/1030 [09:16<19:37,  1.70s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 337/1030 [09:18<21:05,  1.83s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 338/1030 [09:20<20:12,  1.75s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 339/1030 [09:21<19:42,  1.71s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 340/1030 [09:23<19:10,  1.67s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 341/1030 [09:25<19:08,  1.67s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 342/1030 [09:26<19:10,  1.67s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 343/1030 [09:28<18:50,  1.65s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 344/1030 [09:29<18:36,  1.63s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 345/1030 [09:31<18:27,  1.62s/it]\u001b[A\n",
            "Iteration:  34%|███▎      | 346/1030 [09:33<18:18,  1.61s/it]\u001b[A\n",
            "Iteration:  34%|███▎      | 347/1030 [09:34<18:15,  1.60s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 348/1030 [09:36<18:07,  1.59s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 349/1030 [09:37<18:03,  1.59s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 350/1030 [09:39<17:59,  1.59s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 351/1030 [09:41<18:18,  1.62s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 352/1030 [09:42<18:11,  1.61s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 353/1030 [09:44<18:02,  1.60s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 354/1030 [09:46<18:32,  1.65s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 355/1030 [09:47<18:20,  1.63s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 356/1030 [09:49<18:40,  1.66s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 357/1030 [09:51<18:39,  1.66s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 358/1030 [09:52<18:42,  1.67s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 359/1030 [09:54<18:24,  1.65s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 360/1030 [09:56<18:40,  1.67s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 361/1030 [09:57<18:19,  1.64s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 362/1030 [09:59<18:20,  1.65s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 363/1030 [10:00<18:07,  1.63s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 364/1030 [10:02<17:56,  1.62s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 365/1030 [10:04<17:48,  1.61s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 366/1030 [10:05<17:43,  1.60s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 367/1030 [10:07<17:35,  1.59s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 368/1030 [10:08<17:34,  1.59s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 369/1030 [10:10<17:31,  1.59s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 370/1030 [10:12<17:59,  1.64s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 371/1030 [10:13<18:02,  1.64s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 372/1030 [10:15<17:49,  1.62s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 373/1030 [10:17<17:58,  1.64s/it]\u001b[A\n",
            "Iteration:  36%|███▋      | 374/1030 [10:18<17:48,  1.63s/it]\u001b[A\n",
            "Iteration:  36%|███▋      | 375/1030 [10:20<17:37,  1.61s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 376/1030 [10:21<17:34,  1.61s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 377/1030 [10:23<17:46,  1.63s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 378/1030 [10:25<17:53,  1.65s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 379/1030 [10:26<18:10,  1.68s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 380/1030 [10:28<17:50,  1.65s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 381/1030 [10:30<18:10,  1.68s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 382/1030 [10:32<18:12,  1.69s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 383/1030 [10:33<18:04,  1.68s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 384/1030 [10:35<18:16,  1.70s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 385/1030 [10:37<17:53,  1.66s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 386/1030 [10:38<17:38,  1.64s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 387/1030 [10:40<17:29,  1.63s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 388/1030 [10:41<17:16,  1.62s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 389/1030 [10:43<17:10,  1.61s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 390/1030 [10:45<17:21,  1.63s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 391/1030 [10:46<17:39,  1.66s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 392/1030 [10:48<18:12,  1.71s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 393/1030 [10:50<17:46,  1.67s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 394/1030 [10:51<17:30,  1.65s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 395/1030 [10:53<17:31,  1.66s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 396/1030 [10:55<18:03,  1.71s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 397/1030 [10:56<17:40,  1.68s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 398/1030 [10:58<18:08,  1.72s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 399/1030 [11:00<17:56,  1.71s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 400/1030 [11:02<18:05,  1.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.6463689804077148  Mean Loss:  0.9889740671962499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  39%|███▉      | 401/1030 [11:03<17:38,  1.68s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 402/1030 [11:05<17:19,  1.66s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 403/1030 [11:06<17:04,  1.63s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 404/1030 [11:08<16:51,  1.62s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 405/1030 [11:10<16:50,  1.62s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 406/1030 [11:11<16:43,  1.61s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 407/1030 [11:13<16:41,  1.61s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 408/1030 [11:15<16:53,  1.63s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 409/1030 [11:16<16:48,  1.62s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 410/1030 [11:18<16:40,  1.61s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 411/1030 [11:19<16:32,  1.60s/it]\u001b[A\n",
            "Iteration:  40%|████      | 412/1030 [11:21<16:41,  1.62s/it]\u001b[A\n",
            "Iteration:  40%|████      | 413/1030 [11:23<16:48,  1.63s/it]\u001b[A\n",
            "Iteration:  40%|████      | 414/1030 [11:24<16:47,  1.64s/it]\u001b[A\n",
            "Iteration:  40%|████      | 415/1030 [11:26<16:35,  1.62s/it]\u001b[A\n",
            "Iteration:  40%|████      | 416/1030 [11:27<16:26,  1.61s/it]\u001b[A\n",
            "Iteration:  40%|████      | 417/1030 [11:29<17:08,  1.68s/it]\u001b[A\n",
            "Iteration:  41%|████      | 418/1030 [11:31<17:04,  1.67s/it]\u001b[A\n",
            "Iteration:  41%|████      | 419/1030 [11:33<16:49,  1.65s/it]\u001b[A\n",
            "Iteration:  41%|████      | 420/1030 [11:34<16:48,  1.65s/it]\u001b[A\n",
            "Iteration:  41%|████      | 421/1030 [11:36<16:46,  1.65s/it]\u001b[A\n",
            "Iteration:  41%|████      | 422/1030 [11:37<16:34,  1.64s/it]\u001b[A\n",
            "Iteration:  41%|████      | 423/1030 [11:39<17:09,  1.70s/it]\u001b[A\n",
            "Iteration:  41%|████      | 424/1030 [11:41<16:49,  1.67s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 425/1030 [11:42<16:38,  1.65s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 426/1030 [11:44<16:22,  1.63s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 427/1030 [11:46<16:42,  1.66s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 428/1030 [11:47<16:28,  1.64s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 429/1030 [11:49<16:47,  1.68s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 430/1030 [11:51<17:17,  1.73s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 431/1030 [11:53<17:07,  1.72s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 432/1030 [11:54<16:42,  1.68s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 433/1030 [11:56<16:22,  1.65s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 434/1030 [11:57<16:09,  1.63s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 435/1030 [11:59<15:59,  1.61s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 436/1030 [12:01<15:56,  1.61s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 437/1030 [12:02<15:54,  1.61s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 438/1030 [12:04<15:53,  1.61s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 439/1030 [12:05<15:48,  1.60s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 440/1030 [12:07<16:31,  1.68s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 441/1030 [12:09<16:39,  1.70s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 442/1030 [12:11<17:00,  1.74s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 443/1030 [12:12<16:33,  1.69s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 444/1030 [12:14<16:12,  1.66s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 445/1030 [12:16<16:02,  1.65s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 446/1030 [12:17<15:49,  1.63s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 447/1030 [12:19<15:43,  1.62s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 448/1030 [12:20<15:36,  1.61s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 449/1030 [12:22<16:00,  1.65s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 450/1030 [12:24<15:48,  1.64s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 451/1030 [12:25<15:40,  1.62s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 452/1030 [12:27<15:31,  1.61s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 453/1030 [12:29<15:41,  1.63s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 454/1030 [12:31<16:45,  1.75s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 455/1030 [12:32<16:41,  1.74s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 456/1030 [12:34<16:53,  1.76s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 457/1030 [12:36<16:23,  1.72s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 458/1030 [12:37<16:00,  1.68s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 459/1030 [12:39<15:45,  1.66s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 460/1030 [12:41<15:33,  1.64s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 461/1030 [12:42<15:20,  1.62s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 462/1030 [12:44<15:12,  1.61s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 463/1030 [12:45<15:09,  1.60s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 464/1030 [12:47<15:06,  1.60s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 465/1030 [12:49<15:17,  1.62s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 466/1030 [12:50<15:36,  1.66s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 467/1030 [12:52<15:38,  1.67s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 468/1030 [12:54<15:52,  1.70s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 469/1030 [12:55<15:36,  1.67s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 470/1030 [12:57<15:49,  1.70s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 471/1030 [12:59<15:31,  1.67s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 472/1030 [13:01<16:02,  1.72s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 473/1030 [13:02<15:36,  1.68s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 474/1030 [13:04<15:25,  1.66s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 475/1030 [13:05<15:09,  1.64s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 476/1030 [13:07<15:26,  1.67s/it]\u001b[A\n",
            "Iteration:  46%|████▋     | 477/1030 [13:09<15:25,  1.67s/it]\u001b[A\n",
            "Iteration:  46%|████▋     | 478/1030 [13:11<15:34,  1.69s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 479/1030 [13:12<15:29,  1.69s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 480/1030 [13:14<15:13,  1.66s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 481/1030 [13:15<15:12,  1.66s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 482/1030 [13:17<15:00,  1.64s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 483/1030 [13:19<14:50,  1.63s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 484/1030 [13:20<14:55,  1.64s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 485/1030 [13:22<15:00,  1.65s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 486/1030 [13:24<15:43,  1.73s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 487/1030 [13:26<15:18,  1.69s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 488/1030 [13:27<15:31,  1.72s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 489/1030 [13:30<17:18,  1.92s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 490/1030 [13:31<16:51,  1.87s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 491/1030 [13:33<16:32,  1.84s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 492/1030 [13:35<15:47,  1.76s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 493/1030 [13:36<15:15,  1.71s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 494/1030 [13:38<14:54,  1.67s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 495/1030 [13:40<14:36,  1.64s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 496/1030 [13:41<14:35,  1.64s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 497/1030 [13:43<14:27,  1.63s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 498/1030 [13:44<14:18,  1.61s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 499/1030 [13:46<14:26,  1.63s/it]\u001b[A\n",
            "Iteration:  49%|████▊     | 500/1030 [13:48<14:56,  1.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.7274722456932068  Mean Loss:  0.9862378271222114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  49%|████▊     | 501/1030 [13:50<15:03,  1.71s/it]\u001b[A\n",
            "Iteration:  49%|████▊     | 502/1030 [13:51<14:59,  1.70s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 503/1030 [13:53<15:18,  1.74s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 504/1030 [13:55<14:52,  1.70s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 505/1030 [13:56<14:34,  1.67s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 506/1030 [13:58<15:12,  1.74s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 507/1030 [14:00<14:46,  1.69s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 508/1030 [14:01<14:28,  1.66s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 509/1030 [14:03<14:16,  1.64s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 510/1030 [14:05<14:04,  1.62s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 511/1030 [14:06<13:58,  1.62s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 512/1030 [14:08<13:52,  1.61s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 513/1030 [14:09<13:46,  1.60s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 514/1030 [14:11<13:42,  1.59s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 515/1030 [14:13<13:37,  1.59s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 516/1030 [14:14<13:59,  1.63s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 517/1030 [14:16<14:05,  1.65s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 518/1030 [14:18<13:51,  1.62s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 519/1030 [14:19<14:06,  1.66s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 520/1030 [14:21<13:54,  1.64s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 521/1030 [14:22<13:46,  1.62s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 522/1030 [14:24<13:39,  1.61s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 523/1030 [14:26<13:33,  1.60s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 524/1030 [14:27<13:27,  1.60s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 525/1030 [14:29<13:25,  1.59s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 526/1030 [14:30<13:21,  1.59s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 527/1030 [14:32<13:20,  1.59s/it]\u001b[A\n",
            "Iteration:  51%|█████▏    | 528/1030 [14:34<14:10,  1.69s/it]\u001b[A\n",
            "Iteration:  51%|█████▏    | 529/1030 [14:35<13:50,  1.66s/it]\u001b[A\n",
            "Iteration:  51%|█████▏    | 530/1030 [14:37<13:40,  1.64s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 531/1030 [14:39<13:32,  1.63s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 532/1030 [14:40<13:28,  1.62s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 533/1030 [14:42<13:21,  1.61s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 534/1030 [14:43<13:15,  1.60s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 535/1030 [14:45<13:12,  1.60s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 536/1030 [14:47<13:45,  1.67s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 537/1030 [14:48<13:28,  1.64s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 538/1030 [14:50<13:18,  1.62s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 539/1030 [14:52<13:13,  1.62s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 540/1030 [14:53<13:21,  1.64s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 541/1030 [14:55<13:12,  1.62s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 542/1030 [14:57<13:19,  1.64s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 543/1030 [14:58<13:21,  1.65s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 544/1030 [15:00<13:35,  1.68s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 545/1030 [15:02<13:35,  1.68s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 546/1030 [15:03<13:43,  1.70s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 547/1030 [15:05<14:03,  1.75s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 548/1030 [15:07<13:38,  1.70s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 549/1030 [15:09<13:47,  1.72s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 550/1030 [15:10<13:48,  1.73s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 551/1030 [15:12<13:25,  1.68s/it]\u001b[A\n",
            "Iteration:  54%|█████▎    | 552/1030 [15:14<13:10,  1.65s/it]\u001b[A\n",
            "Iteration:  54%|█████▎    | 553/1030 [15:15<13:01,  1.64s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 554/1030 [15:17<12:54,  1.63s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 555/1030 [15:19<13:16,  1.68s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 556/1030 [15:20<13:24,  1.70s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 557/1030 [15:22<13:05,  1.66s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 558/1030 [15:23<12:52,  1.64s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 559/1030 [15:25<13:08,  1.67s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 560/1030 [15:27<13:07,  1.67s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 561/1030 [15:29<13:16,  1.70s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 562/1030 [15:30<13:00,  1.67s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 563/1030 [15:32<12:47,  1.64s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 564/1030 [15:33<12:38,  1.63s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 565/1030 [15:35<12:32,  1.62s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 566/1030 [15:37<12:48,  1.66s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 567/1030 [15:38<12:37,  1.64s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 568/1030 [15:40<12:30,  1.62s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 569/1030 [15:42<12:26,  1.62s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 570/1030 [15:43<12:30,  1.63s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 571/1030 [15:45<12:22,  1.62s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 572/1030 [15:47<12:52,  1.69s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 573/1030 [15:48<12:47,  1.68s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 574/1030 [15:50<12:43,  1.68s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 575/1030 [15:52<12:32,  1.65s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 576/1030 [15:53<12:21,  1.63s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 577/1030 [15:55<12:27,  1.65s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 578/1030 [15:56<12:15,  1.63s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 579/1030 [15:58<12:09,  1.62s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 580/1030 [16:00<12:03,  1.61s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 581/1030 [16:01<12:00,  1.60s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 582/1030 [16:03<12:29,  1.67s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 583/1030 [16:05<12:18,  1.65s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 584/1030 [16:06<12:26,  1.67s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 585/1030 [16:08<12:13,  1.65s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 586/1030 [16:10<12:32,  1.70s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 587/1030 [16:11<12:16,  1.66s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 588/1030 [16:13<12:06,  1.64s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 589/1030 [16:14<11:55,  1.62s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 590/1030 [16:16<12:08,  1.66s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 591/1030 [16:18<11:56,  1.63s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 592/1030 [16:19<11:45,  1.61s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 593/1030 [16:21<12:00,  1.65s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 594/1030 [16:23<11:59,  1.65s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 595/1030 [16:25<12:26,  1.72s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 596/1030 [16:26<12:06,  1.67s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 597/1030 [16:28<12:01,  1.67s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 598/1030 [16:29<11:46,  1.64s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 599/1030 [16:31<11:40,  1.63s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 600/1030 [16:33<11:46,  1.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.5766717195510864  Mean Loss:  0.9801233073075613\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  58%|█████▊    | 601/1030 [16:34<11:57,  1.67s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 602/1030 [16:36<11:45,  1.65s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 603/1030 [16:38<12:08,  1.71s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 604/1030 [16:40<12:26,  1.75s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 605/1030 [16:41<12:06,  1.71s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 606/1030 [16:43<11:48,  1.67s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 607/1030 [16:44<11:36,  1.65s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 608/1030 [16:46<11:28,  1.63s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 609/1030 [16:48<11:20,  1.62s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 610/1030 [16:49<11:27,  1.64s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 611/1030 [16:51<11:21,  1.63s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 612/1030 [16:53<11:25,  1.64s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 613/1030 [16:54<11:15,  1.62s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 614/1030 [16:56<11:10,  1.61s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 615/1030 [16:57<11:05,  1.60s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 616/1030 [16:59<11:02,  1.60s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 617/1030 [17:01<10:57,  1.59s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 618/1030 [17:02<11:05,  1.62s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 619/1030 [17:04<11:22,  1.66s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 620/1030 [17:06<11:41,  1.71s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 621/1030 [17:08<11:44,  1.72s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 622/1030 [17:09<11:44,  1.73s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 623/1030 [17:11<11:26,  1.69s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 624/1030 [17:13<11:33,  1.71s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 625/1030 [17:14<11:38,  1.73s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 626/1030 [17:16<11:21,  1.69s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 627/1030 [17:18<11:17,  1.68s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 628/1030 [17:20<11:36,  1.73s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 629/1030 [17:21<11:19,  1.69s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 630/1030 [17:23<11:04,  1.66s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 631/1030 [17:24<11:02,  1.66s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 632/1030 [17:26<11:21,  1.71s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 633/1030 [17:28<11:23,  1.72s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 634/1030 [17:30<11:15,  1.71s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 635/1030 [17:31<11:11,  1.70s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 636/1030 [17:33<10:55,  1.66s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 637/1030 [17:35<10:54,  1.67s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 638/1030 [17:36<11:03,  1.69s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 639/1030 [17:38<11:00,  1.69s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 640/1030 [17:40<10:57,  1.69s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 641/1030 [17:42<11:14,  1.73s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 642/1030 [17:43<11:05,  1.71s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 643/1030 [17:45<11:06,  1.72s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 644/1030 [17:47<10:49,  1.68s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 645/1030 [17:48<10:36,  1.65s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 646/1030 [17:50<10:27,  1.63s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 647/1030 [17:51<10:32,  1.65s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 648/1030 [17:53<10:38,  1.67s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 649/1030 [17:55<10:27,  1.65s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 650/1030 [17:56<10:18,  1.63s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 651/1030 [17:58<10:09,  1.61s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 652/1030 [17:59<10:12,  1.62s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 653/1030 [18:01<10:14,  1.63s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 654/1030 [18:03<10:14,  1.64s/it]\u001b[A\n",
            "Iteration:  64%|██████▎   | 655/1030 [18:05<10:26,  1.67s/it]\u001b[A\n",
            "Iteration:  64%|██████▎   | 656/1030 [18:06<10:35,  1.70s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 657/1030 [18:08<10:39,  1.71s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 658/1030 [18:10<11:08,  1.80s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 659/1030 [18:12<11:22,  1.84s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 660/1030 [18:14<11:01,  1.79s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 661/1030 [18:15<10:48,  1.76s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 662/1030 [18:17<10:26,  1.70s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 663/1030 [18:19<10:12,  1.67s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 664/1030 [18:20<10:03,  1.65s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 665/1030 [18:22<09:54,  1.63s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 666/1030 [18:23<09:50,  1.62s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 667/1030 [18:25<09:45,  1.61s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 668/1030 [18:26<09:41,  1.61s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 669/1030 [18:28<09:54,  1.65s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 670/1030 [18:30<10:02,  1.67s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 671/1030 [18:32<09:50,  1.65s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 672/1030 [18:33<09:43,  1.63s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 673/1030 [18:35<09:35,  1.61s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 674/1030 [18:36<09:31,  1.61s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 675/1030 [18:38<09:26,  1.60s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 676/1030 [18:39<09:25,  1.60s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 677/1030 [18:41<09:23,  1.60s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 678/1030 [18:43<09:19,  1.59s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 679/1030 [18:44<09:17,  1.59s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 680/1030 [18:46<09:23,  1.61s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 681/1030 [18:48<09:45,  1.68s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 682/1030 [18:49<09:34,  1.65s/it]\u001b[A\n",
            "Iteration:  66%|██████▋   | 683/1030 [18:51<09:26,  1.63s/it]\u001b[A\n",
            "Iteration:  66%|██████▋   | 684/1030 [18:53<09:26,  1.64s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 685/1030 [18:54<09:18,  1.62s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 686/1030 [18:56<09:48,  1.71s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 687/1030 [18:58<09:33,  1.67s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 688/1030 [18:59<09:22,  1.65s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 689/1030 [19:01<09:22,  1.65s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 690/1030 [19:03<09:32,  1.68s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 691/1030 [19:04<09:22,  1.66s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 692/1030 [19:06<09:22,  1.66s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 693/1030 [19:08<09:39,  1.72s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 694/1030 [19:09<09:23,  1.68s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 695/1030 [19:11<09:10,  1.64s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 696/1030 [19:12<09:03,  1.63s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 697/1030 [19:14<08:59,  1.62s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 698/1030 [19:16<08:55,  1.61s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 699/1030 [19:17<09:08,  1.66s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 700/1030 [19:19<09:01,  1.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.5979896783828735  Mean Loss:  0.9680683867420469\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  68%|██████▊   | 701/1030 [19:21<09:05,  1.66s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 702/1030 [19:22<08:56,  1.64s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 703/1030 [19:24<09:06,  1.67s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 704/1030 [19:26<08:57,  1.65s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 705/1030 [19:27<08:59,  1.66s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 706/1030 [19:29<09:06,  1.69s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 707/1030 [19:31<09:02,  1.68s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 708/1030 [19:32<09:02,  1.68s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 709/1030 [19:34<08:51,  1.65s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 710/1030 [19:36<08:45,  1.64s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 711/1030 [19:37<08:39,  1.63s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 712/1030 [19:39<08:33,  1.62s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 713/1030 [19:41<08:53,  1.68s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 714/1030 [19:42<08:41,  1.65s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 715/1030 [19:44<08:33,  1.63s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 716/1030 [19:46<08:41,  1.66s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 717/1030 [19:47<08:32,  1.64s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 718/1030 [19:49<08:40,  1.67s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 719/1030 [19:51<08:33,  1.65s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 720/1030 [19:52<08:27,  1.64s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 721/1030 [19:54<08:27,  1.64s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 722/1030 [19:55<08:22,  1.63s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 723/1030 [19:57<08:15,  1.62s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 724/1030 [19:59<08:12,  1.61s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 725/1030 [20:00<08:08,  1.60s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 726/1030 [20:02<08:12,  1.62s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 727/1030 [20:04<08:22,  1.66s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 728/1030 [20:05<08:16,  1.64s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 729/1030 [20:07<08:10,  1.63s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 730/1030 [20:08<08:04,  1.61s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 731/1030 [20:10<08:00,  1.61s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 732/1030 [20:12<07:58,  1.61s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 733/1030 [20:13<08:03,  1.63s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 734/1030 [20:15<08:13,  1.67s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 735/1030 [20:17<08:13,  1.67s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 736/1030 [20:18<08:19,  1.70s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 737/1030 [20:20<08:06,  1.66s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 738/1030 [20:22<07:58,  1.64s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 739/1030 [20:23<07:52,  1.62s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 740/1030 [20:25<07:49,  1.62s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 741/1030 [20:26<07:51,  1.63s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 742/1030 [20:28<07:45,  1.62s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 743/1030 [20:30<07:55,  1.66s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 744/1030 [20:31<07:55,  1.66s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 745/1030 [20:33<08:01,  1.69s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 746/1030 [20:35<07:58,  1.68s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 747/1030 [20:37<08:01,  1.70s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 748/1030 [20:38<07:49,  1.66s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 749/1030 [20:40<07:41,  1.64s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 750/1030 [20:41<07:35,  1.63s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 751/1030 [20:43<07:31,  1.62s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 752/1030 [20:45<07:26,  1.61s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 753/1030 [20:46<07:35,  1.65s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 754/1030 [20:48<07:28,  1.63s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 755/1030 [20:50<07:29,  1.63s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 756/1030 [20:51<07:30,  1.64s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 757/1030 [20:53<07:36,  1.67s/it]\u001b[A\n",
            "Iteration:  74%|███████▎  | 758/1030 [20:55<07:29,  1.65s/it]\u001b[A\n",
            "Iteration:  74%|███████▎  | 759/1030 [20:56<07:34,  1.68s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 760/1030 [20:58<07:25,  1.65s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 761/1030 [21:00<07:25,  1.66s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 762/1030 [21:01<07:30,  1.68s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 763/1030 [21:03<07:41,  1.73s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 764/1030 [21:05<07:29,  1.69s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 765/1030 [21:06<07:26,  1.68s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 766/1030 [21:08<07:51,  1.78s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 767/1030 [21:10<07:40,  1.75s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 768/1030 [21:12<07:25,  1.70s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 769/1030 [21:13<07:13,  1.66s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 770/1030 [21:15<07:13,  1.67s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 771/1030 [21:16<07:05,  1.64s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 772/1030 [21:18<07:05,  1.65s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 773/1030 [21:20<06:58,  1.63s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 774/1030 [21:21<07:00,  1.64s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 775/1030 [21:23<06:53,  1.62s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 776/1030 [21:25<07:01,  1.66s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 777/1030 [21:26<06:53,  1.64s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 778/1030 [21:28<06:48,  1.62s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 779/1030 [21:29<06:42,  1.60s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 780/1030 [21:31<06:40,  1.60s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 781/1030 [21:33<06:45,  1.63s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 782/1030 [21:34<06:40,  1.62s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 783/1030 [21:36<06:37,  1.61s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 784/1030 [21:38<06:42,  1.64s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 785/1030 [21:39<06:36,  1.62s/it]\u001b[A\n",
            "Iteration:  76%|███████▋  | 786/1030 [21:41<06:38,  1.63s/it]\u001b[A\n",
            "Iteration:  76%|███████▋  | 787/1030 [21:42<06:33,  1.62s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 788/1030 [21:44<06:28,  1.61s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 789/1030 [21:46<06:25,  1.60s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 790/1030 [21:47<06:23,  1.60s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 791/1030 [21:49<06:20,  1.59s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 792/1030 [21:50<06:19,  1.60s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 793/1030 [21:52<06:28,  1.64s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 794/1030 [21:54<06:23,  1.62s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 795/1030 [21:55<06:18,  1.61s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 796/1030 [21:57<06:19,  1.62s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 797/1030 [21:59<06:15,  1.61s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 798/1030 [22:00<06:23,  1.65s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 799/1030 [22:02<06:16,  1.63s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 800/1030 [22:04<06:23,  1.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.7344430685043335  Mean Loss:  0.9657560143247247\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  78%|███████▊  | 801/1030 [22:05<06:16,  1.64s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 802/1030 [22:07<06:10,  1.62s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 803/1030 [22:08<06:11,  1.64s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 804/1030 [22:10<06:06,  1.62s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 805/1030 [22:12<06:02,  1.61s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 806/1030 [22:13<05:57,  1.60s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 807/1030 [22:15<05:55,  1.60s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 808/1030 [22:16<05:54,  1.60s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 809/1030 [22:18<06:03,  1.64s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 810/1030 [22:20<05:57,  1.62s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 811/1030 [22:21<05:53,  1.61s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 812/1030 [22:23<05:49,  1.60s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 813/1030 [22:24<05:48,  1.61s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 814/1030 [22:26<05:46,  1.61s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 815/1030 [22:28<05:44,  1.60s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 816/1030 [22:30<05:58,  1.67s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 817/1030 [22:31<06:01,  1.70s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 818/1030 [22:33<05:54,  1.67s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 819/1030 [22:34<05:46,  1.64s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 820/1030 [22:36<05:45,  1.65s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 821/1030 [22:38<05:40,  1.63s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 822/1030 [22:39<05:47,  1.67s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 823/1030 [22:41<05:39,  1.64s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 824/1030 [22:43<05:50,  1.70s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 825/1030 [22:44<05:41,  1.67s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 826/1030 [22:46<05:34,  1.64s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 827/1030 [22:48<05:35,  1.65s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 828/1030 [22:49<05:30,  1.63s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 829/1030 [22:51<05:26,  1.62s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 830/1030 [22:53<05:22,  1.61s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 831/1030 [22:54<05:19,  1.60s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 832/1030 [22:56<05:22,  1.63s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 833/1030 [22:57<05:18,  1.61s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 834/1030 [22:59<05:15,  1.61s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 835/1030 [23:01<05:11,  1.60s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 836/1030 [23:02<05:08,  1.59s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 837/1030 [23:04<05:06,  1.59s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 838/1030 [23:05<05:09,  1.61s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 839/1030 [23:07<05:05,  1.60s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 840/1030 [23:09<05:03,  1.60s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 841/1030 [23:10<05:05,  1.62s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 842/1030 [23:12<05:03,  1.61s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 843/1030 [23:13<04:59,  1.60s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 844/1030 [23:15<05:00,  1.61s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 845/1030 [23:17<05:03,  1.64s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 846/1030 [23:18<04:59,  1.63s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 847/1030 [23:20<04:55,  1.62s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 848/1030 [23:22<04:58,  1.64s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 849/1030 [23:23<05:03,  1.67s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 850/1030 [23:25<04:57,  1.65s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 851/1030 [23:27<04:52,  1.63s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 852/1030 [23:28<04:51,  1.64s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 853/1030 [23:30<04:47,  1.62s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 854/1030 [23:31<04:43,  1.61s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 855/1030 [23:33<04:41,  1.61s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 856/1030 [23:35<04:46,  1.65s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 857/1030 [23:36<04:41,  1.63s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 858/1030 [23:38<04:45,  1.66s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 859/1030 [23:40<04:49,  1.69s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 860/1030 [23:41<04:42,  1.66s/it]\u001b[A\n",
            "Iteration:  84%|████████▎ | 861/1030 [23:43<04:37,  1.64s/it]\u001b[A\n",
            "Iteration:  84%|████████▎ | 862/1030 [23:45<04:36,  1.65s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 863/1030 [23:46<04:36,  1.65s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 864/1030 [23:48<04:38,  1.68s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 865/1030 [23:50<04:32,  1.65s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 866/1030 [23:51<04:31,  1.66s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 867/1030 [23:53<04:27,  1.64s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 868/1030 [23:55<04:26,  1.65s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 869/1030 [23:56<04:22,  1.63s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 870/1030 [23:58<04:18,  1.62s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 871/1030 [23:59<04:19,  1.64s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 872/1030 [24:01<04:27,  1.69s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 873/1030 [24:03<04:24,  1.69s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 874/1030 [24:05<04:25,  1.70s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 875/1030 [24:06<04:26,  1.72s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 876/1030 [24:08<04:30,  1.75s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 877/1030 [24:10<04:20,  1.71s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 878/1030 [24:11<04:14,  1.67s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 879/1030 [24:13<04:16,  1.70s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 880/1030 [24:15<04:10,  1.67s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 881/1030 [24:16<04:04,  1.64s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 882/1030 [24:18<04:11,  1.70s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 883/1030 [24:20<04:05,  1.67s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 884/1030 [24:22<04:15,  1.75s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 885/1030 [24:23<04:06,  1.70s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 886/1030 [24:25<03:59,  1.66s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 887/1030 [24:27<04:01,  1.69s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 888/1030 [24:28<03:55,  1.66s/it]\u001b[A\n",
            "Iteration:  86%|████████▋ | 889/1030 [24:30<03:54,  1.66s/it]\u001b[A\n",
            "Iteration:  86%|████████▋ | 890/1030 [24:32<03:52,  1.66s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 891/1030 [24:33<03:57,  1.71s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 892/1030 [24:35<03:50,  1.67s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 893/1030 [24:37<03:52,  1.70s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 894/1030 [24:39<04:03,  1.79s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 895/1030 [24:40<03:56,  1.75s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 896/1030 [24:42<03:48,  1.71s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 897/1030 [24:44<03:42,  1.67s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 898/1030 [24:45<03:40,  1.67s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 899/1030 [24:47<03:35,  1.64s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 900/1030 [24:48<03:31,  1.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.432781457901001  Mean Loss:  0.9673556589417988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  87%|████████▋ | 901/1030 [24:50<03:28,  1.61s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 902/1030 [24:52<03:38,  1.71s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 903/1030 [24:53<03:31,  1.67s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 904/1030 [24:55<03:26,  1.64s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 905/1030 [24:57<03:23,  1.63s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 906/1030 [24:59<03:29,  1.69s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 907/1030 [25:00<03:29,  1.71s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 908/1030 [25:02<03:23,  1.67s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 909/1030 [25:03<03:21,  1.66s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 910/1030 [25:05<03:17,  1.64s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 911/1030 [25:07<03:16,  1.65s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 912/1030 [25:09<03:21,  1.71s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 913/1030 [25:10<03:21,  1.72s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 914/1030 [25:12<03:14,  1.67s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 915/1030 [25:14<03:15,  1.70s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 916/1030 [25:15<03:09,  1.66s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 917/1030 [25:17<03:05,  1.64s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 918/1030 [25:18<03:04,  1.65s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 919/1030 [25:20<03:01,  1.64s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 920/1030 [25:22<02:57,  1.62s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 921/1030 [25:23<02:55,  1.61s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 922/1030 [25:25<02:53,  1.60s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 923/1030 [25:26<02:50,  1.59s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 924/1030 [25:28<02:48,  1.59s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 925/1030 [25:30<02:46,  1.58s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 926/1030 [25:31<02:47,  1.61s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 927/1030 [25:33<02:50,  1.66s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 928/1030 [25:35<02:47,  1.64s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 929/1030 [25:36<02:44,  1.63s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 930/1030 [25:38<02:41,  1.62s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 931/1030 [25:39<02:39,  1.61s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 932/1030 [25:41<02:37,  1.61s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 933/1030 [25:43<02:37,  1.63s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 934/1030 [25:44<02:34,  1.61s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 935/1030 [25:46<02:32,  1.60s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 936/1030 [25:48<02:32,  1.62s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 937/1030 [25:49<02:30,  1.61s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 938/1030 [25:51<02:30,  1.63s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 939/1030 [25:52<02:27,  1.62s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 940/1030 [25:54<02:25,  1.61s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 941/1030 [25:56<02:22,  1.60s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 942/1030 [25:57<02:22,  1.62s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 943/1030 [25:59<02:24,  1.66s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 944/1030 [26:01<02:22,  1.66s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 945/1030 [26:02<02:23,  1.69s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 946/1030 [26:04<02:23,  1.71s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 947/1030 [26:06<02:22,  1.72s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 948/1030 [26:08<02:21,  1.73s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 949/1030 [26:09<02:16,  1.69s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 950/1030 [26:11<02:12,  1.66s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 951/1030 [26:12<02:11,  1.67s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 952/1030 [26:14<02:08,  1.65s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 953/1030 [26:16<02:07,  1.65s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 954/1030 [26:17<02:06,  1.66s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 955/1030 [26:19<02:04,  1.66s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 956/1030 [26:21<02:00,  1.63s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 957/1030 [26:22<02:00,  1.65s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 958/1030 [26:24<01:59,  1.65s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 959/1030 [26:26<02:01,  1.71s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 960/1030 [26:28<02:00,  1.73s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 961/1030 [26:29<01:56,  1.69s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 962/1030 [26:31<01:52,  1.66s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 963/1030 [26:32<01:51,  1.66s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 964/1030 [26:34<01:48,  1.64s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 965/1030 [26:36<01:50,  1.70s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 966/1030 [26:38<01:48,  1.70s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 967/1030 [26:39<01:44,  1.66s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 968/1030 [26:41<01:41,  1.64s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 969/1030 [26:42<01:39,  1.63s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 970/1030 [26:44<01:36,  1.62s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 971/1030 [26:46<01:36,  1.63s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 972/1030 [26:47<01:34,  1.62s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 973/1030 [26:49<01:31,  1.61s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 974/1030 [26:50<01:29,  1.60s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 975/1030 [26:52<01:28,  1.60s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 976/1030 [26:54<01:26,  1.60s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 977/1030 [26:55<01:24,  1.60s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 978/1030 [26:57<01:24,  1.62s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 979/1030 [26:58<01:22,  1.61s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 980/1030 [27:00<01:20,  1.61s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 981/1030 [27:02<01:18,  1.61s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 982/1030 [27:03<01:17,  1.60s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 983/1030 [27:05<01:16,  1.63s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 984/1030 [27:07<01:15,  1.64s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 985/1030 [27:08<01:13,  1.63s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 986/1030 [27:10<01:10,  1.61s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 987/1030 [27:11<01:10,  1.63s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 988/1030 [27:13<01:07,  1.61s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 989/1030 [27:15<01:05,  1.60s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 990/1030 [27:16<01:04,  1.60s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 991/1030 [27:18<01:04,  1.65s/it]\u001b[A\n",
            "Iteration:  96%|█████████▋| 992/1030 [27:20<01:01,  1.63s/it]\u001b[A\n",
            "Iteration:  96%|█████████▋| 993/1030 [27:21<01:00,  1.62s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 994/1030 [27:23<00:58,  1.62s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 995/1030 [27:24<00:56,  1.61s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 996/1030 [27:26<00:54,  1.61s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 997/1030 [27:28<00:53,  1.63s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 998/1030 [27:29<00:51,  1.62s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 999/1030 [27:31<00:50,  1.64s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 1000/1030 [27:32<00:48,  1.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.168868899345398  Mean Loss:  0.9592523362040519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  97%|█████████▋| 1001/1030 [27:34<00:46,  1.62s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 1002/1030 [27:36<00:45,  1.61s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 1003/1030 [27:37<00:44,  1.65s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 1004/1030 [27:39<00:42,  1.65s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1005/1030 [27:41<00:40,  1.63s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1006/1030 [27:43<00:40,  1.69s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1007/1030 [27:44<00:38,  1.66s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1008/1030 [27:46<00:36,  1.64s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1009/1030 [27:47<00:34,  1.62s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1010/1030 [27:49<00:32,  1.61s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1011/1030 [27:50<00:30,  1.60s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1012/1030 [27:52<00:28,  1.60s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1013/1030 [27:54<00:28,  1.65s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1014/1030 [27:55<00:26,  1.64s/it]\u001b[A\n",
            "Iteration:  99%|█████████▊| 1015/1030 [27:57<00:24,  1.62s/it]\u001b[A\n",
            "Iteration:  99%|█████████▊| 1016/1030 [27:59<00:23,  1.69s/it]\u001b[A\n",
            "Iteration:  99%|█████████▊| 1017/1030 [28:00<00:21,  1.68s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1018/1030 [28:02<00:19,  1.66s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1019/1030 [28:04<00:18,  1.67s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1020/1030 [28:06<00:17,  1.73s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1021/1030 [28:07<00:15,  1.68s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1022/1030 [28:09<00:13,  1.64s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1023/1030 [28:10<00:11,  1.62s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1024/1030 [28:12<00:09,  1.61s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1025/1030 [28:13<00:07,  1.60s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1026/1030 [28:15<00:06,  1.59s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1027/1030 [28:17<00:04,  1.58s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1028/1030 [28:18<00:03,  1.58s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1029/1030 [28:20<00:01,  1.57s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 1030/1030 [28:20<00:00,  1.30s/it]\u001b[A\n",
            "Epoch:  67%|██████▋   | 2/3 [56:26<28:09, 1689.88s/it]\n",
            "Iteration:   0%|          | 0/1030 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 1/1030 [00:01<28:35,  1.67s/it]\u001b[A\n",
            "Iteration:   0%|          | 2/1030 [00:03<28:34,  1.67s/it]\u001b[A\n",
            "Iteration:   0%|          | 3/1030 [00:04<28:05,  1.64s/it]\u001b[A\n",
            "Iteration:   0%|          | 4/1030 [00:06<27:46,  1.62s/it]\u001b[A\n",
            "Iteration:   0%|          | 5/1030 [00:08<27:28,  1.61s/it]\u001b[A\n",
            "Iteration:   1%|          | 6/1030 [00:09<27:18,  1.60s/it]\u001b[A\n",
            "Iteration:   1%|          | 7/1030 [00:11<27:05,  1.59s/it]\u001b[A\n",
            "Iteration:   1%|          | 8/1030 [00:12<27:04,  1.59s/it]\u001b[A\n",
            "Iteration:   1%|          | 9/1030 [00:14<26:59,  1.59s/it]\u001b[A\n",
            "Iteration:   1%|          | 10/1030 [00:16<27:47,  1.63s/it]\u001b[A\n",
            "Iteration:   1%|          | 11/1030 [00:17<28:19,  1.67s/it]\u001b[A\n",
            "Iteration:   1%|          | 12/1030 [00:19<28:39,  1.69s/it]\u001b[A\n",
            "Iteration:   1%|▏         | 13/1030 [00:21<28:07,  1.66s/it]\u001b[A\n",
            "Iteration:   1%|▏         | 14/1030 [00:22<27:41,  1.63s/it]\u001b[A\n",
            "Iteration:   1%|▏         | 15/1030 [00:24<27:50,  1.65s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 16/1030 [00:26<28:22,  1.68s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 17/1030 [00:27<27:51,  1.65s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 18/1030 [00:29<27:36,  1.64s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 19/1030 [00:31<27:41,  1.64s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 20/1030 [00:32<28:43,  1.71s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 21/1030 [00:34<29:19,  1.74s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 22/1030 [00:36<28:51,  1.72s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 23/1030 [00:37<28:12,  1.68s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 24/1030 [00:39<28:05,  1.68s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 25/1030 [00:41<28:03,  1.67s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 26/1030 [00:43<28:00,  1.67s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 27/1030 [00:44<27:36,  1.65s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 28/1030 [00:46<27:17,  1.63s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 29/1030 [00:47<27:48,  1.67s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 30/1030 [00:49<27:25,  1.65s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 31/1030 [00:51<27:09,  1.63s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 32/1030 [00:52<27:26,  1.65s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 33/1030 [00:54<27:02,  1.63s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 34/1030 [00:55<26:50,  1.62s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 35/1030 [00:57<26:43,  1.61s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 36/1030 [00:59<27:51,  1.68s/it]\u001b[A\n",
            "Iteration:   4%|▎         | 37/1030 [01:01<28:38,  1.73s/it]\u001b[A\n",
            "Iteration:   4%|▎         | 38/1030 [01:02<27:52,  1.69s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 39/1030 [01:04<27:19,  1.65s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 40/1030 [01:06<27:00,  1.64s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 41/1030 [01:07<27:33,  1.67s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 42/1030 [01:09<27:53,  1.69s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 43/1030 [01:11<27:21,  1.66s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 44/1030 [01:12<26:57,  1.64s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 45/1030 [01:14<26:38,  1.62s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 46/1030 [01:15<26:54,  1.64s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 47/1030 [01:17<28:18,  1.73s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 48/1030 [01:19<27:34,  1.69s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 49/1030 [01:21<27:52,  1.71s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 50/1030 [01:23<28:32,  1.75s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 51/1030 [01:24<28:28,  1.75s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 52/1030 [01:26<27:40,  1.70s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 53/1030 [01:27<27:02,  1.66s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 54/1030 [01:29<27:00,  1.66s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 55/1030 [01:31<26:38,  1.64s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 56/1030 [01:32<26:21,  1.62s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 57/1030 [01:34<26:16,  1.62s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 58/1030 [01:36<26:03,  1.61s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 59/1030 [01:37<25:55,  1.60s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 60/1030 [01:39<25:45,  1.59s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 61/1030 [01:40<25:44,  1.59s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 62/1030 [01:42<25:39,  1.59s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 63/1030 [01:44<26:19,  1.63s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 64/1030 [01:45<26:51,  1.67s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 65/1030 [01:47<27:07,  1.69s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 66/1030 [01:49<26:38,  1.66s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 67/1030 [01:50<27:07,  1.69s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 68/1030 [01:52<26:36,  1.66s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 69/1030 [01:54<26:14,  1.64s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 70/1030 [01:55<25:58,  1.62s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 71/1030 [01:57<26:33,  1.66s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 72/1030 [01:59<26:11,  1.64s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 73/1030 [02:00<26:19,  1.65s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 74/1030 [02:02<26:06,  1.64s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 75/1030 [02:03<25:49,  1.62s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 76/1030 [02:05<25:40,  1.62s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 77/1030 [02:07<27:12,  1.71s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 78/1030 [02:09<26:34,  1.68s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 79/1030 [02:10<26:29,  1.67s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 80/1030 [02:12<26:49,  1.69s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 81/1030 [02:14<26:15,  1.66s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 82/1030 [02:15<25:51,  1.64s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 83/1030 [02:17<26:02,  1.65s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 84/1030 [02:18<25:37,  1.62s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 85/1030 [02:20<25:24,  1.61s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 86/1030 [02:22<25:16,  1.61s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 87/1030 [02:23<25:12,  1.60s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 88/1030 [02:25<25:31,  1.63s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 89/1030 [02:26<25:19,  1.61s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 90/1030 [02:28<26:45,  1.71s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 91/1030 [02:30<26:36,  1.70s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 92/1030 [02:32<26:50,  1.72s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 93/1030 [02:33<26:43,  1.71s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 94/1030 [02:35<26:34,  1.70s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 95/1030 [02:37<26:02,  1.67s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 96/1030 [02:38<26:02,  1.67s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 97/1030 [02:40<27:12,  1.75s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 98/1030 [02:42<26:50,  1.73s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 99/1030 [02:44<26:12,  1.69s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 100/1030 [02:45<25:45,  1.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.8850438594818115  Mean Loss:  0.8467750158905983\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  10%|▉         | 101/1030 [02:47<25:44,  1.66s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 102/1030 [02:49<25:42,  1.66s/it]\u001b[A\n",
            "Iteration:  10%|█         | 103/1030 [02:50<25:38,  1.66s/it]\u001b[A\n",
            "Iteration:  10%|█         | 104/1030 [02:52<25:18,  1.64s/it]\u001b[A\n",
            "Iteration:  10%|█         | 105/1030 [02:54<26:12,  1.70s/it]\u001b[A\n",
            "Iteration:  10%|█         | 106/1030 [02:55<26:22,  1.71s/it]\u001b[A\n",
            "Iteration:  10%|█         | 107/1030 [02:57<25:44,  1.67s/it]\u001b[A\n",
            "Iteration:  10%|█         | 108/1030 [02:59<25:20,  1.65s/it]\u001b[A\n",
            "Iteration:  11%|█         | 109/1030 [03:00<25:25,  1.66s/it]\u001b[A\n",
            "Iteration:  11%|█         | 110/1030 [03:02<25:30,  1.66s/it]\u001b[A\n",
            "Iteration:  11%|█         | 111/1030 [03:03<25:07,  1.64s/it]\u001b[A\n",
            "Iteration:  11%|█         | 112/1030 [03:05<24:48,  1.62s/it]\u001b[A\n",
            "Iteration:  11%|█         | 113/1030 [03:07<24:33,  1.61s/it]\u001b[A\n",
            "Iteration:  11%|█         | 114/1030 [03:08<24:23,  1.60s/it]\u001b[A\n",
            "Iteration:  11%|█         | 115/1030 [03:10<24:20,  1.60s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 116/1030 [03:11<24:09,  1.59s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 117/1030 [03:13<24:55,  1.64s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 118/1030 [03:15<24:39,  1.62s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 119/1030 [03:16<25:09,  1.66s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 120/1030 [03:18<24:51,  1.64s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 121/1030 [03:20<25:43,  1.70s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 122/1030 [03:22<25:31,  1.69s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 123/1030 [03:23<25:25,  1.68s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 124/1030 [03:25<25:39,  1.70s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 125/1030 [03:27<26:18,  1.74s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 126/1030 [03:28<25:52,  1.72s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 127/1030 [03:30<25:14,  1.68s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 128/1030 [03:32<25:56,  1.73s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 129/1030 [03:34<26:01,  1.73s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 130/1030 [03:35<25:19,  1.69s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 131/1030 [03:37<24:54,  1.66s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 132/1030 [03:38<24:52,  1.66s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 133/1030 [03:40<24:36,  1.65s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 134/1030 [03:42<24:21,  1.63s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 135/1030 [03:44<25:18,  1.70s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 136/1030 [03:45<25:54,  1.74s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 137/1030 [03:47<25:09,  1.69s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 138/1030 [03:49<24:37,  1.66s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 139/1030 [03:50<24:19,  1.64s/it]\u001b[A\n",
            "Iteration:  14%|█▎        | 140/1030 [03:52<24:05,  1.62s/it]\u001b[A\n",
            "Iteration:  14%|█▎        | 141/1030 [03:53<23:55,  1.62s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 142/1030 [03:55<23:48,  1.61s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 143/1030 [03:56<23:39,  1.60s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 144/1030 [03:58<23:37,  1.60s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 145/1030 [04:00<23:27,  1.59s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 146/1030 [04:01<23:21,  1.59s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 147/1030 [04:03<23:21,  1.59s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 148/1030 [04:05<24:00,  1.63s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 149/1030 [04:06<23:45,  1.62s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 150/1030 [04:08<23:38,  1.61s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 151/1030 [04:09<23:35,  1.61s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 152/1030 [04:11<24:33,  1.68s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 153/1030 [04:13<24:31,  1.68s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 154/1030 [04:15<24:49,  1.70s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 155/1030 [04:16<24:42,  1.69s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 156/1030 [04:18<24:32,  1.69s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 157/1030 [04:20<24:08,  1.66s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 158/1030 [04:21<24:30,  1.69s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 159/1030 [04:23<24:27,  1.68s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 160/1030 [04:25<24:47,  1.71s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 161/1030 [04:26<24:19,  1.68s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 162/1030 [04:28<24:15,  1.68s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 163/1030 [04:30<24:57,  1.73s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 164/1030 [04:32<24:58,  1.73s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 165/1030 [04:33<24:56,  1.73s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 166/1030 [04:35<25:02,  1.74s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 167/1030 [04:37<24:40,  1.72s/it]\u001b[A\n",
            "Iteration:  16%|█▋        | 168/1030 [04:38<24:01,  1.67s/it]\u001b[A\n",
            "Iteration:  16%|█▋        | 169/1030 [04:40<24:00,  1.67s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 170/1030 [04:42<23:39,  1.65s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 171/1030 [04:43<23:20,  1.63s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 172/1030 [04:45<23:28,  1.64s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 173/1030 [04:47<23:31,  1.65s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 174/1030 [04:48<23:35,  1.65s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 175/1030 [04:50<23:38,  1.66s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 176/1030 [04:52<23:35,  1.66s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 177/1030 [04:53<23:11,  1.63s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 178/1030 [04:55<22:57,  1.62s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 179/1030 [04:56<22:46,  1.61s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 180/1030 [04:58<22:40,  1.60s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 181/1030 [05:00<23:14,  1.64s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 182/1030 [05:01<22:59,  1.63s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 183/1030 [05:03<22:52,  1.62s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 184/1030 [05:04<22:41,  1.61s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 185/1030 [05:06<22:34,  1.60s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 186/1030 [05:08<23:12,  1.65s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 187/1030 [05:09<23:18,  1.66s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 188/1030 [05:11<23:00,  1.64s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 189/1030 [05:13<22:46,  1.63s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 190/1030 [05:14<22:35,  1.61s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 191/1030 [05:16<22:24,  1.60s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 192/1030 [05:17<22:24,  1.60s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 193/1030 [05:19<22:24,  1.61s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 194/1030 [05:21<22:21,  1.60s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 195/1030 [05:22<22:52,  1.64s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 196/1030 [05:24<22:36,  1.63s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 197/1030 [05:25<22:23,  1.61s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 198/1030 [05:27<22:14,  1.60s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 199/1030 [05:29<23:08,  1.67s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 200/1030 [05:31<23:00,  1.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.0384163856506348  Mean Loss:  0.8501399482786656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  20%|█▉        | 201/1030 [05:32<23:21,  1.69s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 202/1030 [05:34<23:33,  1.71s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 203/1030 [05:36<24:07,  1.75s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 204/1030 [05:38<24:06,  1.75s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 205/1030 [05:39<23:27,  1.71s/it]\u001b[A\n",
            "Iteration:  20%|██        | 206/1030 [05:41<22:54,  1.67s/it]\u001b[A\n",
            "Iteration:  20%|██        | 207/1030 [05:42<22:33,  1.64s/it]\u001b[A\n",
            "Iteration:  20%|██        | 208/1030 [05:44<22:39,  1.65s/it]\u001b[A\n",
            "Iteration:  20%|██        | 209/1030 [05:46<22:20,  1.63s/it]\u001b[A\n",
            "Iteration:  20%|██        | 210/1030 [05:47<22:06,  1.62s/it]\u001b[A\n",
            "Iteration:  20%|██        | 211/1030 [05:49<21:55,  1.61s/it]\u001b[A\n",
            "Iteration:  21%|██        | 212/1030 [05:50<22:08,  1.62s/it]\u001b[A\n",
            "Iteration:  21%|██        | 213/1030 [05:52<21:59,  1.61s/it]\u001b[A\n",
            "Iteration:  21%|██        | 214/1030 [05:54<21:54,  1.61s/it]\u001b[A\n",
            "Iteration:  21%|██        | 215/1030 [05:55<21:49,  1.61s/it]\u001b[A\n",
            "Iteration:  21%|██        | 216/1030 [05:57<22:21,  1.65s/it]\u001b[A\n",
            "Iteration:  21%|██        | 217/1030 [05:59<22:47,  1.68s/it]\u001b[A\n",
            "Iteration:  21%|██        | 218/1030 [06:01<23:05,  1.71s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 219/1030 [06:02<23:16,  1.72s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 220/1030 [06:04<22:41,  1.68s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 221/1030 [06:06<22:57,  1.70s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 222/1030 [06:07<22:31,  1.67s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 223/1030 [06:09<22:28,  1.67s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 224/1030 [06:11<22:28,  1.67s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 225/1030 [06:12<22:42,  1.69s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 226/1030 [06:14<22:16,  1.66s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 227/1030 [06:15<21:54,  1.64s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 228/1030 [06:17<21:58,  1.64s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 229/1030 [06:19<21:40,  1.62s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 230/1030 [06:20<21:29,  1.61s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 231/1030 [06:22<21:18,  1.60s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 232/1030 [06:23<21:13,  1.60s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 233/1030 [06:25<21:12,  1.60s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 234/1030 [06:27<21:08,  1.59s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 235/1030 [06:28<21:40,  1.64s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 236/1030 [06:30<21:28,  1.62s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 237/1030 [06:32<21:15,  1.61s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 238/1030 [06:33<21:09,  1.60s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 239/1030 [06:35<21:06,  1.60s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 240/1030 [06:36<21:37,  1.64s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 241/1030 [06:38<21:24,  1.63s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 242/1030 [06:40<21:36,  1.65s/it]\u001b[A\n",
            "Iteration:  24%|██▎       | 243/1030 [06:42<22:40,  1.73s/it]\u001b[A\n",
            "Iteration:  24%|██▎       | 244/1030 [06:43<22:27,  1.71s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 245/1030 [06:45<22:35,  1.73s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 246/1030 [06:47<22:44,  1.74s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 247/1030 [06:49<22:22,  1.71s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 248/1030 [06:50<21:53,  1.68s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 249/1030 [06:52<22:07,  1.70s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 250/1030 [06:54<22:19,  1.72s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 251/1030 [06:55<21:48,  1.68s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 252/1030 [06:57<21:42,  1.67s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 253/1030 [06:58<21:22,  1.65s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 254/1030 [07:00<21:06,  1.63s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 255/1030 [07:02<21:34,  1.67s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 256/1030 [07:04<21:51,  1.69s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 257/1030 [07:05<22:01,  1.71s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 258/1030 [07:07<21:30,  1.67s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 259/1030 [07:09<21:48,  1.70s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 260/1030 [07:10<21:20,  1.66s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 261/1030 [07:12<21:39,  1.69s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 262/1030 [07:14<21:17,  1.66s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 263/1030 [07:15<21:17,  1.67s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 264/1030 [07:17<21:15,  1.66s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 265/1030 [07:19<20:55,  1.64s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 266/1030 [07:20<21:40,  1.70s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 267/1030 [07:22<21:29,  1.69s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 268/1030 [07:24<21:03,  1.66s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 269/1030 [07:25<20:39,  1.63s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 270/1030 [07:27<21:07,  1.67s/it]\u001b[A\n",
            "Iteration:  26%|██▋       | 271/1030 [07:29<20:47,  1.64s/it]\u001b[A\n",
            "Iteration:  26%|██▋       | 272/1030 [07:31<22:35,  1.79s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 273/1030 [07:32<22:03,  1.75s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 274/1030 [07:34<21:25,  1.70s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 275/1030 [07:36<21:39,  1.72s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 276/1030 [07:37<21:43,  1.73s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 277/1030 [07:39<21:45,  1.73s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 278/1030 [07:41<22:04,  1.76s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 279/1030 [07:43<21:22,  1.71s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 280/1030 [07:44<21:51,  1.75s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 281/1030 [07:46<21:31,  1.72s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 282/1030 [07:48<20:55,  1.68s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 283/1030 [07:49<20:50,  1.67s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 284/1030 [07:51<20:29,  1.65s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 285/1030 [07:53<20:56,  1.69s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 286/1030 [07:54<21:06,  1.70s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 287/1030 [07:56<20:53,  1.69s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 288/1030 [07:58<20:30,  1.66s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 289/1030 [07:59<20:10,  1.63s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 290/1030 [08:01<20:02,  1.63s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 291/1030 [08:02<19:48,  1.61s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 292/1030 [08:04<20:04,  1.63s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 293/1030 [08:06<20:14,  1.65s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 294/1030 [08:07<19:59,  1.63s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 295/1030 [08:09<20:07,  1.64s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 296/1030 [08:11<20:28,  1.67s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 297/1030 [08:12<20:24,  1.67s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 298/1030 [08:14<20:25,  1.67s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 299/1030 [08:16<20:04,  1.65s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 300/1030 [08:17<20:06,  1.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.6507776975631714  Mean Loss:  0.852971196770668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  29%|██▉       | 301/1030 [08:19<20:24,  1.68s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 302/1030 [08:21<20:04,  1.66s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 303/1030 [08:22<19:47,  1.63s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 304/1030 [08:24<19:40,  1.63s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 305/1030 [08:26<20:28,  1.69s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 306/1030 [08:27<20:05,  1.67s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 307/1030 [08:29<19:47,  1.64s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 308/1030 [08:31<19:33,  1.63s/it]\u001b[A\n",
            "Iteration:  30%|███       | 309/1030 [08:32<20:16,  1.69s/it]\u001b[A\n",
            "Iteration:  30%|███       | 310/1030 [08:34<20:29,  1.71s/it]\u001b[A\n",
            "Iteration:  30%|███       | 311/1030 [08:36<19:58,  1.67s/it]\u001b[A\n",
            "Iteration:  30%|███       | 312/1030 [08:37<19:38,  1.64s/it]\u001b[A\n",
            "Iteration:  30%|███       | 313/1030 [08:39<19:28,  1.63s/it]\u001b[A\n",
            "Iteration:  30%|███       | 314/1030 [08:40<19:14,  1.61s/it]\u001b[A\n",
            "Iteration:  31%|███       | 315/1030 [08:42<19:41,  1.65s/it]\u001b[A\n",
            "Iteration:  31%|███       | 316/1030 [08:44<19:28,  1.64s/it]\u001b[A\n",
            "Iteration:  31%|███       | 317/1030 [08:45<19:15,  1.62s/it]\u001b[A\n",
            "Iteration:  31%|███       | 318/1030 [08:47<19:40,  1.66s/it]\u001b[A\n",
            "Iteration:  31%|███       | 319/1030 [08:49<19:24,  1.64s/it]\u001b[A\n",
            "Iteration:  31%|███       | 320/1030 [08:50<19:24,  1.64s/it]\u001b[A\n",
            "Iteration:  31%|███       | 321/1030 [08:52<19:12,  1.63s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 322/1030 [08:54<18:59,  1.61s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 323/1030 [08:55<19:29,  1.65s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 324/1030 [08:57<19:15,  1.64s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 325/1030 [08:59<19:36,  1.67s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 326/1030 [09:00<19:50,  1.69s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 327/1030 [09:02<19:45,  1.69s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 328/1030 [09:04<19:22,  1.66s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 329/1030 [09:05<19:41,  1.69s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 330/1030 [09:07<19:39,  1.69s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 331/1030 [09:09<19:51,  1.70s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 332/1030 [09:11<19:56,  1.71s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 333/1030 [09:12<20:23,  1.76s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 334/1030 [09:14<20:40,  1.78s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 335/1030 [09:16<20:48,  1.80s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 336/1030 [09:18<20:21,  1.76s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 337/1030 [09:19<20:13,  1.75s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 338/1030 [09:21<20:08,  1.75s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 339/1030 [09:23<20:06,  1.75s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 340/1030 [09:25<20:07,  1.75s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 341/1030 [09:26<19:31,  1.70s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 342/1030 [09:28<19:41,  1.72s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 343/1030 [09:30<19:10,  1.67s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 344/1030 [09:31<19:44,  1.73s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 345/1030 [09:33<19:43,  1.73s/it]\u001b[A\n",
            "Iteration:  34%|███▎      | 346/1030 [09:35<19:14,  1.69s/it]\u001b[A\n",
            "Iteration:  34%|███▎      | 347/1030 [09:37<19:26,  1.71s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 348/1030 [09:38<19:00,  1.67s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 349/1030 [09:40<18:55,  1.67s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 350/1030 [09:41<18:37,  1.64s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 351/1030 [09:43<18:41,  1.65s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 352/1030 [09:45<18:30,  1.64s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 353/1030 [09:46<18:16,  1.62s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 354/1030 [09:48<18:42,  1.66s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 355/1030 [09:50<18:26,  1.64s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 356/1030 [09:51<18:14,  1.62s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 357/1030 [09:53<18:05,  1.61s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 358/1030 [09:54<17:58,  1.60s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 359/1030 [09:56<17:57,  1.61s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 360/1030 [09:58<18:26,  1.65s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 361/1030 [09:59<18:13,  1.64s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 362/1030 [10:01<18:54,  1.70s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 363/1030 [10:03<18:29,  1.66s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 364/1030 [10:04<18:30,  1.67s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 365/1030 [10:06<18:45,  1.69s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 366/1030 [10:08<19:12,  1.74s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 367/1030 [10:10<18:41,  1.69s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 368/1030 [10:11<19:08,  1.73s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 369/1030 [10:13<19:09,  1.74s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 370/1030 [10:15<18:38,  1.69s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 371/1030 [10:16<18:32,  1.69s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 372/1030 [10:18<18:16,  1.67s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 373/1030 [10:20<18:02,  1.65s/it]\u001b[A\n",
            "Iteration:  36%|███▋      | 374/1030 [10:21<17:46,  1.63s/it]\u001b[A\n",
            "Iteration:  36%|███▋      | 375/1030 [10:23<18:25,  1.69s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 376/1030 [10:25<18:34,  1.70s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 377/1030 [10:26<18:10,  1.67s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 378/1030 [10:28<18:11,  1.67s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 379/1030 [10:30<17:54,  1.65s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 380/1030 [10:31<17:53,  1.65s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 381/1030 [10:33<17:40,  1.63s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 382/1030 [10:35<17:28,  1.62s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 383/1030 [10:36<17:22,  1.61s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 384/1030 [10:38<17:33,  1.63s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 385/1030 [10:40<17:54,  1.67s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 386/1030 [10:41<17:40,  1.65s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 387/1030 [10:43<17:26,  1.63s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 388/1030 [10:44<17:20,  1.62s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 389/1030 [10:46<17:09,  1.61s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 390/1030 [10:48<17:09,  1.61s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 391/1030 [10:49<17:04,  1.60s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 392/1030 [10:51<17:02,  1.60s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 393/1030 [10:52<17:27,  1.65s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 394/1030 [10:54<17:17,  1.63s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 395/1030 [10:56<17:22,  1.64s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 396/1030 [10:57<17:26,  1.65s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 397/1030 [10:59<17:14,  1.63s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 398/1030 [11:01<17:34,  1.67s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 399/1030 [11:02<17:19,  1.65s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 400/1030 [11:04<17:08,  1.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.205885410308838  Mean Loss:  0.846357688382268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  39%|███▉      | 401/1030 [11:06<16:56,  1.62s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 402/1030 [11:07<17:35,  1.68s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 403/1030 [11:09<17:30,  1.67s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 404/1030 [11:11<17:08,  1.64s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 405/1030 [11:12<16:55,  1.62s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 406/1030 [11:14<17:03,  1.64s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 407/1030 [11:16<17:07,  1.65s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 408/1030 [11:17<17:28,  1.68s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 409/1030 [11:19<17:08,  1.66s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 410/1030 [11:20<16:54,  1.64s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 411/1030 [11:22<17:00,  1.65s/it]\u001b[A\n",
            "Iteration:  40%|████      | 412/1030 [11:24<17:03,  1.66s/it]\u001b[A\n",
            "Iteration:  40%|████      | 413/1030 [11:25<17:02,  1.66s/it]\u001b[A\n",
            "Iteration:  40%|████      | 414/1030 [11:27<17:05,  1.66s/it]\u001b[A\n",
            "Iteration:  40%|████      | 415/1030 [11:29<16:47,  1.64s/it]\u001b[A\n",
            "Iteration:  40%|████      | 416/1030 [11:30<16:35,  1.62s/it]\u001b[A\n",
            "Iteration:  40%|████      | 417/1030 [11:32<16:42,  1.63s/it]\u001b[A\n",
            "Iteration:  41%|████      | 418/1030 [11:34<16:44,  1.64s/it]\u001b[A\n",
            "Iteration:  41%|████      | 419/1030 [11:35<16:32,  1.62s/it]\u001b[A\n",
            "Iteration:  41%|████      | 420/1030 [11:37<16:24,  1.61s/it]\u001b[A\n",
            "Iteration:  41%|████      | 421/1030 [11:38<16:33,  1.63s/it]\u001b[A\n",
            "Iteration:  41%|████      | 422/1030 [11:40<16:23,  1.62s/it]\u001b[A\n",
            "Iteration:  41%|████      | 423/1030 [11:42<16:45,  1.66s/it]\u001b[A\n",
            "Iteration:  41%|████      | 424/1030 [11:44<17:01,  1.69s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 425/1030 [11:45<16:39,  1.65s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 426/1030 [11:47<16:23,  1.63s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 427/1030 [11:48<16:44,  1.67s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 428/1030 [11:50<16:42,  1.67s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 429/1030 [11:52<16:26,  1.64s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 430/1030 [11:53<16:15,  1.63s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 431/1030 [11:55<16:22,  1.64s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 432/1030 [11:57<16:27,  1.65s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 433/1030 [11:58<16:16,  1.64s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 434/1030 [12:00<16:07,  1.62s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 435/1030 [12:01<15:58,  1.61s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 436/1030 [12:03<15:53,  1.60s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 437/1030 [12:05<16:15,  1.65s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 438/1030 [12:06<16:07,  1.63s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 439/1030 [12:08<15:56,  1.62s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 440/1030 [12:10<15:50,  1.61s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 441/1030 [12:11<15:47,  1.61s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 442/1030 [12:13<15:43,  1.60s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 443/1030 [12:14<15:38,  1.60s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 444/1030 [12:16<16:04,  1.65s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 445/1030 [12:18<16:21,  1.68s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 446/1030 [12:19<16:06,  1.65s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 447/1030 [12:21<16:10,  1.66s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 448/1030 [12:23<15:56,  1.64s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 449/1030 [12:24<16:12,  1.67s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 450/1030 [12:26<15:57,  1.65s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 451/1030 [12:28<15:45,  1.63s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 452/1030 [12:29<15:35,  1.62s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 453/1030 [12:31<15:41,  1.63s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 454/1030 [12:33<16:16,  1.70s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 455/1030 [12:34<15:59,  1.67s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 456/1030 [12:36<16:10,  1.69s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 457/1030 [12:38<16:35,  1.74s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 458/1030 [12:40<16:10,  1.70s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 459/1030 [12:41<16:08,  1.70s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 460/1030 [12:43<15:44,  1.66s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 461/1030 [12:45<15:57,  1.68s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 462/1030 [12:46<16:22,  1.73s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 463/1030 [12:48<16:39,  1.76s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 464/1030 [12:50<16:05,  1.71s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 465/1030 [12:52<16:26,  1.75s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 466/1030 [12:53<15:58,  1.70s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 467/1030 [12:55<15:36,  1.66s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 468/1030 [12:56<15:22,  1.64s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 469/1030 [12:58<15:12,  1.63s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 470/1030 [13:00<15:15,  1.64s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 471/1030 [13:01<15:05,  1.62s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 472/1030 [13:03<14:58,  1.61s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 473/1030 [13:04<14:53,  1.60s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 474/1030 [13:06<14:50,  1.60s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 475/1030 [13:08<14:58,  1.62s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 476/1030 [13:09<14:56,  1.62s/it]\u001b[A\n",
            "Iteration:  46%|████▋     | 477/1030 [13:11<15:30,  1.68s/it]\u001b[A\n",
            "Iteration:  46%|████▋     | 478/1030 [13:13<15:39,  1.70s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 479/1030 [13:14<15:19,  1.67s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 480/1030 [13:16<15:30,  1.69s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 481/1030 [13:18<15:10,  1.66s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 482/1030 [13:20<15:22,  1.68s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 483/1030 [13:21<15:05,  1.66s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 484/1030 [13:23<15:05,  1.66s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 485/1030 [13:24<14:57,  1.65s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 486/1030 [13:26<14:44,  1.63s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 487/1030 [13:28<14:37,  1.62s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 488/1030 [13:29<14:29,  1.60s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 489/1030 [13:31<14:35,  1.62s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 490/1030 [13:32<14:41,  1.63s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 491/1030 [13:34<14:35,  1.62s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 492/1030 [13:36<14:37,  1.63s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 493/1030 [13:37<14:31,  1.62s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 494/1030 [13:39<14:25,  1.61s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 495/1030 [13:41<14:32,  1.63s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 496/1030 [13:42<14:50,  1.67s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 497/1030 [13:44<14:46,  1.66s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 498/1030 [13:46<14:34,  1.64s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 499/1030 [13:47<14:39,  1.66s/it]\u001b[A\n",
            "Iteration:  49%|████▊     | 500/1030 [13:49<14:23,  1.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.8982993364334106  Mean Loss:  0.8354074801504612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  49%|████▊     | 501/1030 [13:51<14:40,  1.66s/it]\u001b[A\n",
            "Iteration:  49%|████▊     | 502/1030 [13:52<14:23,  1.64s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 503/1030 [13:54<14:31,  1.65s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 504/1030 [13:55<14:19,  1.63s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 505/1030 [13:57<14:13,  1.63s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 506/1030 [13:59<14:05,  1.61s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 507/1030 [14:00<14:01,  1.61s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 508/1030 [14:02<14:50,  1.71s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 509/1030 [14:04<14:30,  1.67s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 510/1030 [14:05<14:15,  1.65s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 511/1030 [14:07<14:05,  1.63s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 512/1030 [14:09<14:37,  1.69s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 513/1030 [14:10<14:32,  1.69s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 514/1030 [14:12<14:14,  1.66s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 515/1030 [14:14<14:40,  1.71s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 516/1030 [14:16<14:46,  1.72s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 517/1030 [14:17<14:23,  1.68s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 518/1030 [14:19<14:07,  1.65s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 519/1030 [14:20<13:54,  1.63s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 520/1030 [14:22<13:55,  1.64s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 521/1030 [14:24<13:46,  1.62s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 522/1030 [14:25<14:16,  1.69s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 523/1030 [14:27<14:24,  1.70s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 524/1030 [14:29<14:17,  1.69s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 525/1030 [14:31<14:11,  1.69s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 526/1030 [14:32<13:53,  1.65s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 527/1030 [14:34<13:42,  1.64s/it]\u001b[A\n",
            "Iteration:  51%|█████▏    | 528/1030 [14:35<13:36,  1.63s/it]\u001b[A\n",
            "Iteration:  51%|█████▏    | 529/1030 [14:37<13:27,  1.61s/it]\u001b[A\n",
            "Iteration:  51%|█████▏    | 530/1030 [14:38<13:21,  1.60s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 531/1030 [14:40<13:40,  1.65s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 532/1030 [14:42<13:40,  1.65s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 533/1030 [14:43<13:29,  1.63s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 534/1030 [14:45<13:22,  1.62s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 535/1030 [14:47<13:24,  1.63s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 536/1030 [14:48<13:18,  1.62s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 537/1030 [14:50<13:13,  1.61s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 538/1030 [14:51<13:09,  1.60s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 539/1030 [14:53<13:05,  1.60s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 540/1030 [14:55<13:54,  1.70s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 541/1030 [14:57<13:35,  1.67s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 542/1030 [14:58<13:19,  1.64s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 543/1030 [15:00<13:11,  1.62s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 544/1030 [15:01<13:13,  1.63s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 545/1030 [15:03<13:04,  1.62s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 546/1030 [15:05<13:11,  1.63s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 547/1030 [15:06<13:04,  1.62s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 548/1030 [15:08<12:56,  1.61s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 549/1030 [15:09<12:50,  1.60s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 550/1030 [15:11<12:48,  1.60s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 551/1030 [15:13<12:43,  1.59s/it]\u001b[A\n",
            "Iteration:  54%|█████▎    | 552/1030 [15:14<13:07,  1.65s/it]\u001b[A\n",
            "Iteration:  54%|█████▎    | 553/1030 [15:16<13:06,  1.65s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 554/1030 [15:18<13:05,  1.65s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 555/1030 [15:19<13:04,  1.65s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 556/1030 [15:21<12:57,  1.64s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 557/1030 [15:23<12:49,  1.63s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 558/1030 [15:24<12:44,  1.62s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 559/1030 [15:26<12:46,  1.63s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 560/1030 [15:28<13:00,  1.66s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 561/1030 [15:29<12:48,  1.64s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 562/1030 [15:31<13:01,  1.67s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 563/1030 [15:32<12:46,  1.64s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 564/1030 [15:34<12:51,  1.65s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 565/1030 [15:36<12:51,  1.66s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 566/1030 [15:37<12:53,  1.67s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 567/1030 [15:39<13:02,  1.69s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 568/1030 [15:41<13:00,  1.69s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 569/1030 [15:43<12:58,  1.69s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 570/1030 [15:44<13:20,  1.74s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 571/1030 [15:46<13:18,  1.74s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 572/1030 [15:48<12:57,  1.70s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 573/1030 [15:49<12:38,  1.66s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 574/1030 [15:51<12:27,  1.64s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 575/1030 [15:53<12:29,  1.65s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 576/1030 [15:54<12:19,  1.63s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 577/1030 [15:56<12:18,  1.63s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 578/1030 [15:57<12:19,  1.64s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 579/1030 [15:59<12:09,  1.62s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 580/1030 [16:01<12:27,  1.66s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 581/1030 [16:02<12:15,  1.64s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 582/1030 [16:04<12:06,  1.62s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 583/1030 [16:06<12:01,  1.61s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 584/1030 [16:07<12:20,  1.66s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 585/1030 [16:09<12:28,  1.68s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 586/1030 [16:11<12:14,  1.65s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 587/1030 [16:12<12:02,  1.63s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 588/1030 [16:14<12:42,  1.73s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 589/1030 [16:16<12:44,  1.73s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 590/1030 [16:18<12:35,  1.72s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 591/1030 [16:19<12:14,  1.67s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 592/1030 [16:21<12:01,  1.65s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 593/1030 [16:22<12:01,  1.65s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 594/1030 [16:24<12:11,  1.68s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 595/1030 [16:26<12:19,  1.70s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 596/1030 [16:28<12:38,  1.75s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 597/1030 [16:30<12:34,  1.74s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 598/1030 [16:31<12:11,  1.69s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 599/1030 [16:33<12:07,  1.69s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 600/1030 [16:35<12:12,  1.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.660020112991333  Mean Loss:  0.8318101598570744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  58%|█████▊    | 601/1030 [16:36<12:26,  1.74s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 602/1030 [16:38<12:25,  1.74s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 603/1030 [16:40<12:15,  1.72s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 604/1030 [16:41<11:57,  1.68s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 605/1030 [16:43<12:02,  1.70s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 606/1030 [16:45<11:55,  1.69s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 607/1030 [16:46<11:52,  1.68s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 608/1030 [16:48<11:37,  1.65s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 609/1030 [16:50<11:27,  1.63s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 610/1030 [16:51<11:53,  1.70s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 611/1030 [16:53<12:00,  1.72s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 612/1030 [16:55<11:43,  1.68s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 613/1030 [16:56<11:27,  1.65s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 614/1030 [16:58<11:21,  1.64s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 615/1030 [17:00<11:21,  1.64s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 616/1030 [17:01<11:13,  1.63s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 617/1030 [17:03<11:07,  1.62s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 618/1030 [17:05<11:22,  1.66s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 619/1030 [17:06<11:12,  1.64s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 620/1030 [17:08<11:03,  1.62s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 621/1030 [17:09<11:08,  1.63s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 622/1030 [17:11<11:20,  1.67s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 623/1030 [17:13<11:09,  1.64s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 624/1030 [17:14<11:08,  1.65s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 625/1030 [17:16<11:20,  1.68s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 626/1030 [17:18<11:05,  1.65s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 627/1030 [17:19<11:05,  1.65s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 628/1030 [17:21<10:59,  1.64s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 629/1030 [17:23<11:00,  1.65s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 630/1030 [17:24<10:53,  1.63s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 631/1030 [17:26<10:46,  1.62s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 632/1030 [17:28<11:10,  1.68s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 633/1030 [17:29<11:16,  1.70s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 634/1030 [17:31<11:19,  1.72s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 635/1030 [17:33<11:01,  1.68s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 636/1030 [17:34<10:59,  1.67s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 637/1030 [17:36<10:48,  1.65s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 638/1030 [17:38<10:56,  1.67s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 639/1030 [17:40<11:03,  1.70s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 640/1030 [17:41<11:01,  1.70s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 641/1030 [17:43<10:56,  1.69s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 642/1030 [17:44<10:42,  1.66s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 643/1030 [17:46<10:50,  1.68s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 644/1030 [17:48<10:38,  1.66s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 645/1030 [17:49<10:29,  1.64s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 646/1030 [17:51<10:51,  1.70s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 647/1030 [17:53<10:55,  1.71s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 648/1030 [17:55<10:40,  1.68s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 649/1030 [17:56<10:27,  1.65s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 650/1030 [17:58<10:58,  1.73s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 651/1030 [18:00<10:51,  1.72s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 652/1030 [18:01<10:33,  1.68s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 653/1030 [18:03<10:22,  1.65s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 654/1030 [18:05<10:52,  1.73s/it]\u001b[A\n",
            "Iteration:  64%|██████▎   | 655/1030 [18:06<10:32,  1.69s/it]\u001b[A\n",
            "Iteration:  64%|██████▎   | 656/1030 [18:08<10:21,  1.66s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 657/1030 [18:10<10:14,  1.65s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 658/1030 [18:11<10:04,  1.62s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 659/1030 [18:13<09:59,  1.62s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 660/1030 [18:14<09:54,  1.61s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 661/1030 [18:16<10:00,  1.63s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 662/1030 [18:18<09:53,  1.61s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 663/1030 [18:19<09:50,  1.61s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 664/1030 [18:21<10:03,  1.65s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 665/1030 [18:23<09:53,  1.63s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 666/1030 [18:24<09:45,  1.61s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 667/1030 [18:26<09:39,  1.60s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 668/1030 [18:27<09:35,  1.59s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 669/1030 [18:29<09:32,  1.58s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 670/1030 [18:30<09:30,  1.58s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 671/1030 [18:32<09:30,  1.59s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 672/1030 [18:34<09:29,  1.59s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 673/1030 [18:35<09:44,  1.64s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 674/1030 [18:37<09:38,  1.63s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 675/1030 [18:39<09:32,  1.61s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 676/1030 [18:40<09:45,  1.65s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 677/1030 [18:42<09:54,  1.68s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 678/1030 [18:44<09:41,  1.65s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 679/1030 [18:45<09:49,  1.68s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 680/1030 [18:47<09:53,  1.70s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 681/1030 [18:49<09:39,  1.66s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 682/1030 [18:50<09:31,  1.64s/it]\u001b[A\n",
            "Iteration:  66%|██████▋   | 683/1030 [18:52<09:33,  1.65s/it]\u001b[A\n",
            "Iteration:  66%|██████▋   | 684/1030 [18:54<09:42,  1.68s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 685/1030 [18:55<09:31,  1.66s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 686/1030 [18:57<09:38,  1.68s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 687/1030 [18:59<09:44,  1.70s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 688/1030 [19:00<09:32,  1.67s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 689/1030 [19:02<09:20,  1.65s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 690/1030 [19:04<09:31,  1.68s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 691/1030 [19:05<09:20,  1.65s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 692/1030 [19:07<09:13,  1.64s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 693/1030 [19:09<09:07,  1.63s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 694/1030 [19:10<09:11,  1.64s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 695/1030 [19:12<09:03,  1.62s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 696/1030 [19:13<08:59,  1.62s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 697/1030 [19:15<08:53,  1.60s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 698/1030 [19:17<08:50,  1.60s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 699/1030 [19:18<09:11,  1.67s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 700/1030 [19:20<09:01,  1.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.7457097768783569  Mean Loss:  0.830417061490672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  68%|██████▊   | 701/1030 [19:22<08:56,  1.63s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 702/1030 [19:23<08:59,  1.65s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 703/1030 [19:25<09:00,  1.65s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 704/1030 [19:27<09:08,  1.68s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 705/1030 [19:28<09:09,  1.69s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 706/1030 [19:30<08:56,  1.66s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 707/1030 [19:32<08:48,  1.64s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 708/1030 [19:33<08:43,  1.62s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 709/1030 [19:35<09:12,  1.72s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 710/1030 [19:37<08:59,  1.69s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 711/1030 [19:38<08:49,  1.66s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 712/1030 [19:40<08:42,  1.64s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 713/1030 [19:42<08:42,  1.65s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 714/1030 [19:43<08:35,  1.63s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 715/1030 [19:45<08:53,  1.69s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 716/1030 [19:47<08:57,  1.71s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 717/1030 [19:48<08:45,  1.68s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 718/1030 [19:50<08:43,  1.68s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 719/1030 [19:52<08:41,  1.68s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 720/1030 [19:53<08:31,  1.65s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 721/1030 [19:55<08:56,  1.74s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 722/1030 [19:57<08:41,  1.69s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 723/1030 [19:58<08:30,  1.66s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 724/1030 [20:00<08:22,  1.64s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 725/1030 [20:02<08:29,  1.67s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 726/1030 [20:03<08:19,  1.64s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 727/1030 [20:05<08:35,  1.70s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 728/1030 [20:07<08:23,  1.67s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 729/1030 [20:08<08:14,  1.64s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 730/1030 [20:10<08:08,  1.63s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 731/1030 [20:12<08:04,  1.62s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 732/1030 [20:13<08:00,  1.61s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 733/1030 [20:15<07:56,  1.60s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 734/1030 [20:16<07:53,  1.60s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 735/1030 [20:18<07:58,  1.62s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 736/1030 [20:20<08:01,  1.64s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 737/1030 [20:21<08:04,  1.65s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 738/1030 [20:23<07:56,  1.63s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 739/1030 [20:25<08:05,  1.67s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 740/1030 [20:26<08:11,  1.70s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 741/1030 [20:28<08:23,  1.74s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 742/1030 [20:30<08:16,  1.72s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 743/1030 [20:32<08:16,  1.73s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 744/1030 [20:33<08:02,  1.69s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 745/1030 [20:35<07:58,  1.68s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 746/1030 [20:37<08:03,  1.70s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 747/1030 [20:38<08:01,  1.70s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 748/1030 [20:40<07:56,  1.69s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 749/1030 [20:42<07:45,  1.66s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 750/1030 [20:43<07:51,  1.68s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 751/1030 [20:45<07:42,  1.66s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 752/1030 [20:47<07:49,  1.69s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 753/1030 [20:48<07:40,  1.66s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 754/1030 [20:50<07:45,  1.69s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 755/1030 [20:52<07:37,  1.66s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 756/1030 [20:53<07:44,  1.69s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 757/1030 [20:55<07:41,  1.69s/it]\u001b[A\n",
            "Iteration:  74%|███████▎  | 758/1030 [20:57<07:32,  1.66s/it]\u001b[A\n",
            "Iteration:  74%|███████▎  | 759/1030 [20:59<07:45,  1.72s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 760/1030 [21:00<07:45,  1.72s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 761/1030 [21:02<07:32,  1.68s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 762/1030 [21:04<07:36,  1.70s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 763/1030 [21:05<07:24,  1.67s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 764/1030 [21:07<07:22,  1.67s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 765/1030 [21:09<07:15,  1.64s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 766/1030 [21:10<07:10,  1.63s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 767/1030 [21:12<07:03,  1.61s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 768/1030 [21:13<07:01,  1.61s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 769/1030 [21:15<07:24,  1.70s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 770/1030 [21:17<07:14,  1.67s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 771/1030 [21:19<07:26,  1.72s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 772/1030 [21:20<07:21,  1.71s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 773/1030 [21:22<07:16,  1.70s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 774/1030 [21:24<07:08,  1.67s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 775/1030 [21:25<07:00,  1.65s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 776/1030 [21:27<07:01,  1.66s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 777/1030 [21:29<07:00,  1.66s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 778/1030 [21:30<06:53,  1.64s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 779/1030 [21:32<06:53,  1.65s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 780/1030 [21:33<06:47,  1.63s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 781/1030 [21:35<07:02,  1.70s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 782/1030 [21:37<07:04,  1.71s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 783/1030 [21:39<06:54,  1.68s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 784/1030 [21:40<06:46,  1.65s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 785/1030 [21:42<06:40,  1.64s/it]\u001b[A\n",
            "Iteration:  76%|███████▋  | 786/1030 [21:44<06:47,  1.67s/it]\u001b[A\n",
            "Iteration:  76%|███████▋  | 787/1030 [21:45<07:05,  1.75s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 788/1030 [21:47<06:51,  1.70s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 789/1030 [21:49<06:47,  1.69s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 790/1030 [21:50<06:49,  1.71s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 791/1030 [21:52<06:39,  1.67s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 792/1030 [21:54<06:31,  1.64s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 793/1030 [21:55<06:32,  1.65s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 794/1030 [21:57<06:25,  1.63s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 795/1030 [21:59<06:31,  1.67s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 796/1030 [22:00<06:35,  1.69s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 797/1030 [22:02<06:26,  1.66s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 798/1030 [22:04<06:19,  1.63s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 799/1030 [22:05<06:24,  1.67s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 800/1030 [22:07<06:24,  1.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 1.4529889822006226  Mean Loss:  0.8345671543851495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  78%|███████▊  | 801/1030 [22:09<06:16,  1.64s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 802/1030 [22:10<06:17,  1.66s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 803/1030 [22:12<06:12,  1.64s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 804/1030 [22:14<06:13,  1.65s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 805/1030 [22:15<06:08,  1.64s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 806/1030 [22:17<06:05,  1.63s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 807/1030 [22:18<06:02,  1.62s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 808/1030 [22:20<06:04,  1.64s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 809/1030 [22:22<05:59,  1.63s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 810/1030 [22:23<06:02,  1.65s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 811/1030 [22:25<05:56,  1.63s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 812/1030 [22:27<06:01,  1.66s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 813/1030 [22:28<06:06,  1.69s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 814/1030 [22:30<05:58,  1.66s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 815/1030 [22:32<05:52,  1.64s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 816/1030 [22:33<05:48,  1.63s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 817/1030 [22:35<05:43,  1.61s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 818/1030 [22:37<05:56,  1.68s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 819/1030 [22:38<05:49,  1.66s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 820/1030 [22:40<05:48,  1.66s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 821/1030 [22:42<05:47,  1.66s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 822/1030 [22:43<05:46,  1.66s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 823/1030 [22:45<05:40,  1.64s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 824/1030 [22:46<05:35,  1.63s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 825/1030 [22:48<05:42,  1.67s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 826/1030 [22:50<05:36,  1.65s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 827/1030 [22:52<05:41,  1.68s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 828/1030 [22:53<05:54,  1.75s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 829/1030 [22:55<05:41,  1.70s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 830/1030 [22:57<05:32,  1.66s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 831/1030 [22:58<05:35,  1.69s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 832/1030 [23:00<05:28,  1.66s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 833/1030 [23:02<05:28,  1.67s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 834/1030 [23:03<05:26,  1.66s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 835/1030 [23:05<05:28,  1.68s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 836/1030 [23:07<05:22,  1.66s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 837/1030 [23:08<05:20,  1.66s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 838/1030 [23:10<05:24,  1.69s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 839/1030 [23:12<05:21,  1.68s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 840/1030 [23:13<05:23,  1.70s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 841/1030 [23:15<05:20,  1.69s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 842/1030 [23:17<05:26,  1.74s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 843/1030 [23:19<05:16,  1.69s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 844/1030 [23:20<05:13,  1.68s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 845/1030 [23:22<05:06,  1.66s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 846/1030 [23:24<05:09,  1.68s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 847/1030 [23:25<05:12,  1.70s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 848/1030 [23:27<05:12,  1.72s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 849/1030 [23:29<05:05,  1.69s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 850/1030 [23:30<05:02,  1.68s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 851/1030 [23:32<04:55,  1.65s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 852/1030 [23:34<04:50,  1.63s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 853/1030 [23:35<04:47,  1.62s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 854/1030 [23:37<04:44,  1.62s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 855/1030 [23:38<04:41,  1.61s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 856/1030 [23:40<04:46,  1.65s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 857/1030 [23:42<04:50,  1.68s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 858/1030 [23:44<05:01,  1.76s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 859/1030 [23:45<04:57,  1.74s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 860/1030 [23:47<04:47,  1.69s/it]\u001b[A\n",
            "Iteration:  84%|████████▎ | 861/1030 [23:49<05:02,  1.79s/it]\u001b[A\n",
            "Iteration:  84%|████████▎ | 862/1030 [23:51<04:51,  1.73s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 863/1030 [23:52<04:45,  1.71s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 864/1030 [23:54<04:38,  1.68s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 865/1030 [23:55<04:31,  1.65s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 866/1030 [23:57<04:26,  1.63s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 867/1030 [23:59<04:23,  1.62s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 868/1030 [24:00<04:24,  1.63s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 869/1030 [24:02<04:21,  1.62s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 870/1030 [24:03<04:18,  1.61s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 871/1030 [24:05<04:28,  1.69s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 872/1030 [24:07<04:21,  1.65s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 873/1030 [24:09<04:20,  1.66s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 874/1030 [24:10<04:18,  1.66s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 875/1030 [24:12<04:25,  1.71s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 876/1030 [24:14<04:26,  1.73s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 877/1030 [24:16<04:21,  1.71s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 878/1030 [24:17<04:30,  1.78s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 879/1030 [24:19<04:20,  1.73s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 880/1030 [24:21<04:13,  1.69s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 881/1030 [24:22<04:10,  1.68s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 882/1030 [24:24<04:04,  1.65s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 883/1030 [24:26<04:04,  1.67s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 884/1030 [24:27<03:59,  1.64s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 885/1030 [24:29<03:55,  1.63s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 886/1030 [24:30<03:52,  1.61s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 887/1030 [24:32<03:53,  1.63s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 888/1030 [24:34<03:49,  1.61s/it]\u001b[A\n",
            "Iteration:  86%|████████▋ | 889/1030 [24:35<03:53,  1.65s/it]\u001b[A\n",
            "Iteration:  86%|████████▋ | 890/1030 [24:37<03:48,  1.63s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 891/1030 [24:39<03:50,  1.66s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 892/1030 [24:40<03:52,  1.68s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 893/1030 [24:42<03:53,  1.70s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 894/1030 [24:44<03:53,  1.72s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 895/1030 [24:46<03:46,  1.68s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 896/1030 [24:47<03:41,  1.65s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 897/1030 [24:49<03:36,  1.63s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 898/1030 [24:51<03:44,  1.70s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 899/1030 [24:52<03:38,  1.67s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 900/1030 [24:54<03:35,  1.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.854987621307373  Mean Loss:  0.8317608496546746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  87%|████████▋ | 901/1030 [24:55<03:30,  1.64s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 902/1030 [24:57<03:30,  1.65s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 903/1030 [24:59<03:26,  1.62s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 904/1030 [25:00<03:26,  1.64s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 905/1030 [25:02<03:26,  1.65s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 906/1030 [25:04<03:22,  1.63s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 907/1030 [25:05<03:28,  1.70s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 908/1030 [25:07<03:23,  1.67s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 909/1030 [25:09<03:19,  1.65s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 910/1030 [25:10<03:21,  1.68s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 911/1030 [25:12<03:20,  1.68s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 912/1030 [25:14<03:14,  1.65s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 913/1030 [25:15<03:11,  1.63s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 914/1030 [25:17<03:08,  1.62s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 915/1030 [25:18<03:04,  1.61s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 916/1030 [25:20<03:03,  1.61s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 917/1030 [25:22<03:00,  1.60s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 918/1030 [25:23<03:04,  1.65s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 919/1030 [25:25<03:03,  1.66s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 920/1030 [25:27<03:08,  1.71s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 921/1030 [25:29<03:08,  1.72s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 922/1030 [25:30<03:06,  1.73s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 923/1030 [25:32<03:00,  1.68s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 924/1030 [25:33<02:55,  1.65s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 925/1030 [25:35<02:51,  1.63s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 926/1030 [25:37<02:52,  1.66s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 927/1030 [25:38<02:51,  1.66s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 928/1030 [25:40<02:52,  1.69s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 929/1030 [25:42<02:47,  1.66s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 930/1030 [25:43<02:44,  1.65s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 931/1030 [25:45<02:46,  1.68s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 932/1030 [25:47<02:46,  1.69s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 933/1030 [25:49<02:45,  1.71s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 934/1030 [25:50<02:43,  1.70s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 935/1030 [25:52<02:38,  1.67s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 936/1030 [25:54<02:34,  1.64s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 937/1030 [25:55<02:35,  1.68s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 938/1030 [25:57<02:38,  1.72s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 939/1030 [25:59<02:33,  1.68s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 940/1030 [26:00<02:29,  1.66s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 941/1030 [26:02<02:28,  1.67s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 942/1030 [26:04<02:24,  1.64s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 943/1030 [26:05<02:21,  1.63s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 944/1030 [26:07<02:18,  1.61s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 945/1030 [26:09<02:22,  1.68s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 946/1030 [26:10<02:19,  1.66s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 947/1030 [26:12<02:16,  1.64s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 948/1030 [26:13<02:15,  1.66s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 949/1030 [26:15<02:14,  1.66s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 950/1030 [26:17<02:13,  1.67s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 951/1030 [26:18<02:10,  1.65s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 952/1030 [26:20<02:07,  1.63s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 953/1030 [26:22<02:06,  1.64s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 954/1030 [26:23<02:03,  1.63s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 955/1030 [26:25<02:05,  1.67s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 956/1030 [26:27<02:07,  1.72s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 957/1030 [26:29<02:04,  1.70s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 958/1030 [26:30<02:02,  1.70s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 959/1030 [26:32<01:58,  1.67s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 960/1030 [26:34<02:04,  1.77s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 961/1030 [26:35<01:59,  1.73s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 962/1030 [26:37<01:54,  1.68s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 963/1030 [26:39<01:50,  1.65s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 964/1030 [26:41<01:53,  1.71s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 965/1030 [26:42<01:48,  1.68s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 966/1030 [26:44<01:47,  1.68s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 967/1030 [26:45<01:44,  1.66s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 968/1030 [26:47<01:44,  1.68s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 969/1030 [26:49<01:45,  1.74s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 970/1030 [26:51<01:45,  1.77s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 971/1030 [26:53<01:44,  1.76s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 972/1030 [26:54<01:39,  1.72s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 973/1030 [26:56<01:35,  1.68s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 974/1030 [26:58<01:36,  1.73s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 975/1030 [26:59<01:32,  1.68s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 976/1030 [27:01<01:29,  1.66s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 977/1030 [27:02<01:26,  1.63s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 978/1030 [27:04<01:26,  1.67s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 979/1030 [27:06<01:26,  1.70s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 980/1030 [27:07<01:23,  1.67s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 981/1030 [27:09<01:21,  1.67s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 982/1030 [27:11<01:19,  1.65s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 983/1030 [27:12<01:16,  1.63s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 984/1030 [27:14<01:14,  1.62s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 985/1030 [27:16<01:12,  1.61s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 986/1030 [27:17<01:10,  1.60s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 987/1030 [27:19<01:08,  1.60s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 988/1030 [27:21<01:10,  1.67s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 989/1030 [27:22<01:08,  1.67s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 990/1030 [27:24<01:05,  1.64s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 991/1030 [27:25<01:03,  1.62s/it]\u001b[A\n",
            "Iteration:  96%|█████████▋| 992/1030 [27:27<01:02,  1.64s/it]\u001b[A\n",
            "Iteration:  96%|█████████▋| 993/1030 [27:29<01:00,  1.62s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 994/1030 [27:30<00:58,  1.63s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 995/1030 [27:32<00:58,  1.67s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 996/1030 [27:34<00:57,  1.69s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 997/1030 [27:35<00:54,  1.66s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 998/1030 [27:37<00:54,  1.72s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 999/1030 [27:39<00:54,  1.75s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 1000/1030 [27:41<00:52,  1.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Current Loss 0.5998992919921875  Mean Loss:  0.8284170026183129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  97%|█████████▋| 1001/1030 [27:42<00:49,  1.70s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 1002/1030 [27:44<00:47,  1.71s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 1003/1030 [27:46<00:45,  1.67s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 1004/1030 [27:47<00:42,  1.65s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1005/1030 [27:49<00:42,  1.70s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1006/1030 [27:51<00:40,  1.68s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1007/1030 [27:52<00:37,  1.65s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1008/1030 [27:54<00:36,  1.68s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1009/1030 [27:56<00:34,  1.66s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1010/1030 [27:57<00:32,  1.64s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1011/1030 [27:59<00:32,  1.70s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1012/1030 [28:01<00:30,  1.67s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1013/1030 [28:02<00:28,  1.67s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 1014/1030 [28:04<00:26,  1.67s/it]\u001b[A\n",
            "Iteration:  99%|█████████▊| 1015/1030 [28:06<00:26,  1.77s/it]\u001b[A\n",
            "Iteration:  99%|█████████▊| 1016/1030 [28:08<00:25,  1.79s/it]\u001b[A\n",
            "Iteration:  99%|█████████▊| 1017/1030 [28:10<00:22,  1.76s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1018/1030 [28:11<00:20,  1.73s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1019/1030 [28:13<00:18,  1.70s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1020/1030 [28:15<00:17,  1.77s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1021/1030 [28:16<00:15,  1.74s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1022/1030 [28:18<00:13,  1.70s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1023/1030 [28:20<00:11,  1.68s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 1024/1030 [28:21<00:09,  1.65s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1025/1030 [28:23<00:08,  1.66s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1026/1030 [28:25<00:06,  1.69s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1027/1030 [28:26<00:04,  1.66s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1028/1030 [28:28<00:03,  1.69s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 1029/1030 [28:30<00:01,  1.68s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 1030/1030 [28:30<00:00,  1.40s/it]\u001b[A\n",
            "Epoch: 100%|██████████| 3/3 [1:24:57<00:00, 1696.22s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y4-uxlJUCOuR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Save the trained model"
      ]
    },
    {
      "metadata": {
        "id": "pUIF5MA7BCOG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save a trained model\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  \n",
        "output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "torch.save(model_to_save.state_dict(), output_model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7-T0HXoJCN-o",
        "colab_type": "code",
        "outputId": "097bb1ef-ed05-4fdf-bce0-fca43e59d72b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5321
        }
      },
      "cell_type": "code",
      "source": [
        "# Load a trained model that you have fine-tuned\n",
        "model_state_dict = torch.load(output_model_file)\n",
        "model = BertForSequenceClassification.from_pretrained(bert_model, state_dict=model_state_dict, num_labels=num_labels)\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/08/2019 07:26:53 - INFO - pytorch_pretrained_bert.modeling -   loading archive file uncased_L-12_H-768_A-12\n",
            "02/08/2019 07:26:53 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1)\n",
              "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "TkujD7IJCbck",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run evaluation on dev set"
      ]
    },
    {
      "metadata": {
        "id": "Ie1-OFFQCZxC",
        "colab_type": "code",
        "outputId": "3154c1a2-18fb-4c2d-c4f9-8852f87625ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "cell_type": "code",
      "source": [
        "eval_examples = processor.get_dev_examples(data_dir)\n",
        "eval_features = convert_examples_to_features(\n",
        "    eval_examples, label_list, max_seq_length, tokenizer)\n",
        "\n",
        "logger.info(\"***** Running evaluation *****\")\n",
        "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
        "logger.info(\"  Batch size = %d\", eval_batch_size)\n",
        "\n",
        "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
        "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
        "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "# Run prediction for full data\n",
        "eval_sampler = SequentialSampler(eval_data)\n",
        "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/08/2019 07:26:58 - INFO - __main__ -   *** Example ***\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   guid: dev-1\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   tokens: [CLS] as ##m ##r autonomous sensory meridian response a new way of indian cooking . so relaxing to watch [SEP]\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   input_ids: 101 2004 2213 2099 8392 16792 17984 3433 1037 2047 2126 1997 2796 8434 1012 2061 19613 2000 3422 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   label: 2 (id = 2)\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   *** Example ***\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   guid: dev-2\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   tokens: [CLS] ji ##o phone un ##box ##ing features and first look . must watch . [SEP]\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   input_ids: 101 10147 2080 3042 4895 8758 2075 2838 1998 2034 2298 1012 2442 3422 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   label: 8 (id = 8)\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   *** Example ***\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   guid: dev-3\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   tokens: [CLS] in delhi public services may come to your doorstep for an extra fee of rs [SEP]\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   input_ids: 101 1999 6768 2270 2578 2089 2272 2000 2115 26581 2005 2019 4469 7408 1997 12667 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   label: 5 (id = 5)\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   *** Example ***\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   guid: dev-4\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   tokens: [CLS] anyone here using a k monitor ? are thee ##y worth it ? need suggestions . thanks . [SEP]\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   input_ids: 101 3087 2182 2478 1037 1047 8080 1029 2024 14992 2100 4276 2009 1029 2342 15690 1012 4283 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   label: 0 (id = 0)\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   *** Example ***\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   guid: dev-5\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   tokens: [CLS] the blue whale game in india [SEP]\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   input_ids: 101 1996 2630 13156 2208 1999 2634 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:26:58 - INFO - __main__ -   label: 8 (id = 8)\n",
            "02/08/2019 07:27:03 - INFO - __main__ -   ***** Running evaluation *****\n",
            "02/08/2019 07:27:03 - INFO - __main__ -     Num examples = 4122\n",
            "02/08/2019 07:27:03 - INFO - __main__ -     Batch size = 16\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "maGCAUq7CZ0v",
        "colab_type": "code",
        "outputId": "3d90dac0-fae5-4b2f-cd92-a5e798588b9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "results = []\n",
        "\n",
        "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "    input_ids = input_ids.to(device)\n",
        "    input_mask = input_mask.to(device)\n",
        "    segment_ids = segment_ids.to(device)\n",
        "    label_ids = label_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "        logits = model(input_ids, segment_ids, input_mask)\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    results.append(logits)\n",
        "    label_ids = label_ids.to('cpu').numpy()\n",
        "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
        "\n",
        "    eval_loss += tmp_eval_loss.mean().item()\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    nb_eval_examples += input_ids.size(0)\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "eval_loss = eval_loss / nb_eval_steps\n",
        "eval_accuracy = eval_accuracy / nb_eval_examples\n",
        "loss = tr_loss/nb_tr_steps\n",
        "result = {'eval_loss': eval_loss,\n",
        "          'eval_accuracy': eval_accuracy,\n",
        "          'global_step': global_step,\n",
        "          'loss': loss}\n",
        "\n",
        "output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
        "with open(output_eval_file, \"w\") as writer:\n",
        "    logger.info(\"***** Eval results *****\")\n",
        "    for key in sorted(result.keys()):\n",
        "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 258/258 [04:26<00:00,  1.07it/s]\n",
            "02/08/2019 07:31:30 - INFO - __main__ -   ***** Eval results *****\n",
            "02/08/2019 07:31:30 - INFO - __main__ -     eval_accuracy = 0.6710334788937409\n",
            "02/08/2019 07:31:30 - INFO - __main__ -     eval_loss = 0.9934918261313623\n",
            "02/08/2019 07:31:30 - INFO - __main__ -     global_step = 3090\n",
            "02/08/2019 07:31:30 - INFO - __main__ -     loss = 0.8285030601383413\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bgxhgw0oOkPW",
        "colab_type": "code",
        "outputId": "024e3173-2767-461c-958f-917cde6fe967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "final_preds = []\n",
        "soft = torch.nn.Softmax()\n",
        "\n",
        "for result in results:\n",
        "  for i in result:\n",
        "    final_preds.append(soft(torch.from_numpy(i)).numpy())\n",
        "\n",
        "submissions = []\n",
        "for i in final_preds:\n",
        "  submissions.append(np.argmax(i))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9H8Zau0jOCPW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_classes = 11\n",
        "\n",
        "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "with torch.no_grad():\n",
        "    for t, p in zip(all_label_ids, submissions):\n",
        "      confusion_matrix[t, p] += 1\n",
        "\n",
        "confusion_matrix = np.flip(confusion_matrix, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hhg_HFCsRDAV",
        "colab_type": "code",
        "outputId": "8e53ed90-9dc5-4595-dd03-a5db55a9fe45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "l = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(confusion_matrix, annot=True, ax = ax, ); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(l); ax.yaxis.set_ticklabels(l);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc8AAAFnCAYAAADAN8KoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCFFLkjyFEEKIWvp/7fyeD+EUteYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "nqBfSUOwDbi6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluating on Custom Inputs "
      ]
    },
    {
      "metadata": {
        "id": "YZLN1BCTONxD",
        "colab_type": "code",
        "outputId": "685d1617-e1bc-44fa-e7c0-f88f0d83edf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "cell_type": "code",
      "source": [
        "%time\n",
        "results = []\n",
        "\n",
        "sent = \"Mumbai man is suing his parents because they brought him into the world without his permission.\"\n",
        "test = [InputExample(guid=8, text_a=sent, text_b=None, label='0')]\n",
        "test_features = convert_examples_to_features(test, label_list, max_seq_length, tokenizer)\n",
        "\n",
        "all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long).to(device)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long).to(device)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long).to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(all_input_ids, all_segment_ids, all_input_mask)\n",
        "\n",
        "logits = logits.detach().cpu().numpy()\n",
        "results.append(logits)\n",
        "\n",
        "soft = torch.nn.Softmax()\n",
        "r = np.argmax(soft(torch.from_numpy(results[0])).numpy())\n",
        "print ('\\n\\nLabel: ', id_to_label[r])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/08/2019 07:34:09 - INFO - __main__ -   *** Example ***\n",
            "02/08/2019 07:34:09 - INFO - __main__ -   guid: 8\n",
            "02/08/2019 07:34:09 - INFO - __main__ -   tokens: [CLS] mumbai man is sui ##ng his parents because they brought him into the world without his permission . [SEP]\n",
            "02/08/2019 07:34:09 - INFO - __main__ -   input_ids: 101 8955 2158 2003 24086 3070 2010 3008 2138 2027 2716 2032 2046 1996 2088 2302 2010 6656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:34:09 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:34:09 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/08/2019 07:34:09 - INFO - __main__ -   label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
            "Wall time: 8.11 µs\n",
            "\n",
            "\n",
            "Label:  Non-Political\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}